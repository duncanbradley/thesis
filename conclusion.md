# Discussion

~ 2500 words

# Briefly recap research questions and objectives


# Briefly recount the answers to those questions


# Discuss significance and implication of findings


# Discuss the contributions made


# Critical review of methodology

Intro - this work is a defence of the thesis that… I suggest that…, demonstrate that

Data representations exploit our sensory capabilities, and cognitive heuristics. 

Effective communication of numerical information is typically concerned with clarity and coherence. 

I like the idea of study numbers because they have the illusion of objectivity and trustworthiness, which decreases likelihood of spotting bias and manipulation. They are also more precisely defined than other types of information, which means that we should theoretically all be able to represent them the same - yet this does not happen. We are not even always  consistent even with ourselves. But bias shouldn’t always be considered a negative thing - it reflects valuable features of our cognitive system. The popularity of the ‘chart-junk’ concept also speaks to an instinct that if only we presented data with minimal embellishments, we would show it in its purest form (Kosara, 2016). 

We may say that axis limits *inform* but do not *dictate* magnitude judgements. 

Grice (1975) characterises discourse as essentially cooperative in nature. Among others features, ‘quality’ (i.e. truthfulness) is a central aspect of effective communication. 

  * increase utility (maintain benefits, avoid harms) - through ensuring accuracy

Are these magnitude effects as bad (or worse) than magnitude of *difference* effects?

On horizontal postion: Part of Franconeri et al.’s (2021) discussion of creating understandable charts discusses the role of conventions for representing magnitude. These include color intensity and *horizontal* and vertical position. This is to fit with a schema, facilitating comphrension. 

Ziemkiewicz and Kosara note that cognitive models which assume an ‘information extraction’ perspective tend to ignore the fact that one’s internal representations can influence interpretation - it goes both ways. Similarly, a focus on precision of individual elements fails to account for the overall form of the graphic and how it might fit with metaphors. Consistent with Tversky and Zacks. Thinking about metaphor is a good way to understand how and why design choices might affect interpretation. 


# Findings

How do each of the studies complement each other? What are the outstanding questions from one study that are answered by another study? How do we interpret the results in light of the other studies? What are the repeated findings that we have replicated? What are the main differences between the studies, unique contributions?

discuss counterfacutals - 

The axes are a canvas for conveying context.

What does the bar chart study say about the choropleth map study?
Despite the fact that a map has no axes in the typical sense, its construction is extremely similar to geometric charts. The only difference is that the axes (legend) are not a canvas on which to plot (that’s the purpose of the map). The fact that there is a range of values, and then plotted data, remains the same. From an experimental point of view, however it’s useful. First, it’s a different type of encoding, so expands the evidence to different visualisation techniques. Second, it allows the axis and plotted values to be divorced, such that the values appear the same regardless of the axis limits. Therefore, it’s a particularly strong test of the theory - the appearance of values don’t change as a result of the manipulation. 

There is a difference between the choropleth map study and the bar chart study in terms of upper limits. Because the bar chart data is drawn from a finite population, there is a logical upper limit. Becuase the choropleth map data represents a measurement, rather than a proportion, there is no logical upper limit. This highlights that axes can provide various types of numerical context, but that it’s important to consider what the type of data at hand when deciding axis limits.  

Correll et al. used context-free visualisations to test the effects of truncation, yet advocate for the importance of context when designing charts. Perhaps testing visualisations in context would reveal limitations.

Markant’s findings are relevant here because they also illustrate how external information moderates/mediates the impact of charts. Markant’s work refers to perspective, my work refers to knowledge, but both show how the impact of a design is not fixed, and can depend on what’s in someone’s mind. 

Gigerenzer and Brighton - Homo heuristicus: Why biased minds make better inferences,” Topics in cognitive science, vol. 1, no. 1, pp. 107–143, 2009. - suggests that biases shouldn’t be seen as completely negative, but useful.

# 

For each of the studies, what are the different things that I might have done to extend them further:
Dot Plots: Does the range of plausible values matter? 
Choropleth Maps: Compare against typical design to see what impressions of magnitude are
Bar Charts: Test role of upper axis limit (above/below)


# Data Visualisation Literacy

# Implications

# Limitations

# Future Directions

* decision making - though correll et al use ratings, and various risk studies, incl. Stone et al. 2018
* additional factors influencing magnitude judgements
* investigating effects on magnitude of difference judgements due to extended axes. Explore dual displays e.g. Ritchie et al. (2019). 

# Closing Remarks


————————————————————————————————————————————————————————————
————————————————————————————————————————————————————————————
————————————————————————————————————————————————————————————

# Not Sure Where

Robert Kosara pie chart misconception work

Tversky (1997) argues that our representation of data is entrenched in language, cognition and culture. 

Does inclusion of a visualisation actually increase persuasion/trust/belief (Tal & Wansink) + failed replication

Bar charts not starting at zero were perceived as less credible in a student sample (Geidner and Cameron, 2017), suggesting an awareness of what constitutes a misleading design. 

Pandey et al. (2015) suggest deception occurs when an interpretation of a chart deviates from the true ‘message’. Yet this leaves open the question of what the message should be. One supposes that they are referring to the ‘ground truth’ e.g. Correll et al. (2020). 

Correll and Heer - the space of ‘black hat’ visualisation concerns design of charts with intention to mislead. This involves violating conventions, inappropriate aggregation, concealing important patterns, exploiting visual biases.

We do not passively receive the information. That which has been encoded has to be decoded.

Look at reviewer comments to see what are considered my successful contributions 

It is my thesis that interpretations of magnitude can differ according to the design of a data visualisation. Judgements pertaining to values’ magnitudes are under-explored in data visualisation research, but are relevant to interpretations and decision-making. The effect is one of context: certain aspects of a chart subtly convey the context surrounding values, giving cues to magnitude. However, it must be recognised that there are limits to a chart’s influence, and that magnitudes judgements are not wholly determined by the presentation style.

It is my thesis that the design of a visualisation can influence interpretation of plotted values’ absolute magnitudes. It is well established that design choices (e.g., y-axis truncation) affect interpretation of plotted values’ *relative* magnitudes, yet *absolute* magnitudes have been under-explored. In several controlled experiments, using a range of visualisation types I demonstrate that 

Studying mechanisms is beneficial for understanding how to design effective graphics (Elliott et al., 2020).**

Ceja et al. (2020) point out a limitation of visual reproduction methods for measuring perceived values of previously seen charts. They can only provide insight into how people *remember* values, since any bias in visual perception would apply equally during the process of visually reproducing a value through the method of adjustment. Padilla et al. (2020) - go beyond speed and accuracy measures to gain real insight into the cognitive effort required to complete a task. Index working memory through more sensitive methods: dual-task paradigm and pupilometry. 

Valuable and robust findings have been established by uncovering known psychology mechanisms, apply them to visualisation (attention for focus and declutter, curse of knowledge, datasaurus (Boger)). Other numerical biases have been studied in a datavis context (e.g. Dimara et al. 2017, 2019) Framing effects were initially primarily a reference to valence framing, but have since expanded to encompass a wider range of biases. What does this say about where we need to look for new knoweldge?

Abdul-Rahman et al. (2020) discusses the importance of controlling for non-manipulated variables. 

Pandey et al.’s (2015) interest in deception at the *message* level necessitates appropriate testing. They did not measure precision or accuracy in estimation tasks, but probed participants’ interpretations of visualisations’ overall message. 

The precision work speaks to our concern with (in)accuracy - that we might be misleading others, that others may mislead us, that charts may be weak and lack clarity. But there is so much more than simply extracting values wrong.

Anscombe’s quartet

It may seem suspect to have this level of editorial control, instead wanting to let the data speak for themselves, but considering the various rhetorical tecniques, it’s clear there is no truly objective way to present data. 

Dimara et al. (2020) point out that there has not been a lot of work investigating cognitive biases in visualisation. 

Expanding both axes in scatterplots, such that a cluster of data points appears smaller and is surrounded by blank space, increases the perceived correlation (Cleveland et al., 1982).

This directed the focus for many subsequent studies towards understanding basic processing.

Dimara et al. 2020 also discussed the notion that rational approaches shouldn’t be mistaken for biases (which necessitate genuine error) due to different perspectives.

Hullman and Diakopolous - Rhetoric

Garcia-Retamero and Galesic (2010) - however, people’s understanding of relative differences is improved when foreground information in presented within the context of background information (in bar charts and icon arrays).

**In the same study where they look at truncated line charts, Lauer and O’Brein also explore 3D pie charts which distort the actual expression of numbers. They fail to distinguish between charts that misrepresent values, and those that simply present a certain view.**

Although not empirically tested, a suggestion for avoiding misinterpretation of truncated line charts involves introducing additional blank space below plotted data (Gagnon, 2018).

However, this risks conveying the magnitude of differences as smaller, potentially diminishing viewers’ interpretations of the magnitude of important differences.**

# Cognitive Models and Visualisation Theories - where would this go? - the classic models haven’t really informed my review or experiment 

Padilla (2018) argues for greater adoption and generation of cognitive models in visualisation research. Knowledge of the cognitive mechanisms processes involved in interpreting a particular visualisation helps understand how a suitable experiment can be designed. There’s not a single process, but a combination, starting from encoding of basic visual information, top-down and bottom-up processing, through to decision-making. See Okan et al. 2012 and 2016 for overview of models

The algebriac approach has motivated the study of the effectiveness of different chart options for detecting issues during exploratory data analysis (Correll et al., 2019) and building a tool for identifying misleading visualisations through repeat testing with distinct inputs (McNutt et al., 2020).  

# Beyond Representation - not sure where this would go
Recent work by Markant et al. (2023) highlights how prior attitudes interact with data visualisation, such that even visualisations which successful updated beliefs about specific patterns in data did not alter overall attitudes. It’s not all about portraying accurate and clear plots. 

**The definition of misleading charts can be expanded even further, accounting for the fact that they can be *used* to further a point. Lisnic et al. (2023) - deception with visualisation is typically considered to occur when charts do not apply good visualisation practices. However, this paper suggests that, in their large social media sample, few charts (~12%) actually used deceptive practices. Instead, the deception was achieved by using the (properly constructed) charts to support unreliable reasoning. ‘Vulnerable’ visualisations are those that lack relevant context for exposing weaknesses in these inductive arguments. This lends support the notion that certain uses of visualisation are only misleading *in context*. The issue is *epistemological*: issues arise when the message conveyed by the visualisation is at odds with other knowledge about the world.** 

For example, Wainer (1984) presents a number of examples of bad visualisations, mostly focused on distorting data by manipulating axes, incorrectly implementing area encoding, using inconsistent baselines, etc. 

Misleading visualisation design has often been characterised as interfering with a viewer’s ability to accurate extract numerical information (Wainer et al., 1984; Rougier et al. 2014). 

Measuring objective performance (error, rt, precision) has been the focus of much research (Hegarty, 2011). 

**Lo et al.’s taxonomy tries to be more comprehensive - but ends up being too comprehensive - this illustrates that it’s important to be specific when we’re talking about deception.** Lo et al. (2022) point out that some frequently discussed misleading visualsations are rarely employed (e.g. inverted axes). They create a taxonomy of misleading visualisations, from a large sample of images associated with bad charts harvested from the internet. They identify cases where it’s not the visualisation per se, but just bad data. Some where the chart has inconsistencies or omissions - though it’s not guaranteed this is actually misleading. Others still where the chart suggests something but it isn’t plotted wrong e.g. implying causation through correlation. Argue that visualisations that are not informative are comparable to misleading visualisation, but I suggest there is a crucial difference. Not central to the main issue- maybe not too much detail required here. Nevertheless, it is important to remain cogniscant of what we mean by misleading - Lo’s defintion is too comprehensive.

Ge et al. (2023) also note that assessing response accuracy requires a specific task, so misleadingness will always be related to specific tasks.**

Brust-Renck et al. (2013) - importance of intentionally selecting a message and designing chart appropriately to convey gist. Reyna and Brainerd (2008) - Suggest it’s not just about making humans more like computers and memorising verbatim detail, it’s about making sure the gist that they do extract captures the true nature of the information. 

It’s possible that this occurs because visually representing numerators *and* denominators supports a viewer to take account of the proportion in their reasoning, helping to convey the true magnitude (Stone et al. 2018). 

Visualisations are rarely presented in isolation. Often, they contain captions, titles or annotations, and in scientific or news articles, they are embedded within a larger context. Despite this, relatively little work has explored the effect of accompanying text on the interpretation of information presented in data visualisations. 
Kim et al. (2021) - captions less likely to inform takeaway interpretations when not corresponding to salient features, so the recommendation is to make important features more salient e.g. by excluding irrelevant dates/categories, or adding annotations. 
Cheng et al. (2022) - interested in the content/style of captions. Captions referring to highly salient phenomena facilitate recall when refer to actual numbers. Captions referring to less salient phenomena facilitate recall when they refer to visual patterns.
Hearst (2023) argues that text and charts should be treated with equal importamnce.

Kindlmann and Scheidegger (2014) set out an algebraic approach to visualisation design. This concerns the nature of the correspondence between a dataset and its visual representation, and consists of three principles. Problematic visualisation practices occur when a) the degree of variation within a dataset is disproportate to the appearance of variation, b) different datasets appear indistinct, and c) altering visualisation parameters alters appearance. Yet many visualisations are agnostic about magnitude. 

It is impossible to discuss the construction of effective data visualisations without reference to data visualisation literacy. Increasing popularity and use of data visualisation has expanded the audience who are exposed to charts. This means that individuals with minimal or zero training are often expected to interpret visual displays. 

Burns et al. (2023) discusses research aimed at understanding how relatively inexperienced users understand charts. This is important, but there is inconsistency around what constitutes a novice, and also an inconsistency between general population and samples used to explore that population. The term novice is unhelpful since it forces a binary perspective onto a more nuanced picture. Why not look at individual variation in ability? Burns reports very few cases where novice was defined by literacy.
Some people are ‘novices’ - discuss the two novices papers - but there are not just two groups - literacy scales can be useful

It is useful to understand variability in biases by looking at individual differences. Get a better picture of how they might work, and the types of audience most likely to be affected. 

Visualisation literacy is the ability to comprehend data presented in visualisations (Boy et al.)