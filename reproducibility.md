# Experimental and Computational Methodology

The knowledge generated in a research project is necessarily shaped by the methods of inquiry. A recent survey of visualisation researchers revealed variation in conceptions of how progress is made in the field, with multiple approaches for generating knowledge (Correll et al., 2022). In this chapter, I discuss the epistemological approach which underlies this thesis. This provides a backdrop to the subsequent empirical work and justification for my choices. In addition, by explicitly discussing these decisions, I recognise that they inevitably influence my findings. It is necessary to acknowledge that this methodology is one of many, each carrying their own implications. This reflects the fact that an epistemological approach imposes a particular perspective, unavoidably generating a somewhat narrow view on the topic of interest.

## Experimental Psychology

Selecting a research method involves considering the most suitable type of data for addressing the research question. To understand how data visualisation design choices affect interpretations of absolute magnitude, testing hypotheses using controlled experiments is highly appropriate. This allows for systematic measurement of viewers’ judgements and isolates the graphical features of interest from extraneous features. Controlling for the influence of other variables helps establish a causal link between the manipulation and cognition (Barbosa et al., 2021). Experimental methods are well-established in visualisation research for generating robust empirical evidence on the effects of design choices (Abdul-Rahman et al., 2019). 

The purpose of psychological studies on data visualisation is to understand viewers’ interpretations. However, latent variables cannot be interrogated directly, and must be ‘operationalised’ to enable analysis. That is, interpretations of the magnitude of numerical values are captured through measurable responses which correspond to underlying mental representations. Thus, experimental methods rely on dependent variables which faithfully reflect actual cognition. Designing experiments also requires compromise between ecological validity (the degree to which the experiment reflects a realistic scenario) and experimental control (the degree to which the researcher dictates aspects of the experiment). In this thesis, I have strived for realism where possible, but have prioritised experimental control in order to ensure the robustness of findings. In visualisation research in particular, it is often necessary to control for differences in participants’ knowledge by presenting artificial or abstract data (Lam et al., 2012). Whilst qualitative studies (e.g., in-depth interviews), may produce richer data than experiments, they do not provide the precision required to systematically evaulate biases in interpretation. 

All experiments in this thesis were conducted online using Prolific.co, a website for recruiting research participants. This provided access to a diverse group of participants, which contrasts with the relative homogeniety of a student population. Furthermore, online experiments provide the ability to easily collect data from a large number of participants, which reduces the chance of generating false positives during analysis. In addition to using large participant samples, employing multiple trials per condition helps establish robust effects which are not vulnerable to the particular characteristics a single trial. Similarly, generating generalisable knowledge about mental processing of visualisations often requires multiple experiments. A single experiment is typically not sufficient for an understanding of cognitive factors in interpretation (Chen et al., 2020). 

This is a largely positivist approach, concerned with verifiable results which can be generalised beyond the experiment to describe a cognitive mechanism. However, there is also arguably a *postmodern* quality to highly controlled experiments (Mayrhofer et al., 2021). That is, a controlled experiment can be considered a constructed, stimulated setting, with contrived tasks and stimuli that do not precisely reflect the ‘reality’ under investigation (i.e., spontaneous judgements of authentic data visualisations). Recognising this does not invalidate conclusions from experimental studies, but requires that generalisation of results is treated with caution. 

## Analysis Methodology

Large quantative datasets from controlled experiments require appropriate statistical analysis. 
Judd 2017 - 
Singmann and kellen (2019) - 
Meteyard and Davis (2020) - 
modelling - Barr vs. Bates
buildmer as a solution to this debate

Effect sizes to indiciate the size of the effect - Wilkinson and Task Force on Statistical Inference(1999).

## Reproducibility

In recent years, the typical model for conducting and publishing scientific research has been intensely scrutinised. This has prompted serious concern about whether reported findings can be trusted. For example, Ioannidis (2005) estimated that published research may consist of more falsehoods than true assertions. Researchers also report that in Psychology, many studies are not equipped to generate reliable results (Fraley and Vazire, 2014) and the literature is afflicted with a high rate of false-positive findings (Simmons et al., 2011). A large-scale project performing replications of psychology experiments revealed that the evidence for many established conclusions was not as strong as initially reported (Open Science Collaboration, 2015). A survey of over 1500 researchers found widespread perception that science was facing a ‘crisis’ (Baker, 2016). However, this recognition also has provoked concerted efforts to address these problems in research, through the Open Science movement (Cruwell et al., 2019).

Recommendations for improving scientific research focus on different aspects of the research lifecycle. Improving how studies are conducted, reported, and evaluated requires targeted solutions. For example, rigorous methods and statistical analysis, facilitate researchers in generating valid conclusions. Other practices, such as openly sharing data and code, increase transparency, providing crucial insight into how these conclusions were generated (Munafo et al., 2017). Peng (2011) suggests that the ultimate test of scientific claims is *replication*. This involves independently repeating an entire empirical investigation, thus generating new data to assess consistency with an existing finding. However, this is resource-intensive. A different, albeit less rigorous, approach to evaluating scientific claims involves using a project’s original data and code to validate reported findings. If this is possible, the work is *reproducible*. By reusing existing resources, this is simpler than conducting a replication study, yet still facilitates assessment of whether reported results are reliable. *However, if researchers do not make relevant resources available, this undertaking is impossible. Research that cannot be evaluated in this way is not reproducible.*

The empirical work presented in this thesis has been conducted with a focus on ensuring reproducibility. This chapter will review published work on best practices for sharing code, data, and computational environments, and outline the approach to reproducibility employed in this thesis.

## Sharing Code and Data

There are many convincing arguments for openly sharing code and data. *Scientific approaches require that researchers can properly assess the credibility of published work (Klein et al., 2018) and can independently authenticate other researchers’ conclusions (Blischak et al., 2019). Thus, supporting third parties in reproducing research can increase perceptions of its robustness and reliability (Sandve et al., 2013). *This can also facilitate identification of errors in analysis (Klein et al., 2018). In addition to these motivating factors, authors may even appreciate the advantages of reproducible practices more than their peers (Piccolo and Frampton, 2016). For example, these practices can save time and effort (Sandve et al., 2013), and permanently sharing resources provides insurance against the loss of those resources (Klein et al., 2018).

*A textual description of analysis in a manuscript presents an incomplete and vague account of the analytical process (Piccolo and Frampton, 2016).* Sharing code helps detail the journey from the original dataset to inferential statistics (Klein et al., 2018), otherwise their software is a ‘black box’ (Morin et al., 2012). In the past, the possibility of issues or inconsistencies arising from computer code was overlooked (Plesser, 2018). However, it is now widely recognised that a computational analysis pipeline can present opportunities for error. Making code openly available permits *independent* reproduction of all computational processes (Stodden et al., 2016). This, in turn, can engender trust, promote collaboration, and facilitate new applications (Jiménez et al., 2017). *Each stage of processing must be included (Sandve et al., 2013) and any files produced using the analytical pipeline should be expendable, since reproducing them using the code supplied should be trivial (Marwick et al., 2018). For full transparency, data should be supplied in a raw, unprocessed form (White et al., 2013). *Keeping raw data separate from other files ensures that the original file is not altered and the stages of processing are clear (Marwick et al., 2018). *Other resources, such as stimuli and experiment scripts should also be shared alongside data and code (Klein et al., 2018).*

The FAIR principles (Wilkinson et al., 2016) propose that data (and metadata) should be Findable (easily discovered), Accessible (easily obtained), Interoperable (easily integrated with other tools), and Reusable (easily employed beyond their original use). FAIR principles are also relevant to other computational tools (Lamprecht et al., 2020), with similarities to Open Source Software, which does not place limits on who may examine, adapt and extend the underlying code (Jiménez et al., 2017).

*When sharing resources, a researcher’s choices can either assist or obstruct re-use (Chen et al., 2019)*. For example, using non-proprietary file types ensures that third parties can readily access resources (White et al. 2013). Rather than personal or institutional websites, independent providers (e.g., Open Science Framework) are recommended for depositing these resources (Chen et al., 2019, Klein et al., 2018). Effective documentation is also valuable. A ‘codebook’ or ‘data dictionary’ can be used to explain the contents of a data file (Klein et al., 2018), inline comments can be used to explain code (Rule et al., 2019), and a README can be used to cover elementary information such as setup instructions (Lee et al., 2018). Documentation can also provide details on data collection and known issues (White et al. 2013). Finally, licences contribute to a research project’s longevity, and provide a clear statement for third parties, ensuring that their use of resources is appropriate (Jiménez et al., 2017). Where possible, lenient licences should be employed to avoid unnecessary restrictions (White et al., 2013).

### The Importance of Public Sharing

It is fallacious to assert that if authors consistently shared data and code *on request*, freely available access would be unnecessary. To begin with, papers outlive their authors, and requests obviously cannot be fulfilled by an author after they die (Klein et al., 2018). *Empirical research further demonstrates why it is important to share resources publicly. *In a study of 204 papers from a journal which *required* authors to provide data and code on request, only 44% delivered on this promise (Stodden et al., 2018). Where research code is not publicly available, various issues preclude procurement. These include local storage failures, restrictive institutional licences, concern about potential use, and concern about labour involved in providing support (Collberg & Proebsting, 2016). *Provision of data and code on request simply cannot be guaranteed, necessitating public sharing.* In the field of data visualisation research, public sharing has historically been uncommon. Of papers submitted to the VIS 2017 conference, 15% shared materials openly and 6% shared data openly (Haroz, 2018). *Greater transparency would increase the credibility of data visualisation research and facilitate identification and rectification of issues in published work (Kosara and Haroz, 2018).*

Researchers’ working practices and technological solutions both contribute to reproducibility. Whilst it has been suggested that behaviour and technology play *equal* roles (Sandve et al. 2013), *others argue that innovations have been so effective that researchers’ engagement with these tools is now the primary challenge* (Grüning et al., 2018). *Researchers report that several factors impede or deter their sharing of research data, including lack of expertise, lack of precedent, and lack of time (Houtkoop et al., 2018).*

## Effective Programming Practices

Conducting analysis using an automated approach has three main benefits over manual processing: increased reproducibility, increased efficiency, and reduced error (Sandve et al., 2013). Writing functions in a modular style can avoid redundant repetition, promotes comprehension and supports reuse of code (Wilson et al., 2015). *Researchers should also split code into appropriate chunks which each achieve a clearly-defined goal (Rule et al., 2019). *These techniques share many similarities with the Unix philosophy (Gancarz, 2003). This approach to computer programming emphasises simplicity, modularity, and reusability.

The task of preparing data prior to analysis is an important aspect of working with data. Wickham (2014) presents a set of tools, and underlying theory for this task, arguing that analysis can be facilitated by ensuring that data is in the correct structure. This structure is known as ‘tidy’ data, which consists of a column for each variable (each type of measurement) and a row for each observation (each unit measured). A principled approach simplifies the process of creating a tidy dataset using Wickham’s functions. Because each function treats data in a standardised manner, various functions can be employed in concert. The collection of R packages containing these functions (the ‘Tidyverse’) was designed with a concern for *humans*, not just computational performance (Wickham et al., 2019), so Tidyverse-style code is likely to promote comprehension (Bertin and Baumer, 2020).

Several other coding behaviours can facilitate reproducibility. For example, *absolute* file paths refer to a specific directory on a user’s machine, which will not be replicated on other users’ machines. Using *relative* file paths, which locate files in relation to the project directory, ensure code is *portable* and can be used on any machine (Bertin and Baumer, 2020). *Additionally, independent researchers cannot successfully verify findings if only an approximate resemblance is achieved.* Therefore, for any process involving random number generation, a random seed must be specified within the script, to ensure exact reproduction of results (Sandve et al., 2013). 

### Literate Programming and Dynamic Documents

Knuth (1984) presented a novel perspective on comprehensibility in computer programming which has been influential in the literature on computational reproducibility. Knuth’s premise is that a programming script should not be regarded primarily as a set of instructions for a computer to follow, but a tool to assist humans in understanding those instructions. This approach, known as ‘literate programming’, involves pairing code with corresponding text, such that reporting and documentation are closely linked to underlying code (Sandve et al., 2013; Piccolo and Frampton, 2016). Dynamic documents allow authors to mix code and narrative within a single file, with results updated whenever the document is rendered. Producing (and re-producing) an entire manuscript using a dynamic document offers opportunities to easily observe the implementation of code used for each aspect of analysis (Peikert and Brandmeier, 2021). In addition to descriptive and inferential statistics, data visualisations may also be rendered dynamically (FitzJohn et al., 2014). This efficient format enhances transparency (Holmes et al., 2021), supports interactivity (Rule et al., 2019) and avoids errors which can occur when manually collating results (Peikert and Brandmeier, 2021). Including computationally-expensive code (e.g., complex statistical models) within a dynamic document can be problematic since this code is executed every time the document is rendered (FitzJohn et al., 2014). However, capacity for model caching provides a convenient antidote. This facilitates access to results by storing the output from models, which is then only updated when relevant data and code are updated.

## Computational Environments

Providing data and code is necessary, but not sufficient, for guaranteeing reproducibility. For example, research has found that even when the nominally required resources are available, it is not always possible to reproduce results exactly (Stodden et al., 2018), or even to execute the code (Collberg & Proebsting, 2016). In a high-profile case, a publicly-accessible Python script for processing organic chemistry data relied on the ordering of files by the Windows operating system, producing erroneous results for Linux users (Neupane et al., 2019). A study using an automated approach to test the execution of 379 Python scripts from academic research found that success depended in part on the Python version used and the presence of files capturing dependencies (Trisovic et al., 2021). Another study used a similar approach to test over 9000 R scripts (Trisovic et al., 2022). Approximately three in four scripts produced errors when executed. Implementing a code-cleaning algorithm reduced this number, but the majority (56%) still failed to run successfully. This indicates that good programming practices can improve code but cannot totally eliminate issues. Another source of error was incompatibility of R software versions and required packages. *Thus, a failure to recreate the computational environment used when originally running the script prevented successful execution.*

Peng (2011) argues that reproducibility can be characterised as a spectrum. Sharing code offers some benefits over a standalone publication, providing data increases reproducibility further, but ensuring that the code can be precisely executed is even better. Each researcher’s unique preferences and proficiencies result in roughly the same number of computational environments as individual researchers, illustrating the benefit of recording one’s computational environment (Nüst et al., 2017). Additionally, software under continuous development, such as the Tidyverse collection of packages, is frequently updated, meaning code can stop functioning unless specific versions are recorded (Holmes et al., 2021). Other software dependencies and parameter settings also complicate reproduction, requiring precision and comprehensiveness in documentation in order to achieve full *computational reproducibility* (Piccolo and Frampton, 2016).

### Capturing Computational Environments Using Containers

Like many other aspects of reproducibility, innovations in software have made it possible for researchers to capture their computational environments. R package managers, such as *renv* (Ushey, 2020) conveniently load specific package versions for individual projects. However, they do not guarantee computational reproducibility, because they do not preserve the version of R in the same way (Holmes et al., 2021) or support additional dependencies (Peikert and Brandmeier, 2021, Nüst et al., 2017). Containerisation technology offers an effective solution. A ‘container’ can capture a much greater extent of the computational environment than a package manager (Grüning et al., 2018). This technology also provides an efficient and principled approach for recreating the environment, compared to a list of instructions for manual execution (Marwick et al., 2018).

Docker (Merkel, 2014) is a popular tool for generating containers. This process begins with a Dockerfile: a text-based file which provides instructions for installing specific package versions and loading other dependencies and resources. The Dockerfile is used to build a Docker image, which captures the computational environment. When this image is running, the environment is activated, and users may interact with this environment (Nüst et al., 2020b, Boettiger and Eddelbuettel, 2017).

Collating all dependency information in a single Dockerfile provides simplicity, and ensures that the original computational environment can be reproduced even after updating the software. Since the primary objective is ensuring reproducibility, this approach prioritises openness and human readability over optimising performance (Nüst et al., 2020b, Boettiger, 2015). As well as simple implementations, complex arrangements can be accommodated, but present additional challenges. For example, dynamic document generation may also require specifying LaTeX dependencies (Boettiger, 2015).

### Rocker for Capturing R Environments

Researchers can save time and ensure consistency by using pre-existing Docker images (Nüst et al., 2020b). One particularly valuable example of this is Rocker which captures R environments for use in Docker. This tool provides portable R environments for use with a variety of systems, facilitating computational reproducibility (Boettiger and Eddelbuettel, 2014). Consequently, any researcher can execute, edit, and extend R code in a replica of the environment originally used for its development. Developing Rocker images involves a trade-off between generalisability and specificity. An image designed to be too widely applicable would be cumbersome, but images with overly-specific use cases would be hard to find (Boettiger and Eddelbuettel, 2017). The solution involves providing base images that are easily expanded for specific requirements, with various Rocker images ‘stacked’ together as required, avoiding unnecessary complexity (Nüst et al., 2020a).

### Comparing Containers with Virtual Machines

Virtual machines perform a similar function to containers. However a notable difference is that virtual machines are large, whilst containers are comparatively lightweight (Piccolo and Frampton, 2016). This difference is due to the fact that virtual machines use their own kernel, whereas containers use the operating system kernel provided by the local machine. This reduces the relative size of a container, and enhances its computational power (Cito et al., 2016). Thus, virtual machines may be considered more comprehensive than containers, offering a greater degree of separation from the characteristics of the host machine (Grüning et al., 2018, Piccolo and Frampton, 2016). However, containers are typically compatible with version control systems (Piccolo and Frampton, 2016) and offer greater transparency (Nüst et al., 2020). Furthermore, due to their modular features, making minor adaptations is trivial with a container but comparatively prolonged with a virtual machine. 

## Pragmatism Over Perfectionism

Despite the myriad recommendations for best practice, a principle often endorsed in the literature on reproducibility concerns the merits of small efforts. *Taking some steps to increase reproducibility still enhances a project’s quality compared to neglecting this aspect altogether (Piccolo and Frampton, 2016).* Withholding resources in pursuit of continuous refinement risks never sharing them at all. This fallacy is captured by the maxim ‘the best is the enemy of the good’. Analysis code does not need to be perfect in order to be useful to others (Klein et al., 2018), and it is impossible to benefit from external inquiry if the code is not shared (Barnes, 2010). Barnes (2010) argues that perceived limitations simply reflect that the code works only for the specific scenario at hand; inessential improvements are by definition not required for basic functioning. *Researchers ought to accept these limitations and share their code anyway.* In addition to code, this notion has also been applied to metadata (White et al., 2013) and containerisation (Nüst et al., 2020).

## The Approach to Reproducibility in This Thesis

The following describes the different aspects of reproducibility for the subsequent empirical studies presented in this thesis. Whilst this work does not follow a pre-defined workflow, the approach closely resembles published workflows (e.g., van Lissa et al., 2020; Peikert and Brandmeier, 2021).

### Data, Code, and Dynamic Documents

For each study, raw data is provided. The only pre-processing of this data was the essential removal of sensitive information (transparently documented in corresponding scripts). All subsequent processing, from data cleaning to data wrangling, is included in a Quarto dynamic document (Allaire, et al., 2022), which also includes all data analysis, visualisation, and accompanying text. Therefore, consistent with the principles of literate programming, textual descriptions are presented in conjunction with corresponding code (Sandve et al., 2013).

The process of selecting appropriate statistical model specifications can be opaque, involving random effects, convergence issues, and additional parameters. In the interest of transparency, consistency, and statistical rigour, I use the *buildmer* package (Voeten, 2022) to automatically determine appropriate statistical model structures. This provides a reproducible account of the steps preceding identification of each statistical model employed in analysis, reducing human error and documenting a process as well as its outcome (Rule et al., 2019). As this package is available CRAN (Comprehensible R Archive Network), its source code is archived and transparent.

### Docker Containers

Capturing dependencies requires reproduction of the computational environment used (Boettiger, 2015). Each study in this thesis is associated with a Dockerfile, which can be used to build a Docker container with the appropriate R version and package versions used during analysis. Employing Rocker images provides an Integrated Development Environment (RStudio), and speeds up construction of the Docker image. In each container, an entire manuscript can be generated from scratch. The Dockerfiles also provide important project metadata in a human- and machine-readable format (Leipzig et al., 2015).   

### Experiment Resources

In experimental psychology, sharing stimuli and experiment scripts is another important aspect of transparent research practice (Klein et al., 2018). All data visualisations shown to participants, along with all code used to generate those visualisations, has been made available. Experiments were programmed using PsychoPy, which developed as a tool for conducting open and reproducible research (Peirce et al., 2019). The underlying technology is open source, the experiment scripts use non-proprietary file formats, and the ability to specify particular software versions avoids new releases breaking older code. Its integration with GitLab version control software means that each experiment is packaged in a public online repository. An entire project’s resources can be downloaded to a local machine, and an interactive version of the experiment can be run online. 

## Conclusion

This chapter has discussed *how a lack of reproducibility in published research can reduce credibility, and has revealed how various approaches can increase reproducibility*. At the heart of these recommendations is the need to comprehensively share resources and embrace technological solutions. Making research code and raw data openly available helps an opaque analysis process to become transparent. When an entire paper’s results can be fully reproduced by an independent third party, they can be thoroughly verified.

For each empirical study in this thesis, I share raw data alongside code packaged in a dynamic document. This provides transparency, illustrating exactly how the study’s findings were generated. In addition, creating Docker containers for each study allows the analyses to be reproduced in their original computational environment. This comprehensive approach is uncommon in research on data visualisation, therefore this work serves as an example of how research in this field may be made more reproducible.

Transparent about epistemological approach.