# Experimental, Analytical, and Computational Methodology

## Introduction

The knowledge generated in a research project is necessarily shaped by the methods of inquiry. A recent survey of visualisation researchers revealed variation in conceptions of how progress is made in the field, with multiple approaches for generating knowledge [@correll_position_2022]. In this chapter, I discuss the epistemological approach which underlies this thesis. This provides a backdrop to the subsequent empirical work and a justification for my choices. It is also necessary to acknowledge that this methodology is one of many possible methodologies, and each carries its own implications. By explicitly discussing my decisions, I recognise that they inevitably influence my findings. This reflects the fact that an epistemological approach imposes a particular perspective, unavoidably generating a somewhat narrow view on the topic of interest.

## Experimental Methodology

Selecting a research method involves considering the most suitable type of data for addressing the research question. To understand how data visualisation design choices affect interpretations of absolute magnitude, testing hypotheses using controlled experiments is highly appropriate. This allows for systematic measurement of viewers’ judgements and isolates the graphical features of interest from extraneous features. Controlling for the influence of other variables helps establish a causal link between a manipulation and cognition [@barbosa_practical_2021]. Experimental methods are well-established in visualisation research for generating robust empirical evidence on the effects of design choices [@abdul-rahman_juxtaposing_2019]. 

### Experimental Design and Ecological Validity

The purpose of psychological studies on data visualisation is to develop an understanding of viewers’ interpretations. However, latent variables cannot be interrogated directly, and must be ‘operationalised’ to enable analysis. For example, interpretations of the magnitude of numerical values must be captured through measurable responses which correspond to underlying mental representations. Thus, experimental methods rely on the use of dependent variables which faithfully reflect actual cognition. Designing experiments also requires compromise between ecological validity (the degree to which the experiment reflects a realistic scenario) and experimental control (the degree to which the researcher dictates aspects of the experiment). In this thesis, I have strived for realism where possible, but have prioritised experimental control in order to ensure the robustness of findings. In visualisation research in particular, it is often necessary to control for differences in participants’ knowledge by presenting artificial or abstract data [@lam_empirical_2012]. Whilst qualitative studies (e.g., in-depth interviews), may produce richer data than experiments, they do not provide the precision required to systematically evaluate biases in interpretation. 

This is a largely positivist approach, concerned with verifiable results which can be generalised beyond the experiment to describe a cognitive mechanism. However, there is also arguably a *postmodern* quality to highly controlled experiments [@mayrhofer_practice_2021]. That is, a controlled experiment can be considered a constructed, stimulated setting, with contrived tasks and stimuli that do not precisely reflect the ‘reality’ under investigation (i.e., spontaneous judgements of authentic data visualisations). Recognising this does not invalidate conclusions from experimental studies, but requires that generalisation of results is treated with caution. 

### Sample Size and Generalisability

All experiments in this thesis were conducted online using Prolific.co, a website for recruiting research participants. This provided access to a diverse group of participants, which contrasts with the relative homogeneity of a student population, typically employed in experimental psychology research. Furthermore, online experiments provide the ability to easily collect data from a large number of participants, which reduces the chance of generating false positives during analysis. In addition to using large participant samples, employing multiple trials per condition helps establish robust effects which are not vulnerable to the particular characteristics of a single trial. Similarly, generating generalisable knowledge about mental processing of visualisations often requires multiple experiments. A single experiment is typically not sufficient for an understanding of cognitive factors in interpretation [@chen_huge_2020]. 

## Analytical Methodology

### Linear Mixed-Effects Models

Large quantitative datasets from controlled experiments require appropriate statistical analysis. Determining whether an experimental manipulation has affected participants’ interpretations involves examining variability between different experimental conditions against the background of other variability in the dataset. Linear mixed-effects modelling offers a powerful and reliable approach to this task, and is used throughout this thesis. Much like my experimental methodology, this reflects a positivist approach, wherein conclusions are supported by mathematical verification.

### Fixed Effects and Random Effects

A central aspect of mixed-effects modelling is the distinction between fixed effects and random effects. Independent variables of interest are modelled as *fixed effects*; their influence on the dependent variable is the primary focus of an analysis. Other sources of variability are modelled as *random effects* in order to generate a more comprehensive model of a dataset. For a variable manipulated in an experiment, each relevant level is present in the dataset, so this variable should be modelled as a fixed effect. However, typically, only a *sample* of all possible participants or all possible stimuli are present in a dataset, so these variables should be modelled as random effects. At a minimum, specifying a random effect involves modelling *intercepts*: the average response at each level of the random effect (i.e., a separate baseline for each individual participant or experimental stimulus). Additionally, researchers may model *slopes*: the effect of the independent variable at each level of the random effect (i.e., the difference between conditions for each individual participant or experimental stimulus). Thus, modelling random intercepts *and* random slopes attempts to capture more variability in a dataset than modelling random intercepts alone.

### Benefits of Mixed-Effects Models

Modelling random effects is beneficial as a means of testing the *generalisability* of a fixed effect: whether it is robust when differences across participants and experimental stimuli are taken into account. For example, including random effects for experimental stimuli can improve prediction of whether the results will replicate when different stimuli are used [@judd_experiments_2017]. Furthermore, it is appropriate to recognise the dependencies between data points associated with the same participant, or the same experimental item, rather than incorrectly treating them as independent observations. Compared to simpler models, better parameter estimates in mixed-effects models decrease the likelihood of generating false positives [@spieler_introduction_2019].

### Approaches to Model Construction

@barr_random_2013 argue that researchers should construct ‘maximal’ models which reflect the full complexity of their experimental design. Therefore, in a fully-crossed design where there are observations at each level of the fixed effect for each participant and each experimental stimulus, researchers should employ random intercepts and slopes for participants and experimental stimuli. @barr_random_2013 suggest that modelling all possible random effects increases statistical power without inflating Type 1 error. However, @bates_parsimonious_2018 argue for a different approach, which acknowledges that building complex mixed-effects models is a complex process, and not all datasets are sufficiently rich to support such computations. Estimating a large number of parameters using a small number of observations can result in ‘overfitting’: the resulting estimates are not always reliable. Thus, maximal model structures can be over-ambitious. Including all possible random effects terms does not necessarily improve the modelling of fixed effects.

### An Automated Approach

Whereas Barr et al.’s [-@barr_random_2013] recommendations for specifying random effects structures are primarily informed by the experimental design, Bates et al.’s [-@bates_parsimonious_2018] recommendations are primarily informed by the dataset. The approach to model construction used in this thesis is influenced by both positions, and attempts to balance simplicity and explanatory power. An additional constraint is model convergence, which refers to the process of generating a solution when building a model. In this thesis, models are constructed using a two-stage process which is automated using the *buildmer* package in R [@voeten_buildmer_2022]. First, this software attempts to build the maximal model and identify the most complex random effects structure which results in successful model convergence. Second, the software simplifies the model structure by removing random effects terms which do not contribute significantly to explaining variance in the dataset. This seeks to maximise the variability captured by the model whilst minimising the *redundant* random effects terms which may result in unreliable parameter estimates. Furthermore, using a computational process provides a consistent, rigorous, and transparent approach to model construction. A reproducible account of the steps preceding identification of each statistical model reduces the chance of human error and documents the process as well as its outcome [@rule_ten_2019].

### Reporting Analyses

In addition to the construction of robust statistical models, the *reporting* of statistical analyses is another important consideration. Statistically significant results concerning differences between experimental conditions do not indicate how *substantial* differences are, so I report effect sizes in addition to p-values and test statistics [@wilkinson_statistical_1999]. Following recommendations for mixed-effects modelling, I also report model structures alongside results, for transparency [@meteyard_best_2020].

## Reproducibility

In recent years, the typical model for conducting and publishing scientific research has been intensely scrutinised. This has prompted serious concern about the degree to which reported findings can be trusted. For example, @ioannidis_why_2005 estimated that published research may consist of more falsehoods than true assertions. Researchers also report that in the field of Psychology, many studies are not equipped to generate reliable results [@fraley_n-pact_2014] and the literature is afflicted with a high rate of false-positive findings [@simmons_false-positive_2011]. A large-scale project performing replications of psychology experiments revealed that the evidence for many established conclusions was not as strong as initially reported [@open_science_collaboration_estimating_2015]. A survey of over 1500 researchers found widespread perception that science was facing a ‘crisis’ [@baker_1500_2016]. However, this recognition also has provoked concerted efforts to address these problems in research, through the Open Science movement [@cruwell_seven_2019].

Recommendations for improving scientific research focus variously on different aspects of the research lifecycle. Improving how studies are conducted, reported, and evaluated requires targeted solutions. For example, rigorous methods and statistical analysis facilitate researchers in generating valid conclusions. Other practices, such as openly sharing data and code, increase transparency, providing crucial insight into how these conclusions were generated [@munafo_manifesto_2017]. @peng_reproducible_2011 suggests that the ultimate test of scientific claims is *replication*. This involves independently repeating an entire empirical investigation, thus generating new data to assess its consistency with an existing finding. However, this is resource-intensive. A different, albeit less rigorous, approach to evaluating scientific claims involves using a project’s original data and code to validate reported findings. If this is possible, the work is *reproducible*. By reusing existing resources, this approach is simpler than conducting a replication study, yet still facilitates assessment of whether reported results are reliable. To assist in the evaluation of research reproducibility, researchers can make relevant resources available.

Akin to robust study design and analytical methods, reproducibility also contributes to rigour in research. Adhering to the principles of reproducibility in this thesis informed decisions related to generating stimuli and analysing data, both of which influence the results of this research and ultimately the conclusions drawn. Furthermore, this work serves as an example of how other research in this field may be conducted and shared in a reproducible manner. Discussing this aspect in detail provides an account of this additional contribution. For these reasons, reproducibility has been integral to my approach to conducting research, and is therefore both philosophically and practically relevant to this thesis. The remaining contents of this chapter will review published work on best practices for sharing code, data, and computational environments, and outline the approach to reproducibility employed in this thesis.

### Sharing Code and Data

There are many convincing arguments for openly sharing code and data. Scientific approaches benefit from the capacity to thoroughly assess the credibility of published work [@klein_practical_2018] and can independently authenticate other researchers’ conclusions [@blischak_creating_2019]. Thus, supporting third parties in reproducing research can increase perceptions of its robustness and reliability [@sandve_ten_2013]. This can also facilitate identification of errors in analysis [@klein_practical_2018]. In addition to these motivating factors, authors may even appreciate the advantages of reproducible practices more than their audience [@piccolo_tools_2016]. For example, these practices can save time and effort [@sandve_ten_2013], and permanently sharing resources provides insurance against the loss of those resources [@klein_practical_2018].

A textual description of analysis in a manuscript typically presents an somewhat incomplete and imprecise account of the analytical process [@piccolo_tools_2016]. Sharing code helps detail the precise journey from the original dataset to inferential statistics [@klein_practical_2018], providing a comprehensive account of this process. In the past, the possibility of issues or inconsistencies arising from computer code was overlooked [@plesser_reproducibility_2018]. However, it is now widely recognised that a computational analysis pipeline can present opportunities for error. Making code openly available permits *independent* reproduction of all computational processes [@stodden_enhancing_2016]. This, in turn, can engender trust, promote collaboration, and facilitate new applications [@jimenez_four_2017]. Each stage of processing should be included [@sandve_ten_2013] and any files produced using the analytical pipeline should be expendable, since reproducing them using the code supplied should be trivial [@marwick_packaging_2018]. For full transparency, data should be supplied in a raw, unprocessed form [@white_nine_2013]. Keeping raw data separate from other files ensures that the original file is not altered and the stages of processing are clear [@marwick_packaging_2018]. Other resources, such as stimuli and experiment scripts should also be shared alongside data and code [@klein_practical_2018].

When sharing resources, a researcher’s choices can assist or inhibit re-use [@chen_open_2019]. For example, using non-proprietary file types ensures that third parties can readily access resources [@white_nine_2013]. Rather than personal or institutional websites, independent providers (e.g., Open Science Framework) are recommended for depositing these resources [@chen_open_2019; @klein_practical_2018]. Effective documentation is also valuable. A ‘codebook’ or ‘data dictionary’ can be used to explain the contents of a data file [@klein_practical_2018], inline comments can be used to explain code [@rule_ten_2019], and a README file can be used to cover elementary information such as setup instructions [@lee_ten_2018]. Documentation can also provide details on data collection and known issues in resources [@white_nine_2013]. Finally, licences contribute to a research project’s longevity, and provide a clear statement for third parties, ensuring that their use of resources is appropriate [@jimenez_four_2017]. Where possible, lenient licences should be employed to avoid unnecessary restrictions [@white_nine_2013].

#### The Importance of Public Sharing

It is fallacious to assert that if authors consistently shared data and code *on request*, freely available access would be unnecessary. To begin with, research papers outlive their authors, and requests obviously cannot be fulfilled by an author after they die [@klein_practical_2018]. Empirical research further demonstrates why it is valuable to share resources *publicly.* In a study of 204 papers from a journal which *required* authors to provide data and code on request, only 44% delivered on this promise [@stodden_empirical_2018]. Where research code is not publicly available, various issues preclude procurement. These include local storage failures, restrictive institutional licences, concern about potential use, and concern about labour involved in providing support [@collberg_repeatability_2016]. Provision of data and code on request simply cannot be guaranteed, motivating calls for public sharing. However, in the field of data visualisation research, public sharing has historically been uncommon. Of papers submitted to the VIS 2017 conference, 15% shared materials openly and 6% shared data openly [@haroz_open_2018]. Greater transparency could help to increase the credibility of data visualisation research and also potentially facilitate identification and rectification of issues in published work [@kosara_skipping_2018].

Researchers’ working practices and technological solutions both contribute to reproducibility. Whilst it has been suggested that behaviour and technology play *equal* roles [@sandve_ten_2013], others argue that innovations have been so effective that researchers’ engagement with these tools is now the primary driver of reproducible practices [@gruning_practical_2018]. Researchers report that several factors impede or deter their sharing of research data, including lack of expertise, lack of precedent, and lack of time [@houtkoop_data_2018].

The authors of a Royal Society report on this topic suggest that transparency is a necessary condition for improving science through openness, but argue that solely publishing data is insufficient [@boulton_science_2012]. ‘Intelligent Openness’ encapsulates additional aspects that can maximise the utility of research. Research outputs should be Accessible (can be easily discovered), Intelligible (can help make research findings comprehensible), Assessable (can be evaluated and appraised by third parties), and Usable (can be employed for different purposes by third parties). Therefore, there are a wider set of considerations that just public sharing. Pratical measures which increase *reproducibility* contribute to this goal. Likewise, the FAIR principles [@wilkinson_fair_2016] propose that data (and metadata) should be Findable (easily discovered), Accessible (easily obtained), Interoperable (easily integrated with other tools), and Reusable (easily employed beyond their original use). The FAIR principles are also relevant for other computational tools, such as analysis scripts [@lamprecht_towards_2020].

### Software for Supporting Reproducibility

Open-source software, which does not place limits on who may examine, adapt and extend the underlying code, employs a philosophy with many similarities to the FAIR principles [@jimenez_four_2017]. The availablity of source code in open-source software is a major advantage for reproducible research because it provides transparency about underlying computational processes. Other proprietary software products, such as Wolfram Mathematica, do not adopt a typical open-source development model [i.e., only employees can modify the code, @mcloone_why_2019], but still support reproducibility by offering the ability to inspect source code [@mcloone_six_2021].

Another important consideration is software sustainability and long-term availability. Like some proprietary software, some open-source software continues to be used decades after its inception [@fortunato_case_2021]. The open-source R language, used prominently for the research in this thesis, was first introduced in 1993 [@peng_r_2014]. Recent developments in prominent open-source software, such as Docker and RStudio, have been driven by commercial companies. Regardless of the business model, the permissive licensing associated with open-source software grants users modification and redistribution rights, unlike proprietary software, where proprietors retain control over its content and availability [@fortunato_case_2021]. Therefore, open-source models facilitate the ability to access current and previous versions of software, which is important for reproducibility. On the other hand, the sustainability of proprietary software is threatened if the company distributing the software ceases to operate or withdraws a product [@bretthauer_open_2001; @lundell_practitioner_2011]. Longevity is also an important aspect of platforms used for sharing research data, code, and associated resources. The Open Science Framework, which hosts the resources for the research presented in this thesis, was developed for this purpose. It is protected by a preservation fund which can sustain hosting for at least 50 years in the event that existing funding runs out [@haroz_comparison_2022]. 

#### Effective Programming Practices

Conducting analysis using a programmatical approach has three main benefits over manual processing: increased reproducibility, increased efficiency, and reduced error [@sandve_ten_2013]. Writing functions in a modular style can avoid redundant repetition, promote comprehension, and support reuse of code [@wilson_good_2017]. Similar recommendations include splitting code into appropriate chunks which each achieve a clearly-defined goal [@rule_ten_2019]. These techniques share many similarities with the Unix philosophy, an approach to computer programming which emphasises simplicity, modularity, and reusability [@gancarz_linux_2003].

The task of preparing data prior to analysis is an important aspect of working with data. @wickham_tidy_2014 presents a set of tools and underlying theory for this task, arguing that analysis can be facilitated by ensuring that data is in an appropriate structure. The recommended structure, known as ‘tidy’ data, consists of a column for each variable (each type of measurement) and a row for each observation (each unit measured). A principled approach simplifies the process of creating a tidy dataset using Wickham’s functions. Because each function treats data in a standardised manner, various functions can be employed in concert. The collection of R packages containing these functions (‘the Tidyverse’) was designed with a concern for *humans*, not just computational performance [@wickham_welcome_2019], so Tidyverse-style code is likely to promote comprehension [@bertin_creating_2021].

Several other coding behaviours can facilitate reproducibility. For example, *absolute* file paths refer to a specific directory on a user’s machine, which will not be replicated on other users’ machines. Using *relative* file paths, which locate files in relation to the project directory, ensure code is *portable* and can be used on any machine [@bertin_creating_2021]. Additionally, third parties cannot independently verify findings if only an approximate resemblance is achieved. Therefore, for any process involving random number generation, a random seed should be specified within the script, to ensure exact reproduction of results [@sandve_ten_2013]. 

#### Literate Programming and Dynamic Documents

Knuth’s [-@knuth_literate_1984] novel perspective on comprehensibility in computer programming has been influential in the literature on computational reproducibility. Knuth’s premise is that a programming script should not be regarded primarily as a set of instructions for a computer to follow, but as a tool to assist humans in understanding those instructions. This approach, known as ‘literate programming’, involves pairing code with corresponding text, such that reporting and documentation are closely linked to underlying code [@sandve_ten_2013; @piccolo_tools_2016]. Dynamic documents allow authors to mix code and narrative within a single file, with the results updated whenever the document is rendered. Producing (and re-producing) an entire manuscript using a dynamic document offers opportunities to easily observe the implementation of code used for each aspect of analysis [@peikert_reproducible_2021]. In addition to descriptive and inferential statistics, data visualisations may also be rendered dynamically [@fitzjohn_reproducible_2014]. This efficient format enhances transparency [@holmes_reproducible_2021], supports interactivity [@rule_ten_2019] and avoids errors which can occur when manually collating results [@peikert_reproducible_2021]. Furthermore, it contributes to 'Intelligent Openness' [@boulton_science_2012] by comprehensively exposing the connections between data and conclusions. Including computationally-expensive code (e.g., complex statistical models) within a dynamic document can be problematic since this code is executed every time the document is rendered [@fitzjohn_reproducible_2014]. However, capacity for model caching provides a convenient antidote. This facilitates access to results by storing the output from models, which is then only updated when relevant data and code are updated.

### Computational Environments

Providing data and code is necessary, but not sufficient, for enabling reproducibility. For example, research has found that even when the nominally required resources are available, it is not always possible to reproduce results exactly [@stodden_empirical_2018], or even to execute the code [@collberg_repeatability_2016]. In a high-profile case, a publicly-accessible Python script for processing organic chemistry data relied on the ordering of files by the Windows operating system, producing erroneous results for Linux users [@bhandari_neupane_characterization_2019]. A study using an automated approach to test the execution of 379 Python scripts from academic research found that success depended in part on the Python version used and the presence of files capturing dependencies [@trisovic_repository_2021]. Another study using a similar approach to test over 9000 R scripts found that approximately three in four scripts produced errors when executed [@trisovic_large-scale_2022]. Implementing a code-cleaning algorithm reduced this number, but the majority (56%) still failed to run successfully. This indicates that good programming practices can improve code but cannot totally eliminate issues. A remaining source of error was incompatibility between R software versions and required packages. These studies illustrate that inability to recreate the *computational environment* used when originally running a script can prevent successful execution.

@peng_reproducible_2011 argues that reproducibility can be characterised as a spectrum. Sharing code offers some benefits over a standalone publication, providing data increases reproducibility further, but ensuring that the code can be precisely executed is even better. Each researcher’s unique preferences and proficiencies result in roughly the same number of computational environments as individual researchers, illustrating the utility of recording one’s computational environment [@nust_opening_2017]. Additionally, software under continuous development, such as the Tidyverse collection of packages in R, is frequently updated, which means code can stop functioning unless specific versions are recorded [@holmes_reproducible_2021]. Other software dependencies and parameter settings also complicate reproduction, requiring precision and comprehensiveness in documentation in order to achieve full *computational reproducibility* [@piccolo_tools_2016].

#### Capturing Computational Environments Using Containers

Like many other aspects of reproducibility, innovations in software have made it possible for researchers to easily capture their computational environments. R package managers, such as *renv* [@ushey_renv_2023] conveniently load specific package versions for individual projects. However, they do not guarantee computational reproducibility, because they do not preserve the version of R in the same way [@holmes_reproducible_2021] or support additional dependencies [@peikert_reproducible_2021; @nust_opening_2017]. Containerisation technology offers an effective solution. A ‘container’ can capture a much greater extent of the computational environment than a package manager [@gruning_practical_2018]. This technology also provides an efficient and principled approach for recreating the environment, compared to a list of instructions for manual execution [@marwick_packaging_2018].

Docker [@merkel_docker_2014] is a popular tool for generating containers. This process begins with a Dockerfile: a text-based file which provides instructions for installing specific package versions and loading other dependencies and resources. The Dockerfile is used to build a Docker image, which captures the computational environment. When this image is running, the environment is activated, and users may interact with this environment [@nust_ten_2020; @boettiger_introduction_2017].

Collating all dependency information in a single Dockerfile provides simplicity, and ensures that the original computational environment can be reproduced even after updating the software on a local machine. Since the primary objective is ensuring reproducibility, this approach prioritises openness and human readability over optimising performance [@nust_ten_2020; @boettiger_introduction_2015]. As well as simple implementations, complex arrangements can be accommodated, but present additional challenges. For example, dynamic document generation may also require specifying LaTeX dependencies [@boettiger_introduction_2015].

#### Rocker for Capturing R Environments

Researchers can save time and ensure consistency by using pre-existing Docker images [@nust_ten_2020]. One particularly valuable example of this is Rocker [@boettiger_introducing_2014] which captures R environments for use in Docker. This tool provides portable R environments for use on any operating system, facilitating computational reproducibility. Consequently, any researcher can execute, edit, and extend R code in a replica of the environment originally used for its development [@boettiger_introducing_2014]. Developing Rocker images involves a trade-off between generalisability and specificity. Images designed to be too widely applicable would be cumbersome, but images with overly-specific use cases would be hard to find [@boettiger_introduction_2017]. The solution involves providing base images that are easily expanded for specific requirements, with various Rocker images ‘stacked’ together as required, avoiding unnecessary complexity [@nust_rockerverse_2020].

#### Comparing Containers with Virtual Machines

Virtual machines perform a similar function to containers. However a notable difference is that virtual machines are large, whilst containers are comparatively lightweight [@piccolo_tools_2016]. This difference is due to the fact that virtual machines use their own kernel, whereas containers use the operating system kernel provided by the local machine. This reduces the relative size of a container, and enhances its computational power [@bozzon_using_2016]. Thus, virtual machines may be considered more comprehensive than containers, offering a greater degree of separation from the characteristics of the host machine [@gruning_practical_2018; @piccolo_tools_2016]. However, containers are typically compatible with version control systems [@piccolo_tools_2016] and offer greater transparency [@nust_ten_2020]. Furthermore, due to their modular features, making minor adaptations is trivial when using a container but comparatively prolonged when using a virtual machine. 

### Pragmatism Over Perfectionism

Despite the myriad recommendations for best practice, a principle often endorsed in the literature on reproducibility concerns the merits of small efforts. Taking some steps to increase reproducibility can still enhance a project’s quality relative to overlooking this aspect altogether [@piccolo_tools_2016]. Withholding resources in pursuit of continuous refinement risks never sharing them at all. This fallacy is captured by the maxim ‘the best is the enemy of the good’. Analysis code does not need to be perfect in order to be useful to others [@klein_practical_2018], and it is not possible to benefit from external inquiry if the code is not shared [@barnes_publish_2010]. @barnes_publish_2010 argues that perceived limitations simply reflect that the code works only for the specific scenario at hand; inessential improvements are by definition not required for basic functioning. Researchers are encouraged to accept these limitations and share their code anyway. In addition to code, this notion has also been applied to metadata [@white_nine_2013] and containerisation [@nust_ten_2020].

### The Approach to Reproducibility in This Thesis

The following describes the different aspects of reproducibility present in the subsequent empirical experiments in this thesis. Whilst this work does not follow a pre-defined workflow, the approach closely resembles workflows described in published work [e.g., @van_lissa_worcs_2020; @peikert_reproducible_2021].

#### Data, Code, and Dynamic Documents

For each experiment in this thesis, raw data is provided. The only pre-processing of this data was the essential removal of sensitive information, which is transparently documented in corresponding scripts. All subsequent processing, from data cleaning to data wrangling, is included in a Quarto dynamic document [@allaire_quarto_2022], which also includes all data analysis, visualisation, and accompanying text. Therefore, consistent with the principles of literate programming, textual descriptions are presented in conjunction with corresponding code [@sandve_ten_2013]. The Quarto document associated with each empirical chapter is openly available in its corresponding online repository, but each empirical chapter in this thesis consists of the *rendered* version of the document.

#### Docker Containers

Capturing software dependencies requires reproduction of the computational environment used [@boettiger_introduction_2015]. Each empirical chapter in this thesis is associated with a Dockerfile, which can be used to build a Docker container with the appropriate R version and package versions used during analysis. Employing Rocker images provides an Integrated Development Environment (RStudio), and speeds up construction of the Docker image. In each container, the entire chapter can be generated from scratch. The Dockerfiles also provide important project metadata in a human- and machine-readable format [@leipzig_role_2021].   

The following is the code from a Dockerfile used to reproducible the computational environment for the analysis conducted in Chapter 6:

First, the code specifies the Rocker image upon which the rest of the container will be built. This image includes R (version 4.2.1), plus the RStudio Integrated Development Environment, the Quarto publishing software, and the Tidyverse packages associated with this version of R.

`FROM rocker/verse:4.2.1`


Next, files are added to the image, including Quarto dynamic document itself, the project file, and the bibliography associated with this chapter. These are mounted at the ‘rstudio’ directory.

`ADD axis-extension.qmd /home/rstudio/`

`ADD _quarto.yml /home/rstudio/`

`ADD axis-extension.bib /home/rstudio/`


Next, the contents of folders are added to the image. The first line adds the data files, the second line adds images displayed in the chapter, and the third line adds the cache containing the analysis models.

`ADD data/ /home/rstudio/data/`

`ADD images/ /home/rstudio/images/`

`ADD axis-extension_cache/html/ /home/rstudio/axis-extension_cache/html`


Next, the *renv* package is loaded and installed. This package is a package manager which captures the versions of all other packages used in a project, and their dependencies, in a *renv.lock* file. 

`RUN R -e “install.packages(‘renv’)”`

`RUN R -e “require(renv)”`


Next, the *renv.lock* file is made available to Docker, so that it can access the specific package and dependency versions used for the original analysis.

`COPY renv.lock renv.lock`


Finally, the specific version of each package is installed as specified in the *renv.lock* file. 

`RUN Rscript -e ‘options(warn = 2); renv::restore(packages = c(“ggridges”, “buildmer”, “broom.mixed”, “lme4”, “insight”, “papaja”, “magick”, “patchwork”, “ggpubr”, “kableExtra”, “emmeans”, “knitr”, “effectsize”, “qwraps2”, “report”, “MuMIn”, “shiny”, “markdown”))’`


#### Experiment Resources

In experimental psychology, sharing stimuli and experiment scripts is another important aspect of transparent research practice [@klein_practical_2018]. All data visualisations shown to participants, along with all code used to generate those visualisations, has been made available. Experiments were programmed using PsychoPy, which developed as a tool for conducting open and reproducible research [@peirce_psychopy2_2019]. The underlying technology is open-source, the experiment scripts use non-proprietary file formats, and the ability to specify particular software versions avoids new releases breaking older code. Its integration with GitLab version control software means that each experiment is packaged in a public online repository. An entire project’s resources can be downloaded to a local machine, and an interactive version of the experiment can be run online. 

## Conclusion

In this chapter, I have outlined the experimental, analytical, and computational methodology used in this thesis. This involves conducting controlled experiments in order to systematically examine the cognitive mechanisms underlying the interpretation of data visualisations. Linear mixed-effects analysis of the resulting data provides a powerful inferential approach, with a model selection algorithm providing transparency, consistency, and statistical rigour. I have also discussed academic literature which explains how comprehensively sharing resources and embracing technological solutions can increase the credibility of published research. For each empirical chapter in this thesis, I share raw data alongside code packaged in a dynamic document, which illustrates exactly how the study’s findings were generated. In addition, creating Docker containers for each study allows the analysis to be reproduced in its original computational environment. This comprehensive approach is uncommon in research on data visualisation, therefore this work may serve as an example of how research in this field can be made more transparent.
