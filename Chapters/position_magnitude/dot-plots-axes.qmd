---
title: "Magnitude Judgements Are Influenced by Data Pointsâ€™ Relative Positions Within Axis Limits"

format: pdf

params: 
  eval_models: false

knitr:
  opts_chunk: 
    cache_comments: false
    
execute:
  echo: false
  warning: false
  message: false
  include: false
      
bibliography: dot-plots-axes.bib
---

```{r}
#| label: setup

# Loading packages
library(papaja) 
library(tidyverse) 
library(ordinal) 
library(patchwork)
library(magick) 
library(markdown)
library(shiny)
library(knitr)
library(tinytex)
library(scales) 
library(buildmer) 
library(lme4)
library(broom)
library(insight)
library(kableExtra)
library(effectsize)
library(qwraps2)
library(emmeans)
library(MuMIn)
library(report)
library(markdown)
set.seed(45789) # seed for random number generation
```

```{r}
#| label: lazyload-cache
if (!params$eval_models){ 
  lazyload_cache_dir("dot-plots-axes_cache/pdf") # load cache
  #tlmgr_install('collection-fontsrecommended') # additional font collection required for Docker
  }
```

```{r}
#| label: load-data

# loading data
# see anonymisation.R for the script used to filter rejected participants and remove Prolific IDs
E1_anon <- read_csv("data/E1_anon.csv")
E2_anon <- read_csv("data/E2_anon.csv")
E3_anon <- read_csv("data/E3_anon.csv")
```

```{r}
#| label: wrangle

# perform necessary data wrangling
wrangle <- function(anon_file, .y) {
  # .y captures the index of the file in the list supplied to iwalk

# extract literacy data
# calculate literacy score (sum of five responses)
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)

# define education categories 
edu_labels <- set_names(c('No formal qualications',
                'Secondary education (e.g. GED/GCSE)',
                'High school diploma/A-levels',
                'Technical/community college',
                'Undergraduate degree (BA/BSc/other)',
                'Graduate degree (MA/MSc/MPhil/other)',
                'Doctorate degree (PhD/other)',
                'Don\'t know / not applicable'),
          seq(8,1,-1))

# extract demographics
# link slider response numbers to gender categories 
# link slider response numbers to education categories
  demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  mutate(gender_slider.response = recode(gender_slider.response, 
                                         `1` = "F", 
                                         `2` = "M", 
                                         `3` = "NB")) %>%
  mutate(across(matches("edu_slider.response"),
                ~recode(edu_slider.response, !!!edu_labels))) %>%
      select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response",
                   "edu_slider.response")))
  
# select relevant columns
# select only experimental items
# add literacy and demographic data
# change data types where appropriate
# output this file with suffix 'tidy'
  anon_file %>% 
  select(matches(c("participant", 
                   "item_no",
                   "condition",
                   "pos",
                   "orientation",
                   "chance_slider.response",
                   "severity_slider.response",
                   "chance_slider.rt",
                   "severity_slider.rt",
                   "data_mean",
                   "key_resp.rt",
                   "type",
                   "time_taken"))) %>% 
    filter(type == "E") %>%
    inner_join(literacy, by = "participant") %>%
    inner_join(demographics, by = "participant") %>%
    mutate(across(matches(c("condition", 
                            "pos", 
                            "orientation")), as_factor)) %>%
    mutate(across(c("chance_slider.response",
                  "severity_slider.response"), as.ordered)) %>%
    mutate(across(c("participant",
                  "item_no"), as.character)) %>%
    mutate(time_taken = time_taken / 60) %>%
    rename("ori"= matches("orientation")) %>%
    assign(paste0("E", .y, "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use 'wrangle' function defined above on each data file
iwalk(list(E1_anon, E2_anon, E3_anon), wrangle)

# set contrasts to facilitate statistical analysis
contrasts(E1_tidy$condition) <- matrix(c(.5, -.5))
contrasts(E2_tidy$pos) <- matrix(c(.5, -.5))
contrasts(E2_tidy$ori) <- matrix(c(.5, -.5))
contrasts(E3_tidy$condition) <- matrix(c(.5, -.5))

```

```{r}
#| label: print-formula
# Unfortunately, simplify.formula() ignores the common ordering for mixed effects models where fixed effects come first and random effects afterwards.
# This is solved by simplifying the fixed and random effects separately, then combining them.

print_formula <- function(model){

# simplify fixed effects only 
fixfx <- formula(model) %>% 
  nobars() %>%
  simplify.formula()

# simplify random effects only
ranfx <- formula(model) %>% 
  getrandom() %>%
  simplify.formula()

# combine fixed and random effects
# convert formula to a string in order to replace terms
# and add brackets to random effects
# then convert back to a formula
  merge.formula(fixfx, ranfx) %>%
  format_formula() %>%
  str_replace_all(c("chance_slider.response" = "magnitude",
                    "severity_slider.response" = "severity",
                    "pos" = "position",
                    "ori" = "orientation",
                    "condition" = "position",
                    "position \\+ orientation \\+ position:orientation" = "position * orientation", #because simplify.formula won't simplify interactions in random effects
                    "item_no" = "scenario",
                    '1' = '(1',
                  '(participant|scenario)' =  '\\1)',
                  'formula: ' = ''))
}

getrandom <- function(form) {
    
    parens <- function(x) {paste0("(",x,")")}
    onlyBars <- function(form) {
      reformulate(
        sapply(
          findbars(form), # list of character vector for each random effect
          function(x)  parens(deparse(x))), # put each character vector in brackets
        response = form[[2]]) 
    }
    
    out <- onlyBars(form)
    return(out)
}

merge.formula <- function(form1, form2, ...){
    # adapted from https://stevencarlislewalker.wordpress.com/2012/08/06/merging-combining-adding-together-two-formula-objects-in-r/
    
    # get character strings of the names for the responses 
    # (i.e. left hand sides, lhs)
    lhs1 <- deparse(form1[[2]])
    #print(lhs1)
    lhs2 <- deparse(form2[[2]])
    #print(lhs2)
    if(lhs1 != lhs2) stop('both formulas must have the same response')
    
    # get character strings of the right hand sides
    rhs1 <- strsplit(paste(form1[3]), " \\+ ")[[1]] 
    rhs2 <- strsplit(paste(form2[3]), " \\+ ")[[1]] 
    
    # put the two sides together with the amazing 
    # reformulate function
    out <- reformulate(termlabels = c(rhs1, rhs2), 
                       response = lhs1)
    
    # set the environment of the formula (i.e. where should
    # R look for variables when data aren't specified?)
    #environment(out) <- parent.frame()
    return(out)
  }
```

```{r}
#| label: comparison-function
# this function takes a model and creates a nested model with one fixed effects term removed, for anova comparison
# the default is to remove the last fixed effects term
# but a particular term can be specified use 'remove = '
comparison <- function(model, remove = NULL) {
  
  form <- formula(model)
  
  reducefixed <- function(form) {
    
    fixedfx <- 
      remove.terms(form,"placeholder") %>% # generate full formula (expand '*')
      nobars() # get formula for fixed effects only
    
    fixedterms <-  
      terms.formula(fixedfx) %>% # get terms for fixed effects
      attr("term.labels") # get character vector of fixed effects terms
    
    out <- remove.terms(fixedfx, tail(fixedterms, n=1))
    
    # remove will only take a single character string, not a character vector
    if(!is.null(remove))
      out <- remove.terms(fixedfx, remove)
    
    return(out)
  }
  
  newfixedfx <- reducefixed(form)
  fullranfx <- getrandom(form)
  merge.formula(newfixedfx, fullranfx)
  
}
```

```{r}
#| label: anova-results-function

# this function takes two nested models, runs an anova, and the outputs the Likelihood Ratio Statistic, degrees of freedom, and p value to the global environment
anova_results <- function(model, cmpr_model) {
  
  # first argument 
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
      
  anova_output <- ordinal:::anova.clm(model, cmpr_model)
  # use of ordinal:::anova.clm based on https://github.com/runehaubo/ordinal/issues/38  
  
  assign(paste0(model_name, ".LR"),
         anova_output$LR.stat[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
}
```

```{r}
#| label: summary-extract-function

# this function extracts test statistics and p values from model summaries
summary_extract <- function(model, key_term) {
  
  params <- c("statistic", "p.value", "estimate")

  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  # get the row for the chosen fixed effect term
  one_row <- tidy(model) %>% filter(term == key_term)

    get_cols <- function(param) {

    assign(value = one_row %>% pull(param),
           envir = .GlobalEnv,
           paste0(model_name, 
                  ".", 
                  str_c(unlist(
                    str_extract_all(
                      key_term,
                      "\\b\\w")),
                    collapse = ""),
                  ".",
                  param))
    }

    lapply(params, get_cols)
    
    assign(value = confint(model)[key_term,],
           envir = .GlobalEnv,
           paste0(model_name,
                  ".", 
                  str_c(unlist(
                    str_extract_all(
                      key_term,
                      "\\b\\w")),
                    collapse = ""),
                  ".CI"))

}
```

```{r}
#| label: get-contrasts-function

get_contrasts <- function(contrast_df, condition) {

  df_name <- deparse(substitute(contrast_df))

  contrast_df %>% 
    contrast("consec", 
             simple = "each", 
             combine = TRUE, 
             adjust = "sidak") %>%
    as_tibble() %>% 
    filter(!!sym(condition) != ".") %>%
    group_split(!!sym(condition)) %>%
    map(~ {
      vals <- as.list(.x)
      names(vals) <- paste0(df_name, 
                            "_",
                            pull(., {{condition}}),
                            "_", 
                            names(vals))
      list2env(vals, envir = globalenv())
    })
  
}
```

## Abstract

When visualising data, chart designers have the freedom to choose the upper and lower limits of charts' numerical axes. Axis limits can determine the physical characteristics of plotted values, such as the physical position of data points in dot plots. In three experiments (total N=420), we demonstrate that axis limits affect viewers' interpretations of the magnitudes of plotted values. Participants did not simply associate values presented at higher vertical positions with greater magnitudes. Instead, participants considered data points' relative numerical positions within the axis limits. Data points were considered to represent larger values when they were closer to the end of the axis associated with greater values, even when they were presented at the *bottom* of a chart. This provides further evidence of framing effects in the display of data, and offers insight into the cognitive mechanisms involved in assessing magnitude in data visualisations.

## Introduction

Context is crucial for effectively judging the magnitude of numbers. A 10% probability is twice as great as a 5% probability, but in the absence of context, it is unclear whether this value should be considered large or small. When referring to the chance of losing one's job, a 10% probability may be considered large, but when referring to the chance of losing a sports bet, a 10% probability may be considered small.

Contextual cues may influence interpretation of magnitude in data visualisations. One such cue is the range of values on an axis, which can serve as a frame of reference for assessing whether a data point represents a large or small number. @fig-senators-chart (a reproduction of a similar bar chart from the New York Times), which plots over time the number of Black members of the U.S. senate, provides a striking illustration. Unusually, the y-axis does not terminate just above the highest plotted value. Instead, the y-axis extends all the way to the maximum possible number of senators: 100. As a result, bars representing Black senators are confined to the very bottom, visible just above the x-axis, and a significant expanse of blank space looms above them. This framing situates plotted data points in their numerical context, thus conveying small magnitude.

It is unclear exactly how a viewer's inferences about magnitude might be influenced by axis range. Different axis limits present data points at different positions, so one possible explanation is that viewers interpret the magnitude of data points at higher positions as 'high' and those at lower positions as 'low'. Alternatively, axis limits may provide numerical context: plotted values may be judged as small in magnitude when the potential for larger values is clearly displayed. The present pair of experiments demonstrates the influence of axis limits on viewers' interpretations and explores which of these two accounts best explains how axis limits contribute to the communication of magnitude.

### Overview

In three experiments, we manipulated the axis limits surrounding plotted data. The same data points either appeared close to the upper end of an axis range, or close to the lower end. Likert scale ratings of values' magnitudes were higher when data points were positioned close to the end of the axis which was associated with higher numbers. By employing charts with conventional and inverted y-axis orientations to distinguish between possible explanations, we reveal that magnitude judgements are influenced by data points' relative positions within the axis limits.

## Related Work

### Effects of Axis Limits on Comparison Judgements

Several studies have explored the role of axis limits in data visualisation. Research has typically focused on how axis limits can alter impressions of the *difference between* presented values. For example, when axis ranges are expanded to create blank space around a cluster of data points, correlation between those points is judged as stronger [@cleveland_variables_1982]. Participants also rate the differences between values in bar charts as greater when the vertical gap between bars is larger due to a truncated y-axis [@pandey_how_2015].

Correll et al.'s [@correll_truncating_2020] experiments found that greater truncation resulted in higher effect-size judgements in both line charts and bar charts. They found no reduction in effect size judgements when truncation was communicated using graphical techniques (e.g., axis breaks and gradients). Truncation effects also persisted even when participants estimated the values of specific data points. This suggests the bias is driven by initial impressions, rather than by a misinterpretation of the values portrayed by graphical markings. The unavoidable consequence, Correll et al. suggest, is that designers' choices will influence viewers' interpretations whether axes are truncated or not.

Choosing an appropriate axis range involves a trade-off between participants' bias (over-reliance on the visual appearance of differences) and their sensitivity (capacity to identify actual differences) [@witt_graph_2019]. Just as a highly truncated y-axis can exaggerate trivial differences between values, an axis spanning the entire range of possible values can conceal important differences. Based on participants' judgements of effect size, @witt_graph_2019 found that bias was reduced and sensitivity increased when using an axis range of approximately 1.5 standard deviations of the plotted data, compared to axes which spanned only the range of the data, or the full range of possible values. This provides further evidence of a powerful association between the appearance of data, when plotted, and subjective interpretations of differences between data points.

Further evidence of truncation effects, provided by @yang_truncating_2021 improves on the design of previous studies which employed only a few observations per condition [@pandey_how_2015] or small sample sizes [@witt_graph_2019]. Participants' ratings of the difference between two bars consistently provided evidence of the exaggerating effects of y-axis truncation. @yang_truncating_2021 noted that increasing awareness does not eliminate this effect, which may function like an anchoring bias, in which numerical judgements are influenced by reference points [@tversky_judgment_1974]. Another potential explanation discussed draws upon Grice's cooperative principle [@grice_logic_1975]. According to this account of effective communication, speakers are assumed to be in cooperation, and so will communicate in a manner that is informative, truthful, relevant, and straightforward. Analogously, a viewer will assume that a numerical difference in a chart must be genuinely large if it appears large, else it would not be presented that way. Effective visualisations should be designed so a viewer's instinctive characterisation of the data corresponds closely to their interpretation following a more detailed inspection [@yang_truncating_2021].

```{r}
#| label: fig-senators-chart
#| include: true
#| out-width: "500px"
#| fig-asp: 0.7
#| fig-scap: A reproduction of a bar chart from the New York Times.
#| fig-cap: A reproduction of a bar chart from the New York Times. The y-axis limit is defined by the largest possible value, rather than the largest observed value, thus the magnitude of plotted values appears particularly small.
#| fig-alt: "A bar chart with the title 'Number of Black members of the U.S. Senate'. The y-axis ranges from 0 to 100, and the x-axis ranges from 1789 to 2022. Bars representing Black senators are mostly absent, with small bars representing a single Black senator appearing sporadicly in the late 1800s, late 1900s and early 2000s, before increasing around 2013, yet still occupying only 4% of the total at their peak. Source: Office of the Historian, History, Art & Archives, U.S. House of Representatives."

read_csv("images/senators.csv") %>%
  ggplot(aes(x = year)) +  
  geom_histogram(binwidth = 1, fill = "black") +
  labs(x = NULL,
       y = NULL,
       caption = expression(paste("Source: Office of the Historian, ", italic("History, Art & Archives, U.S. House of Representatives")))) +
  annotate("text", x=1905, y=75, 
           label="Number of Black members of the U.S. Senate",
           size = 5) +
  scale_x_continuous(breaks = seq(1800, 
                                  2020,
                                  by = 20),
                     labels = seq(1800, 
                                  2020,
                                  by = 20)) +
  scale_y_continuous(breaks = seq(0, 
                                  100,
                                  by = 10)) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(colour = "lightgrey"),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(size = 15),
        axis.text.x = element_text(size = 11),
        panel.background = element_blank()) +
  coord_cartesian(ylim = c(0, 100), 
                  xlim = c(1789, 2024),
                  clip = "off",
                  expand = FALSE) + 
  geom_hline(yintercept = 0)
```

### Effects of Axis Limits on Magnitude Judgements

The above research consistently demonstrates that the magnitude of *the difference between values* is interpreted differently depending on the axis limits employed. The present investigation is concerned with how interpretations of the magnitude of *the values themselves* are affected by a chart's design.

Empirical evidence demonstrates that judgement of a value's magnitude can depend on its relationship to a grand total or to surrounding values. This can influence interpretation of verbal approximations, and also absolute values. For example, participants instructed to take 'a few' marbles picked up more when the total number available was larger [@borges_common_1974] and rated satisfaction with the same salary as higher when it appeared in the upper end of a range, compared to the lower end [@brown_does_2008]. As well as context, vertical position also plays a role in magnitude judgements. For example, children appear to intuitively understand the relationship between height and value [@gattis_structure_2002] Both the physical world, and language (e.g., spatial metaphors), provide countless examples where 'higher' is associated with 'more', and 'lower' with 'less', and this principle has been adopted as a convention in data visualisation [@tversky_cognitive_1997].

In charts, inversions of the typical mapping between magnitude and vertical position charts can lead to misinterpretations [@okan_when_2012; @pandey_how_2015; @woodin_conceptual_2022]. Furthermore, when a company's financial performance was displayed entirely in the bottom fifth of a line chart, the company was perceived as less successful, compared to when the axis did not extend above the maximum value [@taylor_misleading_1986]. @sandman_high_1994 investigated assessments of magnitude in risk ladders, where greater risks are presented at physically higher positions on a vertical scale. Participants rated asbestos exposure as a greater threat when it was plotted at a higher position, compared to a lower position.

The above findings can be regarded as preliminary evidence that changing axis limits may affect appraisals of data points' magnitudes. However, the evidence is not substantial. @taylor_misleading_1986 did not disclose how judgements were elicited, or provide details of their sample size. @sandman_high_1994 only explored responses to one specific risk (asbestos), and each participant only took part in a single trial. The perceived threat measure was a composite of several separate ratings, preventing diagnosis of whether manipulations affected interpretations of the plotted information in particular, or just related concepts. Further, both studies introduced a confounding variable by adjusting the difference between the minimum and maximum y-axis values across conditions. Stronger evidence is required regarding how axis limits may bias inferences about magnitude, and the cognitive mechanisms involved in generating these inferences.

### Judgements of Event Outcomes

In the present study, participants viewed charts showing fictitious data on the chance of particular events occurring. This provided participants with a purpose; evaluating information about event outcomes is a more meaningful task than assessing how 'large' an abstract value is. Each value was represented using a single dot on a percentage probability scale. Our use of dot plots for conveying percentages was motivated by their simplicity and use of a single encoding channel (position), thus avoiding confounding variables from other encoding channels.

Presenting data about events with negative consequences warranted consideration of the cognitive processing of this information. These events are composed of two core components: 1) chance of occurrence and 2) outcome magnitude (severity). Individuals' assessments of chance and severity are not necessarily independent. Events are perceived as more likely when they are described as having more severe consequences [@harris_communicating_2011; @harris_estimating_2009]. In a similar manner, events are associated with more substantial consequences when they are described as more likely [@kupor_probable_2020].

One account suggests that perceptions of probability and outcome magnitude are related because they are both assumed to reflect the potency of the event's cause (probability-outcome correspondence principle; [@keren_probabilityoutcome_2001]). According to this account, probabilities can occasionally provide meaningful indications of outcome magnitude (e.g., rainfall), but it is inappropriate to apply this perspective to all situations (e.g., volcanic eruptions). Therefore, even though charts in the present study only display the *chance* of events occurring, assessments of the *severity* of events' consequences may also differ between conditions. Collecting separate judgements for chance and severity of consequences provides a clearer picture of how the manipulation affects distinct aspects of participants' representations. Our use of Likert scales (with discrete options) rather than visual analogue scales (with continuous options; @sung_visual_2018) prevents participants from simply mapping probability percentages directly onto a linear scale.

### Data Visualisation Literacy

When faced with charts that violate graphical conventions by using atypical scales, individuals with low data visualisation literacy are more likely to draw on data points' physical positions when making inferences about their magnitudes [@okan_when_2012; @okan_how_2016]. We administered Garcia-Retamero et al.'s [@garcia-retamero_measuring_2016] subjective graph literacy measure to determine whether responses to our manipulation of axis limits were associated with data visualisation literacy.

## Experiments

We conducted three experiments manipulating y-axis limits in visualisations of fictitious data. This manipulation altered the physical positions of data points in a chart, but crucially the numerical values themselves remained the same.

Experiment 1 sought to establish whether y-axis limits affected magnitude judgements. To provide context for participants, text accompanying the charts outlined (fictitious) scenarios involving a specific negative outcome (e.g., loss on financial investment, delayed flights, etc.). Three plotted data points in each chart represented the chance of the negative outcome occurring (%) for three instances associated with the scenario (e.g., three investment opportunities, three airlines, etc.).

Experiment 2 introduced another factor in addition to the manipulation of y-axis limits. Half of the visualisations presented employed inverted y-axis orientations, where data points at lower physical positions represented greater values. This 2x2 experiment allowed us to investigate whether magnitude judgements were driven by data points' absolute positions, or their relative positions within the context of the axis limits.

Experiment 3 manipulated y-axis limits in inverted charts only, providing clarity on the ambiguous results of the previous experiment. Importantly, the use of inverted charts should not be considered an endorsement (see issues above). However, they serve to distinguish between two possible explanations, since they reverse the typical associations between physical position and magnitude.

Ethical approval was granted by The University of Manchester's Division of Neuroscience & Experimental Psychology Ethics Committee (Experiment 1: Ref 2021-11115-18258; Experiment 2: Ref 2021-11115-20464; Experiment 3: Ref. 2021-11115-20745). Data, analysis code, experimental scripts, materials and a link to run the experiments are available at https://osf.io/3epm2/. We also provide all necessary resources for running a Docker container, within which the computational environment used for analysis is recreated, meaning a fully-reproducible version of this paper can be generated.

### Experiment 1

#### Method

##### Materials

###### Datasets

For each dataset, we generated three values from a normal distribution. Population means were specified manually in order to represent plausible values for the probability of the event occurring (28% - 72%). All datasets had a population standard deviation of 0.5. The same dataset was employed for both of the experimental conditions associated with a given event scenario.

###### Charts

```{r}
#| label: fig-example-charts
#| include: true
#| out-width: "500px"
#| fig-asp: 0.6
#| fig-scap: Example charts, taken from Experiment 1.
#| fig-cap: Example charts, taken from Experiment 1. The *high physical position* condition (left) presents data points near the top of the chart; the *low physical position* condition (right) presents the same data points near the bottom of the chart.
#| fig-alt: Two dot plots, where plotted data points are repsented by small black circles. In the left dot plot, the y-axis ranges from 61.5% to 71.5%, thus the three data points, which are around the 70% point, appear near the top of the axis. In the right chart, the y-axis ranges from 68.5% to 78.5%, thus the same three data points appear near the bottom of the axis. Each y-axis is labelled 'Chance (%)', accompanied by an upwards arrow. 

# combine the two images of the charts in the two separate conditions
img1 <- image_read("images/E23_hi.png")
img2 <- image_read("images/E23_lo.png")
image_append(c(img1, img2))
```

Datasets were displayed using dot plots. In experimental trials (n = 40), upper and lower axis limits were manipulated such that data points either appeared in the top third of the chart (high physical position: @fig-example-charts , left) or in the bottom third (low physical position: @fig-example-charts, right).

The y-axis range in each chart was 10 percentage points. Horizontal gridlines appeared at one-unit increments. The horizontal gridlines 1.5 units from the extremes were labelled with numerical values.

Filler trials (n = 15) and attention check trials (n = 5) presented data points in the middle third of the chart. Filler trials employed this additional variation to prevent participants from identifying the purpose of the study.

##### Procedure

The experiment was programmed in PsychoPy (version 2021.1.4, [@peirce_psychopy2_2019]) and hosted on pavlovia.org. Participants were instructed to complete the experiment on a desktop computer or laptop, not a tablet or mobile phone. Instructions explained that their task involved assessing the chance and severity of negative outcomes in various scenarios involving risks and noted that some scenarios might appear similar to other scenarios. Participants were asked to complete the task as quickly and accurately as possible. Two practice trials preceded the experiment proper.

An example of a single trial is shown in @fig-example-trial. Participants provided two responses in each trial: a rating of the chance of the negative event occurring; and a rating of the severity of the consequences if that negative event occurred. Both 7-point Likert scales had two anchors at their extremes: *'Very unlikely'* and *'Very likely'*; for the 'Chance' scale and *'Very mild'* to *'Very severe'*. for the 'Severity' scale. All other points were unlabelled. Text specified that answers should be given in response to the plotted data (e.g., *"If you camp on one of these days..."*). The term 'chance' was used instead of 'probability' to avoid confusion with the standard 0-1 scale for probabilities, and to reflect casual usage.

Participants could change their responses as many times as they wished before proceeding to the next trial, but could not return to previous trials. In attention check trials, participants were instructed not to attend to the chart, and instead to provide specified responses on the Likert scales.

Before exiting the experiment, participants were informed that all presented data were fictitious and guidance was provided in case of distress.

```{r}
#| label: fig-example-trial
#| include: true
#| out-width: "500px"
#| fig-asp: 0.625
#| fig-scap: An example trial, taken from Experiment 1.
#| fig-cap: "An example trial, taken from Experiment 1. Participants provided two ratings in each trial: the chance of an event occurring (magnitude rating), and the severity of consequences."
#| fig-alt: "A screenshot of an example trial. The title is 'Heavy Rainfall' and the introductory text reads 'You are going on a camping trip next week. The graph below shows the chance of heavy rainfall for three randomly selected days next week'. The graph shows three data points located at high physical positions on the axis. Alongside this, text reads 'If you camp on one of these days... What is the chance you experience heavy rainfall?; What is the severity of consequences if you experience heavy rainfall?'. Below the first question is a 7-point Likert scale with anchors 'Very unlikely' and 'Very likely' at the extremes, below the second question is a 7-point Likert scale with anchors 'Very mild' and 'Very severe'."

include_graphics("images/example_trial.png")
```

##### Design

We employed a repeated-measures, within-participants design. Participants encountered scenarios from experimental trials twice: once with data presented at a high physical position and once with data presented at a low physical position.

Materials were divided into two lists to minimise the likelihood of different versions of the same scenario appearing in close succession. One list contained half of the high-condition items and half of the low-condition items for the experimental scenarios. The other list contained the alternate versions of each of the experimental scenarios. Fillers and attention check questions were split between the two lists, and did not appear more than once. The order of the two lists was counterbalanced across participants. Within each list, scenarios were presented in a random order.

##### Participants

The experiments were advertised on Prolific.co, a platform for recruiting participants for online studies. Normal or corrected-to-normal vision and English fluency were required for participation.

```{r}
#| label: E1_demographics

# extract age data
age_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data
gender_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data
literacy_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy), sd = sd(literacy))

# extract timing data
# timing data was not available for one participant, and was over-estimated for another because submission was manually returned
time_E1 <- distinct(E1_tidy, participant, .keep_all = TRUE) %>% filter(time_taken < 300 & !is.na(time_taken)) %>% summarise(mean = mean(time_taken), sd = sd(time_taken))
```

Data were returned by 160 participants. Ten participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. This left a total of 150 participants whose submissions were used for analysis (`r printnum(gender_E1$M, digits = 0)`% male, `r printnum(gender_E1$F, digits = 0)`% female, `r printnum(gender_E1$NB, digits = 0)`% non-binary). Mean age was `r printnum(age_E1$mean)` (*SD* = `r printnum(age_E1$sd)`)[^1]. The mean data visualisation literacy score was `r printnum(literacy_E1$mean)` (*SD* = `r printnum(literacy_E1$sd)`), out of a maximum of 30. Participants whose submissions were approved were paid Â£3.55. Average completion time was `r printnum(time_E1$mean, digits = 0)` minutes.[^2]

[^1]: Age data were unavailable for one participant.

[^2]: Timing data were unavailable for two participants.

##### Analysis Technique

Analyses were conducted using R [version 4.2.1; @r_core_team_r_2022].

Likert scales express granularity at the level of ordinal data. They record whether one rating is higher or lower than another, but not the magnitude of this difference. Therefore, Likert scales do not necessarily capture values from latent distributions (mental representations) in a linear manner. The distance between one pair of points and another pair may appear equal, but may represent different distances on the latent distribution. Therefore, it is inappropriate to analyse Likert scale data with metric models, such as linear regression [@liddell_analyzing_2018]. Throughout this paper, we construct cumulative link mixed-effects models, using the *ordinal* package [version 2019.12-10, @christensen_ordinalregression_2019] to analyse Likert scale ratings. Odds ratio effect sizes were converted to Cohen' d values using the *effectsize* package [version 0.8.2, @ben-shachar_effectsize_2020].

Selection of model random effects structures was automated using the *buildmer* package in R [version 2.3, @voeten_buildmer_2022]. The maximal random effects structure included random intercepts for participants and scenarios, plus corresponding slopes for the position variable [@barr_random_2013]. The *buildmer* package initially identified the most complex model which could successfully converge. It subsequently removed terms which did not contribute substantially to explaining variance in ratings.

#### Results

```{r}
#| label: likert-plot-function

# this function plots the distribution of responses for ratings of data points' magnitudes (chance of negative outcomes occurring)
likert_plot <- function(df) {
  
  # select name of key IV
  IV1 <- df %>% select(matches(c("condition", "pos"))) %>% names() %>% as.name()
  
  # extract names of the conditions
  conds <- df %>% select(matches(c("condition", "pos", "ori"))) %>% names() 
  
  # get number of observations in each separate condition
  n_obs <- df %>% nrow()/(length(conds)*2)

  df %>% 
    ggplot(aes(y = {{IV1}}, fill = chance_slider.response)) +
    geom_bar(width=0.9,
             position = position_stack(reverse = TRUE)) +
    scale_fill_brewer(type = "div", palette = "RdYlGn",
                      direction = -1) +
    labs(x = NULL,
         y = NULL) +
    scale_y_discrete(labels=c("Low", "High"),
                     limits = c("lo", "hi"))  +
    scale_x_continuous(labels=c("\"Very\nunlikely\"",
                                "\"Very\nlikely\""), 
                       breaks = c(0, n_obs),
                       position = "top") +
    theme_minimal(base_size = 10) +
    theme(axis.text.x = element_text(face = "italic"),
          legend.position = "none",
          panel.grid = element_blank(),
          plot.title = element_text(hjust = 0.5)) +
    coord_fixed(ratio = n_obs/15, 
                ylim = c(0.5, 2.5), 
                xlim = c(0, n_obs+n_obs/10),
                    clip = "off", expand = FALSE) 
}
```

##### Magnitude Ratings

```{r}
#| label: fig-E1-c
#| include: true
#| out-width: "500px"
#| fig-asp: 0.35
#| fig-scap: The distribution of Likert scale ratings of the magnitude of data points.
#| fig-cap: The distribution of Likert scale ratings of the magnitude of data points. The width of each response option represents the proportion of ratings recorded for that option. Note that data points presented at high physical positions (top) elicited a larger proportion of ratings on the right-hand side (representing greater magnitudes), compared to data points at low physical positions (bottom), which elicited a larger proportion of ratings on the left-hand side (representing smaller magnitudes).
#| fig-alt: "A horizontal stacked bar chart, showing the responses at each Likert scale category for data points presented at high and low physical positions. The title reads 'Experiment 1: Ratings of Data Points' Magnitudes'. The label 'Very unlikely' appears on the far left, and the label 'Very likely' appears on the far right. For the 'high' condition, there are a greater proportion of responses at the 'Very likely' end of the scale. For the 'low' condition, there are a greater proportion of responses at the 'Very unlikely' end of the scale."

# create Likert plot for E1 data - chance (magnitude) ratings
L1 <- likert_plot(E1_tidy) +
  annotate("segment", x=400, y=3.2, xend=2700, yend=3.2,
           arrow=arrow(ends = "both", type = "closed", length = unit(7, "pt"))) +
  labs(title = "Experiment 1:\nRatings of Data Points' Magnitudes")

E1_prop <- E1_tidy %>%
  group_by(condition, chance_slider.response) %>%
  summarize(freq = n(),
            raw = round(n()/3000*100, 0),
            perc = paste0(round(n()/3000*100, 0), "%")) %>%
  ungroup() %>%
  filter(raw > 3)

L1 <- L1 + geom_text(
  data=E1_prop,
  aes(x=freq, label=(perc), group=condition),
  position = position_stack(reverse = TRUE, vjust = 0.5),
  color="black", size=2.8
)

L1
```

```{r}
#| label: E1-c
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 chance (magnitude) rating model
E1_c <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E1_tidy)
```

```{r}
#| label: E1-c-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 chance (magnitude) rating model without fixed effect
E1_c_cmpr <- clmm(comparison(E1_c),
             data = E1_tidy)
```

```{r}
#| label: E1-c-anova

# extract anova results for E1 chance (magnitude) rating models
anova_results(E1_c, E1_c_cmpr)

# summary output for E1 chance (magnitude) rating model
summary_extract(E1_c, "condition1")
```

@fig-E1-c plots the distribution of participants' ratings of the magnitude of data points. This illustrates that values presented at high physical positions elicited a greater proportion of responses at the higher end of the rating scale than values presented at low physical positions.

A likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include physical position as a fixed effect: $\chi^2$(`r in_paren(E1_c.df)`) = `r printnum(E1_c.LR)`, p `r printp(E1_c.p, add_equals = TRUE)`. Data points' magnitudes were rated as greater when those data points were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E1_c.c.statistic))`, p `r printp(E1_c.c.p.value, add_equals = TRUE)`).

The odds ratio for the difference between conditions is `r printnum(exp(E1_c.c.estimate))` (`r print_confint(exp(E1_c.c.CI))`). Participants were `r printnum(exp(E1_c.c.estimate))` times more likely to respond with a higher magnitude rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E1_c.c.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E1_c.c.estimate))))` effect size.

This model included random intercepts for each participant and each scenario. The model formula was as follows: `` `r paste(print_formula(E1_c))` ``.

##### Severity Ratings

```{r}
#| label: E1-s
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 severity rating model
E1_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E1_tidy)
```

```{r}
#| label: E1-s-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 severity rating model without fixed effect
E1_s_cmpr <- clmm(comparison(E1_s),
             data = E1_tidy)
```

```{r}
#| label: E1-s-anova

# extract anova results for E1 severity rating models
anova_results(E1_s, E1_s_cmpr)

# summary output for E1 severity rating model
summary_extract(E1_s, "condition1")
```

For ratings of the severity of consequences, a likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include condition as a fixed effect: $\chi^2$(`r in_paren(E1_s.df)`) = `r printnum(E1_s.LR)`, p `r printp(E1_s.p, add_equals = TRUE)`. The severity of consequences was rated as greater when data points representing the chance of an event occurring were presented at high physical positions, compared to when the same data points were presented at low physical positions (z = `r printnum(abs(E1_s.c.statistic))`, p `r printp(E1_s.c.p.value, add_equals = TRUE)`).

The odds ratio for the difference between conditions is `r printnum(exp(E1_s.c.estimate))` (`r print_confint(exp(E1_s.c.CI))`). Participants were `r printnum(exp(E1_s.c.estimate))` times more likely to respond with a higher severity rating to data points presented at high positions than data points presented at low positions. This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E1_s.c.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E1_s.c.estimate))))` effect size.

This model employed random intercepts for each scenario, plus random intercepts and slopes for each participant. The slopes modelled, for each participant, the average difference between responses to data presented at different positions (henceforth referred to as *by-position slopes*). The model formula was as follows: `` `r paste(print_formula(E1_s))` ``.

##### Data Visualisation Literacy

```{r}
#| label: E1-cl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E1 chance (magnitude) rating model with literacy as additional fixed effect
E1_cl <- clmm(add.terms(formula(E1_c), "literacy"),
              data = E1_tidy)
```

```{r}
#| label: E1-sl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E1 severity rating model with literacy as additional fixed effect
E1_sl <- clmm(add.terms(formula(E1_s), "literacy"),
              data = E1_tidy)
```

```{r}
#| label: E1-clint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# 
E1_clint <- buildclmm(chance_slider.response ~ condition*literacy + 
                    (1 + condition | participant) +
                    (1 + condition*literacy | item_no),
                  buildmerControl=list(                         include='chance_slider.response ~ condition*literacy'), 
                    data = E1_tidy)

emtrends(E1_clint@model, pairwise ~ condition, var = "literacy")

emmip(E1_clint@model, condition ~ literacy, cov.reduce = range)
```

```{r}
#| label: E1-slint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# 
E1_slint <- buildclmm(severity_slider.response ~ condition*literacy + 
                    (1 + condition | participant) +
                    (1 + condition*literacy | item_no),
                 buildmerControl=list(                         include='severity_slider.response ~ condition*literacy'), 
                     data = E1_tidy)

emtrends(E1_slint@model, pairwise ~ condition, var = "literacy")

emmip(E1_slint@model, condition ~ literacy, cov.reduce = range)
```

```{r}
#| label: E1-lit-summary

# summary output for E1 chance (magnitude) rating model with literacy covariate
summary_extract(E1_cl, "condition1")

# summary output for E1 severity rating model with literacy covariate
summary_extract(E1_sl, "condition1")

# summary output for E1 chance (magnitude) rating model with literacy interaction
summary_extract(E1_clint, "condition1:literacy")

# summary output for E1 severity rating model with literacy interation
summary_extract(E1_slint, "condition1:literacy")

emtrends(E1_clint@model, "condition", var = "literacy") %>% 
  as_tibble() %>% 
  group_split(condition) %>% map(~ {
    vals <- as.list(.x)
    names(vals) <- paste0("E1_clint", 
                          "_", 
                          .x$condition, 
                          "_", 
                          names(vals))
    list2env(vals, envir = globalenv())
})
```

Adjusting for participants' data visualisation literacy scores did not eliminate the effect of data points' positions on ratings of the magnitude of data points themselves (z = `r printnum(abs(E1_cl.c.statistic))`, p `r printp(E1_cl.c.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_cl.c.estimate))`, `r print_confint(exp(E1_cl.c.CI))`) or the severity of consequences (z = `r printnum(abs(E1_sl.c.statistic))`, p `r printp(E1_sl.c.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_sl.c.estimate))`, `r print_confint(exp(E1_sl.c.CI))`). These models were identical to the above models except for the inclusion of participants' subjective data visualisation literacy scores as an additional fixed effect.

However, further analysis found that, for magnitude ratings, there was an interaction between physical position and data visualisation literacy: z = `r printnum(abs(E1_clint.cl.statistic))`, p `r printp(E1_clint.cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_clint.cl.estimate))` ( `r print_confint(exp(E1_clint.cl.CI))`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E1_clint.cl.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E1_clint.cl.estimate))))` effect size. The difference between magnitude ratings for data points presented at different positions diminished as data visualisation literacy increased. This is shown in @fig-E1-clint. This model employed random intercepts and by-literacy slopes for scenarios, and random intercepts for participants. The model formula was as follows: `` `r paste(print_formula(E1_clint))` ``.

```{r}
#| label: fig-E1-clint
#| include: true
#| fig-scap: The interaction between data visualisation literacy and the physical position of data points, for magnitude ratings in Experiment 1.
#| fig-cap: "The interaction between data visualisation literacy and the physical position of data points, for magnitude ratings in Experiment 1. On the y-axis, the grey horizontal lines depict the thresholds between the seven response categories for magnitude ratings, from 'Very unlikely' to 'Very likely'. The data displayed are based on the estimated marginal means from the interaction model."
my_palette <- unname(palette.colors(palette = "Okabe-Ito")[2:3])

Thresh = E1_clint@model$Theta
Slopes = E1_clint@model$coefficients
Literacy = E1_tidy$literacy

emmip(E1_clint@model, condition ~ literacy, cov.reduce = range) +
lims(y = c(min(Thresh)-0.7,
             max(Thresh)+0.7)) +
  geom_hline(yintercept = c(unname(Thresh)), colour = "lightgrey") +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
                panel.grid.major = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  annotate("text",
           y = c(min(Thresh)-0.7,
                 max(Thresh)+0.7), 
           x = max(Literacy)-(max(Literacy)-min(Literacy))/2, 
           label = c("Very unlikely", "Very likely"),
           size = 3,
           colour = "darkgrey") +
  labs(title = "Experiment 1:\nData Visualisation Literacy x Physical Position Interaction",
       subtitle = "Estimated Marginal Means",
    x = "Data Visualisation Literacy Score",
       y = "Magnitude Rating",
       colour = "Physical Position",
    linewidth = NULL) +
  scale_color_manual(limits = c('hi', 'lo'), 
                       labels = c('High', 'Low'),
                   values = my_palette) +
  theme(axis.text.y = element_blank()) +
  geom_line(linewidth = 1.5, linetype = "solid")
```

For ratings of the severity of consequences, there was no significant interaction between physical position and data visualisation literacy: z = `r printnum(abs(E1_slint.cl.statistic))`, p `r printp(E1_slint.cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E1_slint.cl.estimate))` (`r print_confint(exp(E1_slint.cl.CI))`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E1_slint.cl.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E1_slint.cl.estimate))))` effect size. This model employed random intercepts and by-position slopes for participants, and random intercepts and by-literacy slopes for scenarios. The model formula was as follows: `` `r paste(print_formula(E1_slint))` ``.

#### Discussion

This experiment demonstrates that axis limits, which determine the position of plotted values, influence inferences about data points' magnitudes. Participants rated *the same values* as greater when these values were plotted at high positions, compared to low positions. Higher data visualisation literacy levels were associated with a reduced effect for magnitude ratings, but accounting for data visualisation did not eliminate this effect. Even though the charts only displayed data on the chance of negative outcomes occurring, ratings of severity of consequences were also greater when data points were presented at high positions. The effect on severity ratings was not associated with differences in data visualisation literacy.

### Experiment 2

#### Introduction

Experiment 1 found that participants associated data points with greater magnitudes when those data points were positioned near the *top* of a chart, compared to when the same data points were positioned near the *bottom* of a chart.

One possible explanation for this finding is that participants made simple associations between absolute position and magnitude, equating physically higher data points with larger magnitudes and physically lower data points with smaller magnitudes. This is congruent with well-established conceptual metaphors for magnitude, where greater vertical positions denote greater magnitudes [@tversky_cognitive_1997].

An alternative explanation is that participants used the y-axis as a frame of reference for assessing the magnitude of plotted values. For example, when considering data points near the bottom of the axis, participants may have recognised the potential for values larger than those observed, consequently associating plotted values with smaller magnitudes.

Experiment 1 does not provide a means of differentiating these competing explanations. Drawing inferences from data points' absolute positions would bias magnitude judgements in the same direction as drawing inferences from their relative positions. A high magnitude is implied by a data point's high physical position *and* its superior position in the context other of presented values. Therefore, further investigation is required in order to distinguish between the two competing explanations.

Plotting numerical values along the x-axis would not assist in answering this question, since values that are large in the context of the x-axis limits would be positioned on the right-hand side, which is also typically associated with larger magnitudes [@woodin_placing_2018]. However, inverting a vertical axis changes the typical relationship between physical position and numerical value: increasingly *lower* positions represent increasingly *higher* numerical values. This means data points presented near the *bottom* of a chart are numerically *larger* than the accompanying y-axis values. Therefore, inferences invoking relative numerical position would bias magnitude judgements in the opposite direction compared to inferences invoking data points' physical positions. This is illustrated in @fig-rationale.

In Experiment 2, we manipulate data points' physical positions by changing axis limits (as in Experiment 1), but *also* manipulate axis orientation, by employing conventional and inverted axes (in a 2 x 2 design). This allows us to identify the mechanism responsible for the previously observed bias in magnitude judgements. Use of absolute position would be indicated by higher magnitude ratings for data points at *high* physical positions (regardless of axis orientation). Alternatively, use of relative position would be indicated by higher magnitude ratings for data points at *high* physical positions in conventional charts and *low* physical positions in inverted charts.

Previous research suggests that charts with inverted axes can be prone to misinterpretation when viewers are not informed about the inversion [@pandey_how_2015; @woodin_conceptual_2022]. Therefore, we provided explicit instruction to ensure participants were aware that inverted charts were presented.

```{r}
#| label: fig-rationale
#| include: true
#| out-width: "500px"
#| fig-asp: 0.7
#| fig-scap: "Rationale for Experiment 2: distinguishing the roles of absolute and relative position."
#| fig-cap: "Rationale for Experiment 2: distinguishing the roles of absolute and relative position. In charts with conventional axis orientations (left column), there is congruence between data pointsâ€™ physical positions and their relative numerical positions in the chart. In charts with inverted axis orientations (right column), there is incongruence between data pointsâ€™ physical positions and their relative numerical positions in the chart. This allows us to test whether physical positions or relative numerical positions influence magnitude judgements."
#| fig-alt: "A 2x2 grid of dot plots. Where data points are plotted at a high physical position, using a conventional axis, the label reads 'Plotted values have HIGH relative position'. Where data points are plotted at a low physical position, using a conventional axis, the label reads 'Plotted values have LOW relative position'. Where data points are plotted at a high physical position, using an inverted axis, the label reads 'Plotted values have LOW relative position'. Where data points are plotted at a low physical position, using an inverted axis, the label reads 'Plotted values have HIGH relative position'."

# visualising the similarities/differences between absolute and relative position in conventional and inverted charts

include_graphics("images/fig-rationale-1.pdf")

# create my theme
# my_theme <- function() {
#   theme_minimal(base_size = 12) +
#     theme(panel.border = element_rect(fill = NA, linewidth = 1),
#           panel.grid.minor = element_blank(),
#           panel.grid.major = element_blank(),
#           axis.title.x = element_blank(),
#           axis.text.x = element_blank(),
#           axis.ticks.x = element_blank(),
#           axis.title.y = element_text(size = 12, face = "bold"),
#           axis.text.y = element_text(size = 12, colour = "black", face = "bold"),
#           plot.title = element_text(face = "bold"),
#           aspect.ratio = 1.5,
#     )
# }
# 
# # generate example data
# x <- c('A','B','C')
# y <- c(38, 38.2, 37.8)
# 
# df <- as_tibble(cbind(x, y)) %>%
#   mutate_at(vars("y"), as.numeric)
# 
# data_mean <- 38
# 
# # dataframe with four possible combinations of conditions
# id <- expand_grid(c("hi", "lo"), c("conventional", "inverted"))
# 
# create_plot <- function(pos, orientation){
# 
#   # set upper and lower limits around the population mean of the data, depending on conditions
#   lower_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean - 1.5, 
#                          pos == "hi" & orientation == "conventional" ~ data_mean - 8.5,
#                          pos == "lo" & orientation == "inverted" ~ data_mean - 8.5, 
#                          pos == "hi" & orientation == "inverted" ~ data_mean - 1.5)
#   upper_lim <- case_when(pos == "lo" & orientation == "conventional" ~ data_mean + 8.5, 
#                          pos == "hi" & orientation == "conventional" ~ data_mean + 1.5,
#                          pos == "lo" & orientation == "inverted" ~ data_mean + 1.5, 
#                          pos == "hi" & orientation == "inverted" ~ data_mean + 8.5)
#   
#   # set the values for other variables
#   
#   # y_order = the order for the two y-axis value labels (bottom, top)
#   y_order <- case_when(orientation == "conventional" ~ c(lower_lim, upper_lim),
#                        orientation == "inverted" ~ c(upper_lim, lower_lim))
#   
#   # axis_transform = reverse axis or not
#   axis_transform <- case_when(orientation == "conventional" ~ "identity",
#                               orientation == "inverted" ~ "reverse")
#   
#   # background colour
#   fill_colour <- case_when(orientation == "conventional" ~ "white",
#                            orientation == "inverted" ~ "grey")
#   
#   # opacity of dividing line
#   divider_alpha <- case_when(orientation == "conventional" ~ 1,
#                            orientation == "inverted" ~ 0)
#   
#   # text which states whether values are higher or lower than implied alternatives
#   comparison_text <- case_when(pos == "lo" & orientation == "conventional" ~ "LOW", 
#                                pos == "hi" & orientation == "conventional" ~ "HIGH",
#                                pos == "lo" & orientation == "inverted" ~ "HIGH", 
#                                pos == "hi" & orientation == "inverted" ~ "LOW")
#   
#   # position of comparison text in between data points and farthest limits
#   text_pos <- min(upper_lim, lower_lim) + 
#     (max(upper_lim, lower_lim) - min(upper_lim, lower_lim))/2
# 
#   # start and end points for direction arrow at the side
#   arrow_start <- lower_lim +2.5
#   arrow_end <- upper_lim -2.5
#   
#   # create the plot
#   g <- df %>% ggplot(aes(x = x,
#                          y = y)) + 
#     geom_point(size = 4) + 
#     ylab("") +
#     coord_cartesian(ylim = y_order, 
#                     xlim = c(0.5, 3.5), 
#                     clip = "off",
#                     expand = FALSE) +
#     geom_segment(aes(x = -0.21, xend = -0.21, # specifying the vertical arrow
#                      y = arrow_start,
#                      yend = arrow_end),
#                  arrow = arrow(length = unit(0.3,"cm")), 
#                  linewidth = 1, colour = "black") +
#         geom_segment(aes(x = -1, xend = -1, 
#                      y = -Inf,
#                      yend = Inf), alpha = divider_alpha, linewidth = 1) + 
#     geom_label(aes(x = 2, y = text_pos), label = paste(" Plotted values \nhave", comparison_text, "\n"), fill = "lightgrey") + 
#     annotate("text", x = 2, y = text_pos, label = expression(paste(italic("relative "), "position")), vjust = 2) +
#     scale_y_continuous(breaks = seq(lower_lim + 1.5, # breaks where the y-axis labels will be
#                                     upper_lim - 1.5,
#                                     by = 7),
#                        labels = percent_format(scale = 1, accuracy = 1),
#                        trans = axis_transform) + # y-axis labels
#     my_theme() +
#     theme(plot.background = element_rect (fill = fill_colour, color = 'black')) 
#   
#   
#   assign(value = g, envir = .GlobalEnv, paste0("g", 
#                                                substr(pos, 1, 1),
#                                                substr(orientation, 1, 1)))
#   
# }
# 
# # run the above function for every combination of conditions
# invisible(do.call(mapply, c(create_plot, unname(id))))
# 
# # add all four plots together, with addition y-axis labels
# ghc +  ggtitle('Conventional') + ylab("HIGH\nPhysical Position\n\n") +
#   ghi + ggtitle('Inverted') + glc + ylab("LOW\nPhysical Position\n\n") + gli 
```

#### Method

##### Materials

For this experiment, we used a Latin-squared design where participants only viewed one chart per scenario. In response to this, we increased the number of scenarios. This provided some compensation for the reduced experimental power caused by a reduction the number of observations per participant (as well as a reduction in participant numbers).

Two scenarios which were fillers in Experiment 1 were used as experimental scenarios[^3] and three additional scenarios were created. One filler scenario was removed due to a concern about its quality (it concerned the risk to others as well as the risk to oneself). This resulted in a total of 24 experimental scenarios, 12 filler scenarios, and 5 attention check questions (41 trials in total).

[^3]: For one of these scenarios, the mean of the plotted data was also modified.

##### Procedure

The experiment used PsychoPy version 2021.2.3. Participants specified the highest level of education they had received, in addition to answering demographic questions on age and gender. An additional slide in the instructions explained how to identify and interpret the different axis orientations, and encouraged participants to pay attention to this aspect of the charts:

> *You should pay attention to the direction of the arrow on the 'Chance' axis. If the arrow points upwards, the numbers in the graph get bigger as the axis goes up. Alternatively, if the arrow points downwards, the numbers get bigger as the axis goes down.*

Otherwise, the procedure was identical to Experiment 1.

##### Design

We employed a Latin-squared, within-participants design. Participants encountered each individual scenario only once, but were exposed to all combinations of physical plotting position and axis orientation throughout the experiment.

##### Participants

A viral social media post on 24th July 2021 endorsing the Prolific.co platform attracted many new users from a narrow demographic, skewing studies' participant distributions [@charalambides_we_2021]. Therefore, the experiment was not advertised to users who signed-up to Prolific.co after 24th July 2021. The experiment was also not advertised to those who had participated in Experiment 1.

```{r}
#| label: E2_demographics

# extract age data
age_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data
gender_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract education data
edu_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% group_by(edu_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% filter(edu_slider.response != 'No formal qualications' & edu_slider.response != 'Don\'t know / not applicable') %>% tally(perc)


# extract literacy data
literacy_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy), sd = sd(literacy))

# extract timing data
# timing data was not available for one participant, and was over-estimated for another because submission was manually returned
time_E2 <- distinct(E2_tidy, participant, .keep_all = TRUE) %>% filter(time_taken < 300 & !is.na(time_taken)) %>% summarise(mean = mean(time_taken), sd = sd(time_taken))
```

Data were returned by 129 participants. Per pre-registered exclusion criteria, five participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. Submissions from four other participants were excluded from the final dataset for the following reasons: maximum completion time (67 minutes) was exceeded (two participants); the submission constituted second attempt following a saving error on first attempt (one participant); data were collected prior to pre-registration (one participant). This left a total of 120 participants whose submissions were used in the analysis (`r printnum(gender_E2$M, digits = 0)`% male, `r printnum(gender_E2$F, digits = 0)`% female). Mean age was `r printnum(age_E2$mean)` (*SD* = `r printnum(age_E2$sd)`). `r printnum(edu_E2$n, digits = 0)`% had completed at least secondary education. The mean data visualisation literacy score was `r printnum(literacy_E2$mean)` (*SD* = `r printnum(literacy_E2$sd)`), out of a maximum of 30. Participants whose submissions were approved were paid Â£3.55. Average completion time was `r printnum(time_E2$mean, digits = 0)` minutes.

#### Results

##### Magnitude Ratings

```{r}
#| label: fig-E2-c
#| include: true
#| message: false
#| out-width: "500px"
#| fig-asp: 0.5
#| fig.cap: "Participants rated the chance of each negative event occurring on a 7-point Likert scale. The distribution of ratings, ranging from \"Very unlikely\" (far left, dark green) to \"Very likely\" (far right, red) is shown separately for each combination of the levels of each condition (axis orientation: conventional, inverted; data points' physical position: high, low). Note that the pattern of responses to data presented at different positions in the Conventional Axis condition appears to be the opposite to the pattern for Inverted Axis condition. When charts used conventional axes, greater magnitude ratings were more common for data presented at high physical positions, whereas when charts used inverted axes, greater magnitude ratings were more common for data presented at low physical positions."

# create likert plot for E2 data - chance (magnitude) ratings
L2 <- likert_plot(E2_tidy) + facet_wrap(~ ori, ncol = 1, labeller = labeller(ori = str_to_title)) +
  geom_segment(x=100, y=3.8, xend=650, yend=3.8,
               arrow=arrow(ends = "both", type = "closed", length = unit(7, "pt")), data = E2_tidy %>% slice_head(n = 1)) +
  labs(title = "Experiment 2:\nRatings of Data Points' Magnitudes")

r2_prop <- E2_tidy %>%
  group_by(pos, ori, chance_slider.response) %>%
  summarize(freq = n(),
            raw = round(n()/720*100, 0),
            perc = paste0(round(n()/720*100, 0), "%")) %>% ungroup() %>%
  filter(raw > 3)

L2 <- L2 + geom_text(
  data=r2_prop,
  aes(x=freq, label=(perc)),
  position = position_stack(reverse = TRUE, vjust = 0.5),
  color="black", size=2.8
)

L2
```

```{r}
#| label: E2-c
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E2 chance (magnitude) rating model
E2_c <- buildclmm(chance_slider.response ~ pos*ori +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori | item_no),
                  buildmerControl = buildmerControl(include = "pos:ori"),
                  data = E2_tidy)
```

```{r}
#| label: E2-c-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E2 chance (magnitude) rating model without fixed effect
E2_c_cmpr <- clmm(comparison(E2_c),
             data = E2_tidy)
```

```{r}
#| label: E2-c-anova

# extract anova results for E1 chance (magnitude) rating models
anova_results(E2_c, E2_c_cmpr)

# summary output for E2 chance (magnitude) rating model
summary_extract(E2_c, "pos1:ori1")
```

@fig-E2-c plots the distribution of participants' ratings of data points' magnitudes, for data points presented at high and low physical positions, in charts with conventional axis orientations and inverted axis orientations.

A likelihood ratio test reveals that a model including the interaction between physical position and axis orientation as a fixed effect explains significantly more variability in ratings than a model without this interaction as a fixed effect ($\chi^2$(`r in_paren(E2_c.df)`) = `r printnum(E2_c.LR)`, p `r printp(E2_c.p, add_equals = TRUE)`). There was a significant interaction between physical position and y-axis orientation (z = `r printnum(abs(E2_c.po.statistic))`, p `r printp(E2_c.po.p.value, add_equals = TRUE)`).

The odds ratio associated with this interaction is `r printnum(exp(E2_c.po.estimate))` (`r print_confint(exp(E2_c.po.CI))`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E2_c.po.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E2_c.po.estimate))))` effect size.

This model employed random intercepts and by-position slopes for each scenario. Random intercepts were included for each participant, as well as slopes capturing differences in participants' responses to data presented at different positions, with different orientations, and the interaction between these. The model formula was as follows: `` `r paste(print_formula(E2_c))` ``.

```{r}
#| label: E2-c-contrasts

E2_c_emm <- emmeans(E2_c@model, pairwise ~ pos * ori, adjust = 'sidak') 
  
get_contrasts(E2_c_emm, condition = "ori")
```

Pairwise comparisons (with Sidak adjustment) reveal that the effect of position in charts with conventional y-axis orientations (Experiment 1) was replicated (z = `r printnum(E2_c_emm_conventional_z.ratio)`, p `r printp(E2_c_emm_conventional_p.value, add_equals = TRUE)`). Data points' magnitudes were rated as greater when they were presented at high physical positions, compared to when they were presented at low physical positions. There was no significant difference between magnitude ratings for data points plotted at different positions when inverted axes were used (z = `r printnum(E2_c_emm_inverted_z.ratio)`, p `r printp(E2_c_emm_inverted_p.value, add_equals = TRUE)`). Therefore, we observe a different pattern of results when an inverted axis is used, compared to when a conventional axis is used. This suggests that differences in ratings for data points at different absolute positions in physical space are not due to simple associations between vertical position and magnitude.

##### Severity Ratings

```{r}
#| label: E2-s
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E2 severity rating model
E2_s <- buildclmm(severity_slider.response ~ pos*ori +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori | item_no),
                  data = E2_tidy)
```

```{r}
#| label: E2-s-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E2 severity rating model without fixed effect
E2_s_cmpr <- clmm(comparison(E2_s),
             data = E2_tidy)
```

```{r}
#| label: E2-s-anova

# extract anova results for E1 chance (magnitude) rating models
anova_results(E2_s, E2_s_cmpr)

# summary output for E2 chance (magnitude) rating model
summary_extract(E2_s, "ori1:pos1")
```

For ratings of the severity of consequences, a likelihood ratio test reveals that a model including the interaction between physical position and axis orientation as a fixed effect explains significantly more variability in ratings than a model without this interaction as a fixed effect ($\chi^2$(`r in_paren(E2_s.df)`) = `r printnum(E2_s.LR)`, p `r printp(E2_s.p, add_equals = TRUE)`). There was a significant interaction between physical position and y-axis orientation (z = `r printnum(abs(E2_s.op.statistic))`), p `r printp(E2_s.op.p.value, add_equals = TRUE)`).

The odds ratio associated with this interaction is `r printnum(exp(E2_s.op.estimate))` (`r print_confint(exp(E2_s.op.CI))`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E2_s.op.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E2_s.op.estimate))))` effect size.

This model employed random intercepts for each scenario. Random intercepts were included for each participant, as well as slopes capturing differences in participants' responses to data presented at different positions, different orientations, and the interaction between these.

```{r}
#| label: E2-s-contrasts

E2_s_emm <- emmeans(E2_s@model, pairwise ~ pos * ori, adjust = 'sidak')
  
get_contrasts(E2_s_emm, condition = "ori")
get_contrasts(E2_s_emm, condition = "pos")
```

Despite the interaction, the main effect in severity ratings from Experiment 1, different responses to data points at different positions in conventional charts, was not replicated (z = `r printnum(E2_s_emm_conventional_z.ratio)`, p `r printp(E2_s_emm_conventional_p.value, add_equals = TRUE)`). There was also no evidence of different responses to data points at different positions in inverted charts (z = `r printnum(E2_s_emm_inverted_z.ratio)`, p `r printp(E2_s_emm_inverted_p.value, add_equals = TRUE)`). This interaction appears to be driven by a weak and likely spurious difference between ratings for data points at high physical positions in inverted and conventional charts (z = `r printnum(E2_s_emm_hi_z.ratio)`, p `r printp(E2_s_emm_hi_p.value, add_equals = TRUE)`).

##### Data Visualisation Literacy

```{r}
#| label: E2-cl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E3 chance (magnitude) rating model with literacy as additional fixed effect
E2_cl <- clmm(add.terms(formula(E2_c), "literacy"),
              data = E2_tidy)
```

```{r}
#| label: E2-sl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E2 severity rating model with literacy as additional fixed effect
E2_sl <- clmm(add.terms(formula(E2_s), "literacy"),
              data = E2_tidy)
```

```{r}
#| label: E2-clint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models


# 
E2_clint <- buildclmm(chance_slider.response ~ pos*ori*literacy +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori*literacy | item_no),
                  buildmerControl=list(                         include='chance_slider.response ~ pos*ori*literacy'), 
                      data = E2_tidy)

emtrends(E2_clint@model, pairwise ~ pos:ori, var = "literacy")

emmip(E2_clint@model, pos:ori ~ literacy, cov.reduce = range)
```

```{r}
#| label: E2-slint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models


# 
E2_slint <- buildclmm(severity_slider.response ~ pos*ori*literacy +
                        (1 + pos*ori | participant) +
                        (1 + pos*ori*literacy | item_no),
                  buildmerControl=list(                         include='severity_slider.response ~ pos*ori*literacy'), 
                      data = E2_tidy)

emtrends(E2_slint@model, pairwise ~ pos:ori, var = "literacy")

emmip(E2_slint@model, pos:ori ~ literacy, cov.reduce = range)
```

```{r}
#| label: E2-lit-summary

# summary output for E2 chance (magnitude) rating model with literacy
summary_extract(E2_cl, "pos1:ori1")

# summary output for E2 severity rating model with literacy
summary_extract(E2_sl, "ori1:pos1")

summary_extract(E2_clint, "pos1:literacy")
summary_extract(E2_clint, "ori1:literacy")
summary_extract(E2_clint, "pos1:ori1:literacy")

summary_extract(E2_slint, "pos1:literacy")
summary_extract(E2_slint, "ori1:literacy")
summary_extract(E2_slint, "pos1:ori1:literacy")

emtrends(E2_clint@model, "pos", var = "literacy", infer=T) %>%
  as_tibble() %>% 
  group_split(pos) %>% map(~ {
    vals <- as.list(.x)
    names(vals) <- paste0("E2_clint", 
                          "_", 
                          .x$pos, 
                          "_", 
                          names(vals))
    list2env(vals, envir = globalenv())
})

```

Adjusting for participants' data visualisation literacy scores did not change the interaction for ratings of the magnitude of data points themselves (z = `r printnum(abs(E2_cl.po.statistic))`, p `r printp(E2_cl.po.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_cl.po.estimate))`, `r print_confint(rev(1/exp(E2_cl.po.CI)), conf.int = 0.95)`) or the severity of consequences (z = `r printnum(abs(E2_sl.op.statistic))`, p `r printp(E2_sl.op.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_sl.op.estimate))`, `r print_confint(rev(1/exp(E2_sl.op.CI)), conf.int = 0.95)`. These models were identical to the above models except for the inclusion of participants' subjective data visualisation literacy scores as an additional fixed effect.

For magnitude ratings, there was an interaction between data visualisation literacy and physical position: z = `r printnum(abs(E2_clint.pl.statistic))`, p `r printp(E2_clint.pl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_clint.pl.estimate))` (`r print_confint(rev(1/exp(E2_clint.pl.CI)), conf.int = 0.95)`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E2_clint.pl.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E2_clint.pl.estimate))))` effect size. The difference between magnitude ratings for data points presented at different positions diminished as data visualisation literacy increased. This is shown in @fig-E2-clint. There was no interaction between data visualisation literacy and axis orientation: z = `r printnum(abs(E2_clint.ol.statistic))`, p `r printp(E2_clint.ol.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_clint.ol.estimate))` (`r print_confint(rev(1/exp(E2_clint.ol.CI)), conf.int = 0.95)`. There was also no three-way interaction between data visualisation literacy, physical position, and axis orientation: z = `r printnum(abs(E2_clint.pol.statistic))`, p `r printp(E2_clint.pol.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_clint.pol.estimate))` (`r print_confint(rev(1/exp(E2_clint.pol.CI)), conf.int = 0.95)`. This model employed random intercepts for scenarios, with random slopes for position and literacy, plus random intercepts for participants, with random slopes for position, orientation, and the interaction between the position and orientation. The model formula was as follows: `` `r paste(print_formula(E2_clint))` ``.


```{r}
#| label: fig-E2-clint
#| include: true
#| fig-scap: The interaction between data visualisation literacy and physical position, for magnitude ratings in Experiment 2.
#| fig-cap: "The interaction between data visualisation literacy and physical position, for magnitude ratings in Experiment 2. On the y-axis, the grey horizontal lines depict the thresholds between the seven response categories for magnitude ratings, from 'Very unlikely' to 'Very likely'. The data displayed are based on the estimated marginal means from the interaction model."
my_palette <- unname(palette.colors(palette = "Okabe-Ito")[2:3])

Thresh = E2_clint@model$Theta
Slopes = E2_clint@model$coefficients
Literacy = E2_tidy$literacy

emmip(E2_clint@model, pos ~ literacy, cov.reduce = range) +
lims(y = c(min(Thresh)-0.7,
             max(Thresh)+0.7)) +
  geom_hline(yintercept = c(unname(Thresh)), colour = "lightgrey") +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
                panel.grid.major = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  annotate("text",
           y = c(min(Thresh)-0.7,
                 max(Thresh)+0.7), 
           x = max(Literacy)-(max(Literacy)-min(Literacy))/2, 
           label = c("Very unlikely", "Very likely"),
           size = 3,
           colour = "darkgrey") +
  labs(title = "Experiment 2:\nData Visualisation Literacy x Physical Position Interaction",
       subtitle = "Estimated Marginal Means", 
    x = "Data Visualisation Literacy Score",
       y = "Magnitude Rating",
       colour = "Physical Position",
    linewidth = NULL) +
  scale_color_manual(limits = c('hi', 'lo'), 
                       labels = c('High', 'Low'),
                   values = my_palette) +
  theme(axis.text.y = element_blank()) +
  geom_line(linewidth = 1.5, linetype = "solid")

```

For severity ratings, there was no interaction between data visualisation literacy and physical position (z = `r printnum(abs(E2_slint.pl.statistic))`, p `r printp(E2_slint.pl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_slint.pl.estimate))`, `r print_confint(rev(1/exp(E2_slint.pl.CI)), conf.int = 0.95)`), no interaction between data visualisation literacy and axis orientation (z = `r printnum(abs(E2_slint.ol.statistic))`, p `r printp(E2_slint.ol.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_slint.ol.estimate))`, `r print_confint(rev(1/exp(E2_slint.ol.CI)), conf.int = 0.95)`), and no three-way interaction between data visualisation literacy, physical position, and axis orientation (z = `r printnum(abs(E2_slint.pol.statistic))`, p `r printp(E2_slint.pol.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E2_slint.pol.estimate))`, `r print_confint(rev(1/exp(E2_slint.pol.CI)), conf.int = 0.95)`). This model employed random intercepts for scenarios, plus random intercepts for participants, with random slopes for position, orientation, and the interaction between position and orientation. The model formula was as follows: `` `r paste(print_formula(E2_slint))` ``.

#### Discussion

In Experiment 1, when using conventional charts only, we found that displaying data within different axis limits affected magnitude judgements. However, it was unclear whether judgements were based on data points' absolute physical positions, or their relative positions within axis limits, because both would generate similar interpretations. Therefore, in Experiment 2, for half of trials, we reversed the mapping of values in physical space, so these two aspects would imply different magnitudes for a given value. Ratings of the severity of consequences were not significantly affected by the position of data points representing the chance of negative events occurring. Overall, accounting for differences in participants' data visualisation literacy did not alter the pattern of results, but higher data visualisation literacy scores were associated with a diminished effect of physical position, for magnitude ratings.

In Experiment 2, we replicated the primary finding from Experiment 1. In charts with conventional axis orientations, the same data points elicited different magnitude judgements when presented at different positions. These differences were consistent with magnitudes implied by data points' absolute physical positions *and* their relative positions within axis limits. However, in charts with inverted axis orientations, we did not observe the same pattern. Therefore, we can conclude that the interpreting the magnitude of data points presented at distinct physical positions depends on how axes are oriented.

Figure @fig-E2-c suggests that the pattern of results for inverted charts is the reverse of the pattern for conventional charts. However, our analysis indicates that the same data points did not elicit significantly different magnitude judgements when presented at different positions in *inverted* charts. Therefore, we cannot conclude from this analysis that magnitude judgements are driven solely by the relative positions of data points within axis limits. This lack of significant difference is likely due to a lack of experimental power. An additional experiment is required to confirm whether there is a genuine difference.

### Experiment 3

#### Introduction

The interaction in Experiment 2 revealed that the influence of position on magnitude judgements depends on how different numerical values are arranged in a chart (axis orientation). The pattern of responses in inverted charts appeared to be the inverse of the pattern for conventional charts. This suggests that participants did not generate inferences about magnitude based on data points' absolute physical positions. However, the absence of a significant difference between ratings for data points at different positions in *inverted* charts prohibits the conclusion that interpretations are influenced by data points' relative positions within axis limits.

It is possible that no significant effect was detected for inverted charts due to insufficient experimental power. Unlike Experiment 1, which recruited 150 participants for a single-factor design, Experiment 2 recruited 120 participants for a 2x2 Latin-squared design. Despite an increase in the number of experimental scenarios (from 20 to 24), there were still fewer observations for each unique condition (3000 in Experiment 1 vs. 720 in Experiment 2).

In Experiment 3, we increase the experimental power and present inverted charts only, using the same experimental design as Experiment 1. This provides a clearer account of how magnitude is interpreted in inverted charts, furthering understanding of the mechanism by which axis limits influence interpretations of magnitude. Inferences based on data points' absolute physical positions would be indicated by higher magnitude ratings for data points at *high* physical positions (mirroring the finding for conventional charts). Alternatively, inferences based on data points' relative positions within axis limits would be indicated by higher magnitude ratings for data points at *low* physical positions (the reverse of the finding for conventional charts).

#### Method

##### Materials

Materials were identical to Experiment 1, except for the inversion of the y-axis in all charts, including practice trials.

##### Procedure

The experiment used PsychoPy version 2021.2.3. One slide in the instructions explained to participants how charts with inverted axes function: *"In all graphs in this experiment, the arrow on the 'Chance' axis points downwards, meaning the numbers get bigger as the axis goes down."*. Otherwise, the procedure was identical to Experiment 1.

##### Design

As in Experiment 1, we employed a repeated-measures, within-participants design.

##### Participants

The experiment was not advertised on Prolific.co to those who had participated in Experiment 1 or Experiment 2, or those who signed-up to Prolific.co after 24th July 2021 (due to the shift in participant demographics). Otherwise, the inclusion criteria were the same as the previous experiments.

```{r}
#| label: E3_demographics

# extract age data
age_E3 <- distinct(E3_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(age_textbox.text, na.rm = TRUE), sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data
gender_E3 <- distinct(E3_tidy, participant, .keep_all = TRUE) %>% group_by(gender_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract education data
edu_E3 <- distinct(E3_tidy, participant, .keep_all = TRUE) %>% group_by(edu_slider.response) %>% summarise(perc = n()/nrow(.)*100) %>% filter(edu_slider.response != 'No formal qualications' & edu_slider.response != 'Don\'t know / not applicable') %>% tally(perc)

# extract literacy data
literacy_E3 <- distinct(E3_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(literacy), sd = sd(literacy))

# extract timing data
time_E3 <- distinct(E3_tidy, participant, .keep_all = TRUE) %>% summarise(mean = mean(time_taken), sd = sd(time_taken))
```

Data were returned by 161 participants. Ten participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. One additional participant was excluded from the final dataset because they exceeded the maximum completion time (87 minutes). This left a total of 150 participants whose submissions were used for analysis: (`r printnum(gender_E3$M, digits = 0)`% male, `r printnum(gender_E3$F, digits = 0)`% female). Mean age was `r printnum(age_E3$mean)` (*SD* = `r printnum(age_E3$sd)`)[^4]. `r printnum(edu_E3$n, digits = 0)`% had completed at least secondary education. The mean data visualisation literacy score was `r printnum(literacy_E3$mean)` (*SD* = `r printnum(literacy_E3$sd)`). Participants whose submissions were approved were paid Â£3.45, and average completion time was `r printnum(time_E3$mean, digits = 0)` minutes.

[^4]: Age data were unavailable for two participants.

#### Results

##### Magnitude Ratings

```{r}
#| label: fig-E3-c
#| include: true
#| out-width: "500px"
#| fig-asp: 0.35
#| fig-scap: The distribution of Likert scale ratings of data points' magnitudes.
#| fig-cap: The distribution of Likert scale ratings of data points' magnitudes. The width of each response option represents the proportion of ratings recorded for that option. Note that data points presented at high physical positions (top) elicited a larger proportion of ratings on the left-hand side (representing *smaller* magnitudes), compared to data points at low physical positions (bottom), which elicited a larger proportion of ratings on the right-hand side (representing *larger* magnitudes).
#| fig-alt: "A horizontal stacked bar chart, showing the responses at each Likert scale category for data points presented at high and low physical positions. The title reads 'Experiment 3 (Inverted Axes): Ratings of Data Points' Magnitudes'. The label 'Very unlikely' appears on the far left, and the label 'Very likely' appears on the far right. For the 'high' condition, there are a greater proportion of responses at the 'Very unlikely' end of the scale. For the 'low' condition, there are a greater proportion of responses at the 'Very likely' end of the scale."

# create likert plot for E3 data - chance (magnitude) ratings
L2 <- likert_plot(E3_tidy) +
  annotate("segment", x=400, y=3.2, xend=2700, yend=3.2,
           arrow=arrow(ends = "both", type = "closed", length = unit(7, "pt"))) +
  labs(title = "Experiment 3 (Inverted Axes):\nRatings of Data Points' Magnitudes")

E3_prop <- E3_tidy %>%
  group_by(condition, chance_slider.response) %>%
  summarize(freq = n(),
            raw = round(n()/3000*100, 0),
            perc = paste0(round(n()/3000*100, 0), "%")) %>% ungroup() %>%
  filter(raw > 3)

L2 <- L2 + geom_text(
  data=E3_prop,
  aes(x=freq, label=(perc), group=condition),
  position = position_stack(reverse = TRUE, vjust = 0.5),
  color="black", size=2.8
)

L2
```

```{r}
#| label: E3-c
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 chance (magnitude) rating model
E3_c <- buildclmm(chance_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E3_tidy)
```

```{r}
#| label: E3-c-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E1 chance (magnitude) rating model without fixed effect
E3_c_cmpr <- clmm(comparison(E3_c),
             data = E3_tidy)
```

```{r}
#| label: E3-c-anova

# extract anova results for E1 chance (magnitude) rating models
anova_results(E3_c, E3_c_cmpr)

# summary output for E1 chance (magnitude) rating model
summary_extract(E3_c, "condition1")
```

@fig-E3-c plots the distribution of participants' ratings of data points' magnitudes, showing that values presented at *low* physical positions elicited a greater proportion of responses at the higher end of the rating scale than values presented at *high* physical positions.

A likelihood ratio test reveals that a model including physical position as a fixed effect explains significantly more variability in ratings than a model which does not include physical position as a fixed effect ($\chi^2$(`r in_paren(E3_c.df)`) = `r printnum(E3_c.LR)`, p `r printp(E3_c.p, add_equals = TRUE)`). Data points' magnitudes were rated as larger when those data points were presented at *low* physical positions, compared to when the same data points were presented at high physical positions, in contrast to the findings in Experiment 1 (z = `r printnum(abs(E3_c.c.statistic))`, p `r printp(E3_c.c.p.value, add_equals = TRUE)`).

The odds ratio for the difference between conditions is `r printnum(1/exp(E3_c.c.estimate))` (`r print_confint(rev(1/exp(E3_c.c.CI)), conf.int = 0.95)`). Participants were `r printnum(1/exp(E3_c.c.estimate))` times more likely to respond with a higher magnitude rating to data points presented at *low* positions than data points presented at high positions. This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(1/exp(E3_c.c.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E3_c.c.estimate))))` effect size.

This model employed random intercepts for each scenario. The model formula was as follows:  `` `r paste(print_formula(E3_c))` ``.

##### Severity Ratings

```{r}
#| label: E3-s
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E3 severity rating model
E3_s <- buildclmm(severity_slider.response ~ condition + 
                    (1 + condition | participant) +
                    (1 + condition | item_no),
                  data = E3_tidy)
```

```{r}
#| label: E3-s-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# E3 severity rating model without fixed effect
E3_s_cmpr <- clmm(comparison(E3_s),
             data = E3_tidy)
```

```{r}
#| label: E3-s-anova

# extract anova results for E1 severity rating models
anova_results(E3_s, E3_s_cmpr)

# summary output for E1 severity rating model
summary_extract(E3_s, "condition1")
```

For ratings of the severity of consequences, a likelihood ratio test reveals that a model including physical position as a fixed effect did not explain significantly more variability in ratings than a model without physical condition as a fixed effect: ($\chi^2$(`r in_paren(E3_s.df)`) = `r printnum(E3_s.LR)`, p `r printp(E3_s.p, add_equals = TRUE)`). The odds ratio for the difference between conditions is `r printnum(1/exp(E3_s.c.estimate))` (`r print_confint(rev(1/exp(E3_s.c.CI)), conf.int = 0.95)`). Participants were `r printnum(1/exp(E3_s.c.estimate))` times more likely to respond with a higher severity rating to data points presented at low positions than data points presented at high positions. This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(1/exp(E3_s.c.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E3_s.c.estimate))))` effect size.

This model employed random intercepts for each scenario, plus random intercepts and by-position slopes for each participant. The model formula was as follows: `` `r paste(print_formula(E3_s))` ``.

##### Data Visualisation Literacy

```{r}
#| label: E3-cl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E3 chance (magnitude) rating model with literacy as additional fixed effect
E3_cl <- clmm(add.terms(formula(E3_c), "literacy"),
              data = E3_tidy)
```

```{r}
#| label: E3-sl
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# generate E3 severity rating model with literacy as additional fixed effect
E3_sl <- clmm(add.terms(formula(E3_s), "literacy"),
              data = E3_tidy)
```

```{r}
#| label: E3-clint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

E3_clint <- buildclmm(chance_slider.response ~ condition*literacy + 
                    (1 + condition | participant) +
                    (1 + condition*literacy | item_no),
                  buildmerControl=list(                         include='chance_slider.response ~ condition*literacy'), 
                    data = E3_tidy)

emtrends(E3_clint@model, pairwise ~ condition, var = "literacy")

emmip(E3_clint@model, condition ~ literacy, cov.reduce = range)
```

```{r}
#| label: E3-slint
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

E3_slint <- buildclmm(severity_slider.response ~ condition*literacy + 
                    (1 + condition | participant) +
                    (1 + condition*literacy | item_no),
                  buildmerControl=list(                         include='severity_slider.response ~ condition*literacy'), 
                    data = E3_tidy)

emtrends(E3_slint@model, pairwise ~ condition, var = "literacy")

emmip(E3_slint@model, condition ~ literacy, cov.reduce = range)
```

```{r}
#| label: E3-lit-summary

# summary output for E3 chance (magnitude) rating model with literacy
summary_extract(E3_cl, "condition1")

# summary output for E3 severity rating model with literacy
summary_extract(E3_sl, "condition1")

# summary output for E3 chance (magnitude) rating model with literacy
summary_extract(E3_clint, "condition1:literacy")

# summary output for E3 severity rating model with literacy
summary_extract(E3_slint, "condition1:literacy")

emtrends(E3_clint@model, "condition", var = "literacy") %>% 
  as_tibble() %>% 
  group_split(condition) %>% map(~ {
    vals <- as.list(.x)
    names(vals) <- paste0("E3_clint", 
                          "_", 
                          .x$condition, 
                          "_", 
                          names(vals))
    list2env(vals, envir = globalenv())
})
```

Adjusting for participants' data visualisation literacy scores did not change the pattern of results regarding ratings of the magnitude of data points themselves (z = `r printnum(abs(E3_cl.c.statistic))`, p `r printp(E3_cl.c.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E3_cl.c.estimate))`, `r print_confint(rev(1/exp(E3_cl.c.CI)), conf.int = 0.95)`) or the severity of consequences (z = `r printnum(abs(E3_sl.c.statistic))`, p `r printp(E3_sl.c.p.value, add_equals = TRUE)`, odds ratio = `r printnum(1/exp(E3_sl.c.estimate))`, `r print_confint(rev(1/exp(E3_sl.c.CI)), conf.int = 0.95)`).

For magnitude ratings, there was an interaction between data visualisation literacy and physical position: z = `r printnum(abs(E3_clint.cl.statistic))`, p `r printp(E3_clint.cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E3_clint.cl.estimate))`, `r print_confint(exp(E3_clint.cl.CI))`. This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E3_clint.cl.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E3_clint.cl.estimate))))` effect size. The difference between magnitude ratings for data points presented at different positions diminished as data visualisation literacy increased. This is shown in @fig-E3-clint. This model employed random intercepts and by-literacy slopes for scenarios. The model formula was as follows: `` `r paste(print_formula(E3_clint))` ``.

```{r}
#| label: fig-E3-clint
#| include: true
#| fig-scap: The interaction between data visualisation literacy and physical position, for magnitude ratings in Experiment 3.
#| fig-cap: "The interaction between data visualisation literacy and physical position, for magnitude ratings in Experiment 3. On the y-axis, the grey horizontal lines depict the thresholds between the seven response categories for magnitude ratings, from 'Very unlikely' to 'Very likely'. The data displayed are based on the estimated marginal means from the interaction model."
my_palette <- unname(palette.colors(palette = "Okabe-Ito")[2:3])

Thresh = E3_clint@model$Theta
Slopes = E3_clint@model$coefficients
Literacy = E3_tidy$literacy

emmip(E3_clint@model, condition ~ literacy, cov.reduce = range) +
lims(y = c(min(Thresh)-0.7,
             max(Thresh)+0.7)) +
  geom_hline(yintercept = c(unname(Thresh)), colour = "lightgrey") +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
                panel.grid.major = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  annotate("text",
           y = c(min(Thresh)-0.7,
                 max(Thresh)+0.7), 
           x = max(Literacy)-(max(Literacy)-min(Literacy))/2, 
           label = c("Very unlikely", "Very likely"),
           size = 3,
           colour = "darkgrey") +
  labs(title = "Experiment 3:\nData Visualisation Literacy x Physical Position Interaction",
       subtitle = "Estimated Marginal Means", 
    x = "Data Visualisation Literacy Score",
       y = "Magnitude Rating",
       colour = "Physical Position",
    linewidth = NULL) +
  scale_color_manual(limits = c('lo', 'hi'), 
                       labels = c('Low', 'High'),
                   values = my_palette) +
  theme(axis.text.y = element_blank()) +
  geom_line(linewidth = 1.5, linetype = "solid")
```

For ratings of the severity of consequences, there was no significant interaction between physical position and data visualisation literacy: z = `r printnum(abs(E3_slint.cl.statistic))`, p `r printp(E3_slint.cl.p.value, add_equals = TRUE)`, odds ratio = `r printnum(exp(E3_slint.cl.estimate))` (`r print_confint(exp(E3_slint.cl.CI))`). This is equivalent to a Cohen's d value of `r printnum(abs(oddsratio_to_d(exp(E3_slint.cl.estimate))))`, a `r paste(interpret_cohens_d(oddsratio_to_d(exp(E3_slint.cl.estimate))))` effect size. This model employed random intercepts and by-literacy slopes for scenarios, and random intercepts and by-position slopes for participants. The model formula was as follows: `` `r paste(print_formula(E3_slint))` ``.

#### Discussion

This experiment demonstrates that inferences about data points' magnitudes are influenced by their *relative* positions on a chart's axis, rather than their absolute physical positions. Viewing data in charts with inverted y-axes, participants rated the same values as greater when these values were plotted at *low* positions, compared to high positions. Ratings of the severity of consequences were not significantly affected by the position of data points representing the chance of negative events occurring. Overall, accounting for differences in participants' data visualisation literacy did not alter the pattern of results, but higher data visualisation literacy scores were associated with a diminished effect of physical position, for magnitude ratings.

In the previous experiment (Experiment 2), we did not observe a *significant* difference between magnitude ratings for data points at different physical positions in inverted charts. However, the pattern was consistent with the inferences based on data points' relative positions within axis limits. This experiment, with increased experimental power, provides evidence for a statistically significant difference. In light of this new experiment, there is stronger evidence for the claim that axis limits influence interpretations of magnitude by determining data points' relative numerical positions.

## General Discussion

Given the use of data visualisation for reliable communication of numerical information, understanding how design choices affect interpretations is an important matter. In a pair of experiments, we demonstrate that judgements of data points' magnitudes are influenced by a chart's axis limits. These experiments provide insight into the cognitive processes involved in assessing magnitudes in data visualisations.

We manipulated the axis limits accompanying plotted data, which affected the numerical context in which data appeared *and* the physical positions of data points.

*Mention 2x2 here*

*Although a 2x2 experiment produced ambiguous results regarding interpretation of magnitude in charts with inverted y-axes, a followup...*

However, regardless of their physical positions, data points were associated with greater magnitudes when they appeared close to the end of the axis associated with higher values. Interpretation of the same absolute value is biased by its relative numerical position. This highlights viewers' sensitivity to surrounding information when assessing data. We illustrate that this framing effect occurs even when no contrasting data points are present to provide context: axis values are sufficient for informing magnitude judgements.

Our findings suggest that axis limits influence, but do not wholly *dictate*, impressions of magnitude. The distribution of magnitude judgements approximately followed the distribution of plotted numerical values, suggesting numerical values *also* contributed to magnitude judgements. In addition, the effect size associated with manipulating axis limits was larger for charts with conventional axis orientations, compared to charts with inverted y-axes. This suggests that the absolute physical position of data points partially contributed to participants' assessments. However, it is evident from the pattern of results for inverted charts that relative numerical position exerts a greater influence on magnitude judgements than absolute physical position. Whilst we cannot conclude that viewers interpret the axis range as the complete context for assessing plotted data, it is clear that axes inform magnitude judgements by defining plotted values' relative positions.

### Relationship to Prior Work

The present data complement findings from research on y-axis truncation, which has observed that axis limits accompanying plotted values can influence viewers' impressions of those values. While previous investigations have shown that *comparisons* of plotted values are affected by y-axis limits [@pandey_how_2015; @correll_truncating_2020; @witt_graph_2019; @yang_truncating_2021], the present findings show that they also influence *magnitude judgements*. This finding supports the notion that viewers are sensitive to visualisation rhetoric, which involves provoking a specific interpretation through a particular presentation of numerical information [@hullman_visualization_2011].

A previous study addressing a similar question also concluded that a data point's location within a range of values affects interpretation of its magnitude [@sandman_high_1994]. The present study builds upon this research by identifying the mechanism behind this effect and removing the confound of variable axes ranges. It also extends the finding beyond a single scenario (asbestos) to a wider range of situations. By analysing different types of judgement separately, rather than using a combined measure, we verify that axis limits affect interpretations of the specific variable displayed in a chart.

In addition to the conceptual metaphor for magnitude, physical positions are also linked to *emotional valence* (where high positions are associated with positive valence). [@woodin_conceptual_2022] found that physical arrangements of data consistent with the conceptual metaphor for valence somewhat facilitate comprehension, but that associations between position and magnitude affect interpretations more strongly. Visualisations in the present experiments displayed data on negative events, so data were aligned with the conceptual metaphor for valence in inverted charts, and misaligned in conventional charts. Participants evidently did not use valence metaphors to interpret values in conventional charts; this would have produced the opposite pattern of results to those observed. The simplest explanation for our results suggests that participants relied on relative position when interpreting both conventional and inverted charts, rather than sometimes generating inferences based on a conceptual metaphor for valence.

### Additional Findings

Prior research has observed positive correlations between participants' perceptions of event probability and outcome magnitude [@harris_communicating_2011; @harris_estimating_2009; @kupor_probable_2020]. We did not find robust evidence that assessments of the severity of consequences were affected by our manipulation of data points representing the chance of events occurring. However, whereas prior work substantially manipulated underlying scenarios, our subtler manipulation retained the same probability values, changing only the surrounding context. In addition, participants evaluated the severity of an event's *consequences*, which is one step removed from the property explored in prior research: the potency of the event itself. The effects of axis limits on interpretation of data about the incidence of events do not reliably extend to judgements about their consequences.

Adjusting for data visualisation literacy did not eliminate the influence of axis limits on interpretations. @yang_truncating_2021 also observed that data visualisation literacy could not sufficiently explain variance in the degree of bias caused by y-axis truncation. This measure reflects comprehension of the conventions of data visualisation, indicating receipt of elementary instruction [@okan_how_2016]. Therefore, it is perhaps better suited for capturing viewers' application of basic knowledge in interpretation [@yang_truncating_2021], whereas Ge et al.'s CALVI test [@ge_calvi_2023] may be more appropriate for predicting susceptibility to differences in presentation format.

### Limitations and Future Directions

We employed inverted y-axes solely for the purpose of distinguishing competing explanations; their use should not be considered an endorsement. To avoid misinterpretations, participants were given instruction on how to read inverted charts. With this explicit instruction, our data provide evidence *contrary* to the typical finding of misinterpretation resulting from associating higher positions with higher values [@woodin_conceptual_2022; @pandey_how_2015]. However, this instruction may have suppressed a spontaneous interpretation of magnitude, based on physical position, in favour of a learned interpretation. Our investigation therefore only explores the cognitive processing associated with assessing magnitude in charts which viewers know how to read.

This study was designed to explore one factor involved in assessing magnitude. The influence of axis limits on interpretations was relatively small, raising the question of how much this factor influences real-world decision-making and behaviour. A forced choice measure, or response scale with concrete values, would be suitable for capturing these outcomes in future work. Addressing this question will help to quantify how much a designer's choice of axis limits affects a viewer's choices and actions. An appreciable effect would have implications for visualisation design, suggesting use of axis limits which convey magnitude appropriately to avoid misleading users. Suitable axis limits cannot be objectively determined, but must be informed by the designer, based on their assessment of the data [@correll_truncating_2020]. The effects of axis range on discrimination ability would also warrant consideration, taking account of the intended application.

### Conclusion

We conducted two experiments investigating how axis limits inform interpretations of plotted values' magnitudes. Participants' subjective judgements were affected by data points' positions in relation to accompanying axis limits. The association between data points' positions and magnitude judgements critically depends on whether plotted data appear closer to the axis limit associated with higher or lower values. The cognitive processes associated with assessing magnitude in data visualisations involve taking into account the numerical context in which the data appear.