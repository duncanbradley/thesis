{
  "hash": "126bcd2ec37d6840efe389cf47eae953",
  "result": {
    "markdown": "---\ntitle: \"Magnitude Judgements Are Influenced by Data Pointsâ€™ Relative Positions Within Axis Limits\"\n\nformat: pdf\n\nparams: \n  eval_models: true\n\nknitr:\n  opts_chunk: \n    cache_comments: false\n    \nexecute:\n  echo: false\n  warning: false\n  message: false\n  include: false\n      \nbibliography: dot-plots-axes.bib\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Abstract\n\nWhen visualising data, chart designers have the freedom to choose the upper and lower limits of charts' numerical axes. Axis limits can determine the physical characteristics of plotted values, such as the physical position of data points in dot plots. In three experiments (total N=420), we demonstrate that axis limits affect viewers' interpretations of the magnitudes of plotted values. Participants did not simply associate values presented at higher vertical positions with greater magnitudes. Instead, participants considered data points' relative numerical positions within the axis limits. Data points were considered to represent larger values when they were closer to the end of the axis associated with greater values, even when they were presented at the *bottom* of a chart. This provides further evidence of framing effects in the display of data, and offers insight into the cognitive mechanisms involved in assessing magnitude in data visualisations.\n\n## Introduction\n\nContext is crucial for effectively judging the magnitude of numbers. A 10% probability is twice as great as a 5% probability, but in the absence of context, it is unclear whether this value should be considered large or small. When referring to the chance of losing one's job, a 10% probability may be considered large, but when referring to the chance of losing a sports bet, a 10% probability may be considered small.\n\nContextual cues may influence interpretation of magnitude in data visualisations. One such cue is the range of values on an axis, which can serve as a frame of reference for assessing whether a data point represents a large or small number. @fig-senators-chart (a reproduction of a similar bar chart from the New York Times), which plots over time the number of Black members of the U.S. senate, provides a striking illustration. Unusually, the y-axis does not terminate just above the highest plotted value. Instead, the y-axis extends all the way to the maximum possible number of senators: 100. As a result, bars representing Black senators are confined to the very bottom, visible just above the x-axis, and a significant expanse of blank space looms above them. This framing situates plotted data points in their numerical context, thus conveying small magnitude.\n\nIt is unclear exactly how a viewer's inferences about magnitude might be influenced by axis range. Different axis limits present data points at different positions, so one possible explanation is that viewers interpret the magnitude of data points at higher positions as 'high' and those at lower positions as 'low'. Alternatively, axis limits may provide numerical context: plotted values may be judged as small in magnitude when the potential for larger values is clearly displayed. The present pair of experiments demonstrates the influence of axis limits on viewers' interpretations and explores which of these two accounts best explains how axis limits contribute to the communication of magnitude.\n\n### Overview\n\nIn three experiments, we manipulated the axis limits surrounding plotted data. The same data points either appeared close to the upper end of an axis range, or close to the lower end. Likert scale ratings of values' magnitudes were higher when data points were positioned close to the end of the axis which was associated with higher numbers. By employing charts with conventional and inverted y-axis orientations to distinguish between possible explanations, we reveal that magnitude judgements are influenced by data points' relative positions within the axis limits.\n\n## Related Work\n\n### Effects of Axis Limits on Comparison Judgements\n\nSeveral studies have explored the role of axis limits in data visualisation. Research has typically focused on how axis limits can alter impressions of the *difference between* presented values. For example, when axis ranges are expanded to create blank space around a cluster of data points, correlation between those points is judged as stronger [@cleveland_variables_1982]. Participants also rate the differences between values in bar charts as greater when the vertical gap between bars is larger due to a truncated y-axis [@pandey_how_2015].\n\nCorrell et al.'s [@correll_truncating_2020] experiments found that greater truncation resulted in higher effect-size judgements in both line charts and bar charts. They found no reduction in effect size judgements when truncation was communicated using graphical techniques (e.g., axis breaks and gradients). Truncation effects also persisted even when participants estimated the values of specific data points. This suggests the bias is driven by initial impressions, rather than by a misinterpretation of the values portrayed by graphical markings. The unavoidable consequence, Correll et al. suggest, is that designers' choices will influence viewers' interpretations whether axes are truncated or not.\n\nChoosing an appropriate axis range involves a trade-off between participants' bias (over-reliance on the visual appearance of differences) and their sensitivity (capacity to identify actual differences) [@witt_graph_2019]. Just as a highly truncated y-axis can exaggerate trivial differences between values, an axis spanning the entire range of possible values can conceal important differences. Based on participants' judgements of effect size, @witt_graph_2019 found that bias was reduced and sensitivity increased when using an axis range of approximately 1.5 standard deviations of the plotted data, compared to axes which spanned only the range of the data, or the full range of possible values. This provides further evidence of a powerful association between the appearance of data, when plotted, and subjective interpretations of differences between data points.\n\nFurther evidence of truncation effects, provided by @yang_truncating_2021 improves on the design of previous studies which employed only a few observations per condition [@pandey_how_2015] or small sample sizes [@witt_graph_2019]. Participants' ratings of the difference between two bars consistently provided evidence of the exaggerating effects of y-axis truncation. @yang_truncating_2021 noted that increasing awareness does not eliminate this effect, which may function like an anchoring bias, in which numerical judgements are influenced by reference points [@tversky_judgment_1974]. Another potential explanation discussed draws upon Grice's cooperative principle [@grice_logic_1975]. According to this account of effective communication, speakers are assumed to be in cooperation, and so will communicate in a manner that is informative, truthful, relevant, and straightforward. Analogously, a viewer will assume that a numerical difference in a chart must be genuinely large if it appears large, else it would not be presented that way. Effective visualisations should be designed so a viewer's instinctive characterisation of the data corresponds closely to their interpretation following a more detailed inspection [@yang_truncating_2021].\n\n\n::: {.cell fig.asp='0.7'}\n::: {.cell-output-display}\n![A reproduction of a bar chart from the New York Times. The y-axis limit is defined by the largest possible value, rather than the largest observed value, thus the magnitude of plotted values appears particularly small.](dot-plots-axes_files/figure-pdf/fig-senators-chart-1.pdf){#fig-senators-chart fig-alt='A bar chart with the title \\'Number of Black members of the U.S. Senate\\'. The y-axis ranges from 0 to 100, and the x-axis ranges from 1789 to 2022. Bars representing Black senators are mostly absent, with small bars representing a single Black senator appearing sporadicly in the late 1800s, late 1900s and early 2000s, before increasing around 2013, yet still occupying only 4% of the total at their peak. Source: Office of the Historian, History, Art & Archives, U.S. House of Representatives.' width=300px height=210px}\n:::\n:::\n\n\n### Effects of Axis Limits on Magnitude Judgements\n\nThe above research consistently demonstrates that the magnitude of *the difference between values* is interpreted differently depending on the axis limits employed. The present investigation is concerned with how interpretations of the magnitude of *the values themselves* are affected by a chart's design.\n\nEmpirical evidence demonstrates that judgement of a value's magnitude can depend on its relationship to a grand total or to surrounding values. This can influence interpretation of verbal approximations, and also absolute values. For example, participants instructed to take 'a few' marbles picked up more when the total number available was larger [@borges_common_1974] and rated satisfaction with the same salary as higher when it appeared in the upper end of a range, compared to the lower end [@brown_does_2008]. As well as context, vertical position also plays a role in magnitude judgements. For example, children appear to intuitively understand the relationship between height and value [@gattis_structure_2002] Both the physical world, and language (e.g., spatial metaphors), provide countless examples where 'higher' is associated with 'more', and 'lower' with 'less', and this principle has been adopted as a convention in data visualisation [@tversky_cognitive_1997].\n\nIn charts, inversions of the typical mapping between magnitude and vertical position charts can lead to misinterpretations [@okan_when_2012; @pandey_how_2015; @woodin_conceptual_2022]. Furthermore, when a company's financial performance was displayed entirely in the bottom fifth of a line chart, the company was perceived as less successful, compared to when the axis did not extend above the maximum value [@taylor_misleading_1986]. @sandman_high_1994 investigated assessments of magnitude in risk ladders, where greater risks are presented at physically higher positions on a vertical scale. Participants rated asbestos exposure as a greater threat when it was plotted at a higher position, compared to a lower position.\n\nThe above findings can be regarded as preliminary evidence that changing axis limits may affect appraisals of data points' magnitudes. However, the evidence is not substantial. @taylor_misleading_1986 did not disclose how judgements were elicited, or provide details of their sample size. @sandman_high_1994 only explored responses to one specific risk (asbestos), and each participant only took part in a single trial. The perceived threat measure was a composite of several separate ratings, preventing diagnosis of whether manipulations affected interpretations of the plotted information in particular, or just related concepts. Further, both studies introduced a confounding variable by adjusting the difference between the minimum and maximum y-axis values across conditions. Stronger evidence is required regarding how axis limits may bias inferences about magnitude, and the cognitive mechanisms involved in generating these inferences.\n\n### Judgements of Event Outcomes\n\nIn the present study, participants viewed charts showing fictitious data on the chance of particular events occurring. This provided participants with a purpose; evaluating information about event outcomes is a more meaningful task than assessing how 'large' an abstract value is. Each value was represented using a single dot on a percentage probability scale. Our use of dot plots for conveying percentages was motivated by their simplicity and use of a single encoding channel (position), thus avoiding confounding variables from other encoding channels.\n\nPresenting data about events with negative consequences warranted consideration of the cognitive processing of this information. These events are composed of two core components: 1) chance of occurrence and 2) outcome magnitude (severity). Individuals' assessments of chance and severity are not necessarily independent. Events are perceived as more likely when they are described as having more severe consequences [@harris_communicating_2011; @harris_estimating_2009]. In a similar manner, events are associated with more substantial consequences when they are described as more likely [@kupor_probable_2020].\n\nOne account suggests that perceptions of probability and outcome magnitude are related because they are both assumed to reflect the potency of the event's cause (probability-outcome correspondence principle; [@keren_probabilityoutcome_2001]). According to this account, probabilities can occasionally provide meaningful indications of outcome magnitude (e.g., rainfall), but it is inappropriate to apply this perspective to all situations (e.g., volcanic eruptions). Therefore, even though charts in the present study only display the *chance* of events occurring, assessments of the *severity* of events' consequences may also differ between conditions. Collecting separate judgements for chance and severity of consequences provides a clearer picture of how the manipulation affects distinct aspects of participants' representations. Our use of Likert scales (with discrete options) rather than visual analogue scales (with continuous options; @sung_visual_2018) prevents participants from simply mapping probability percentages directly onto a linear scale.\n\n### Data Visualisation Literacy\n\nWhen faced with charts that violate graphical conventions by using atypical scales, individuals with low data visualisation literacy are more likely to draw on data points' physical positions when making inferences about their magnitudes [@okan_when_2012; @okan_how_2016]. We administered Garcia-Retamero et al.'s [@garcia-retamero_measuring_2016] subjective graph literacy measure to determine whether responses to our manipulation of axis limits were associated with data visualisation literacy.\n\n## Experiments\n\nWe conducted three experiments manipulating y-axis limits in visualisations of fictitious data. This manipulation altered the physical positions of data points in a chart, but crucially the numerical values themselves remained the same.\n\nExperiment 1 sought to establish whether y-axis limits affected magnitude judgements. To provide context for participants, text accompanying the charts outlined (fictitious) scenarios involving a specific negative outcome (e.g., loss on financial investment, delayed flights, etc.). Three plotted data points in each chart represented the chance of the negative outcome occurring (%) for three instances associated with the scenario (e.g., three investment opportunities, three airlines, etc.).\n\nExperiment 2 introduced another factor in addition to the manipulation of y-axis limits. Half of the visualisations presented employed inverted y-axis orientations, where data points at lower physical positions represented greater values. This 2x2 experiment allowed us to investigate whether magnitude judgements were driven by data points' absolute positions, or their relative positions within the context of the axis limits.\n\nExperiment 3 manipulated y-axis limits in inverted charts only, providing clarity on the ambiguous results of the previous experiment. Importantly, the use of inverted charts should not be considered an endorsement (see issues above). However, they serve to distinguish between two possible explanations, since they reverse the typical associations between physical position and magnitude.\n\nEthical approval was granted by The University of Manchester's Division of Neuroscience & Experimental Psychology Ethics Committee (Experiment 1: Ref 2021-11115-18258; Experiment 2: Ref 2021-11115-20464; Experiment 3: Ref. 2021-11115-20745). Data, analysis code, experimental scripts, materials and a link to run the experiments are available at https://osf.io/3epm2/. We also provide all necessary resources for running a Docker container, within which the computational environment used for analysis is recreated, meaning a fully-reproducible version of this paper can be generated.\n\n### Experiment 1\n\n#### Method\n\n##### Materials\n\n###### Datasets\n\nFor each dataset, we generated three values from a normal distribution. Population means were specified manually in order to represent plausible values for the probability of the event occurring (28% - 72%). All datasets had a population standard deviation of 0.5. The same dataset was employed for both of the experimental conditions associated with a given event scenario.\n\n###### Charts\n\n\n::: {.cell fig.asp='0.6'}\n::: {.cell-output-display}\n![Example charts, taken from Experiment 1. The *high physical position* condition (left) presents data points near the top of the chart; the *low physical position* condition (right) presents the same data points near the bottom of the chart.](dot-plots-axes_files/figure-pdf/fig-example-charts-1.png){#fig-example-charts fig-alt='Two dot plots, where plotted data points are repsented by small black circles. In the left dot plot, the y-axis ranges from 61.5% to 71.5%, thus the three data points, which are around the 70% point, appear near the top of the axis. In the right chart, the y-axis ranges from 68.5% to 78.5%, thus the same three data points appear near the bottom of the axis. Each y-axis is labelled \\'Chance (%)\\', accompanied by an upwards arrow.' width=300px height=180px}\n:::\n:::\n\n\nDatasets were displayed using dot plots. In experimental trials (n = 40), upper and lower axis limits were manipulated such that data points either appeared in the top third of the chart (high physical position: @fig-example-charts , left) or in the bottom third (low physical position: @fig-example-charts, right).\n\nThe y-axis range in each chart was 10 percentage points. Horizontal gridlines appeared at one-unit increments. The horizontal gridlines 1.5 units from the extremes were labelled with numerical values.\n\nFiller trials (n = 15) and attention check trials (n = 5) presented data points in the middle third of the chart. Filler trials employed this additional variation to prevent participants from identifying the purpose of the study.\n\n##### Procedure\n\nThe experiment was programmed in PsychoPy (version 2021.1.4, [@peirce_psychopy2_2019]) and hosted on pavlovia.org. Participants were instructed to complete the experiment on a desktop computer or laptop, not a tablet or mobile phone. Instructions explained that their task involved assessing the chance and severity of negative outcomes in various scenarios involving risks and noted that some scenarios might appear similar to other scenarios. Participants were asked to complete the task as quickly and accurately as possible. Two practice trials preceded the experiment proper.\n\nAn example of a single trial is shown in @fig-example-trial. Participants provided two responses in each trial: a rating of the chance of the negative event occurring; and a rating of the severity of the consequences if that negative event occurred. Both 7-point Likert scales had two anchors at their extremes: *'Very unlikely'* and *'Very likely'*; for the 'Chance' scale and *'Very mild'* to *'Very severe'*. for the 'Severity' scale. All other points were unlabelled. Text specified that answers should be given in response to the plotted data (e.g., *\"If you camp on one of these days...\"*). The term 'chance' was used instead of 'probability' to avoid confusion with the standard 0-1 scale for probabilities, and to reflect casual usage.\n\nParticipants could change their responses as many times as they wished before proceeding to the next trial, but could not return to previous trials. In attention check trials, participants were instructed not to attend to the chart, and instead to provide specified responses on the Likert scales.\n\nBefore exiting the experiment, participants were informed that all presented data were fictitious and guidance was provided in case of distress.\n\n\n::: {.cell fig.asp='0.625'}\n::: {.cell-output-display}\n![An example trial, taken from Experiment 1. Participants provided two ratings in each trial: the chance of an event occurring (magnitude rating), and the severity of consequences.](images/example_trial.png){#fig-example-trial fig-alt='A screenshot of an example trial. The title is \\'Heavy Rainfall\\' and the introductory text reads \\'You are going on a camping trip next week. The graph below shows the chance of heavy rainfall for three randomly selected days next week\\'. The graph shows three data points located at high physical positions on the axis. Alongside this, text reads \\'If you camp on one of these days... What is the chance you experience heavy rainfall?; What is the severity of consequences if you experience heavy rainfall?\\'. Below the first question is a 7-point Likert scale with anchors \\'Very unlikely\\' and \\'Very likely\\' at the extremes, below the second question is a 7-point Likert scale with anchors \\'Very mild\\' and \\'Very severe\\'.' width=320px height=200px}\n:::\n:::\n\n\n##### Design\n\nWe employed a repeated-measures, within-participants design. Participants encountered scenarios from experimental trials twice: once with data presented at a high physical position and once with data presented at a low physical position.\n\nMaterials were divided into two lists to minimise the likelihood of different versions of the same scenario appearing in close succession. One list contained half of the high-condition items and half of the low-condition items for the experimental scenarios. The other list contained the alternate versions of each of the experimental scenarios. Fillers and attention check questions were split between the two lists, and did not appear more than once. The order of the two lists was counterbalanced across participants. Within each list, scenarios were presented in a random order.\n\n##### Participants\n\nThe experiments were advertised on Prolific.co, a platform for recruiting participants for online studies. Normal or corrected-to-normal vision and English fluency were required for participation.\n\n\n\n\n\nData were returned by 160 participants. Ten participants' submissions were rejected because they answered more than two of 10 attention check questions incorrectly. This left a total of 150 participants whose submissions were used for analysis (52.00% male, 45.33% female, 2.67% non-binary). Mean age was 31.49 (*SD* = 12.47)[^1]. The mean data visualisation literacy score was 21.28 (*SD* = 4.58), out of a maximum of 30. Participants whose submissions were approved were paid Â£3.55. Average completion time was 25 minutes [^2].\n\n[^1]: Age data were unavailable for one participant.\n\n[^2]: Timing data were unavailable for two participants.\n\n##### Analysis Technique\n\nAnalyses were conducted using R (version 4.2.1; [@r_core_team_r_2022]).\n\nLikert scales express granularity at the level of ordinal data. They record whether one rating is higher or lower than another, but not the magnitude of this difference. Therefore, Likert scales do not necessarily capture values from latent distributions (mental representations) in a linear manner. The distance between one pair of points and another pair may appear equal, but may represent different distances on the latent distribution. Therefore, it is inappropriate to analyse Likert scale data with metric models, such as linear regression [@liddell_analyzing_2018]. Throughout this paper, we construct cumulative link mixed-effects models, using the *ordinal* package (version 2019.12-10, [@christensen_ordinalregression_2019]) to analyse Likert scale ratings. Odds ratio effect sizes were converted to Cohen' d values using the *effectsize* package (version 0.8.2, [@ben-shachar_effectsize_2020]).\n\nSelection of model random effects structures was automated using the *buildmer* package in R (version 2.3, [@voeten_buildmer_2022]). The maximal random effects structure included random intercepts for participants and scenarios, plus corresponding slopes for the position variable [@barr_random_2013]. *buildmer* initially identified the most complex model which could successfully converge. It subsequently removed terms which did not contribute substantially to explaining variance in ratings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}