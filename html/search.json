[
  {
    "objectID": "intro.html#the-quality-of-evidence-on-data-visualisations",
    "href": "intro.html#the-quality-of-evidence-on-data-visualisations",
    "title": "2  Introduction",
    "section": "2.1 The Quality of Evidence on Data Visualisations",
    "text": "2.1 The Quality of Evidence on Data Visualisations\nRegrettably, our understanding of how people interpret data visualisations (and subsequent guidance) is built on shaky foundations. Some received wisdom has not been empirically tested at all, other claims have been discredited or confirmed only recently (Kosara, 2016). Consequently, it is not always clear where evidence ends and opinion starts; intuition and unsubstantiated statements make for “visualisation folklore” (Correll, 2022, pg. 3). For example, there is mixed evidence regarding the harm caused by ‘chart junk’ (i.e., visual embellishments, Franconeri et al. 2021). However, its deprecation is superficially attractive, appealing to aesthetic judgements and ‘common sense’, so it persists (Kosara, 2016). Rigorous data visualisation research is required to fill gaps in knowledge and generate a reliable evidence-base.\nVisualisation research takes many forms. Studies on data visualisation have employed a range of techniques, including controlled experiments, usability tests, interviews, observations, and case studies, and have focused variously on perception, cognition, exploratory data analysis, and user experience (Lam et al., 2011). Experimental psychology studies on data visualisation are particularly valuable because they generate fundamental evidence on how visualisations are interpreted. Conversely, research focused on visualisation design which fails to consider human interpretation does not produce the same type of generalisable knowledge. Inadequate best practice recommendations indicate insufficient understanding of psychological mechanisms. However, progress can be slow, since theories about cognitive and perceptual processes are built through cumulative work (Chen et al., 2020). Psychological research confers benefits in the form of related empirical work, alongside established methods and theories (Correll, 2022, Rensink, 2021).\nMultiple studies illustrate that preferences and introspection are not a reliable source of information on effective visualisation practices. For example, an experiment exploring physicians’ judgements about clinical trials found that icon arrays resulted in the most accurate judgements, compared to tables, pie charts, and bar charts (Elting et al., 1999). However, none of the 34 physicians in the sample preferred this format. In another study, medical students almost unanimously preferred visualisations with a rainbow colour scheme, but made fewer errors when using a diverging (e.g., red-blue) colour scheme (Borkin et al., 2011). Tables of values may be favoured over visualisations in certain tasks where the visualisations actually offer significant benefits (Saket et al., 2019). Similarly, participants in Burns et al.’s (2021) study estimated that pictographs took longer to understand, compared to equivalent visualisations without icons. However, this self-report measure was at odds with recorded response times, which indicated no differences between visualisations types. There is also evidence that choropleth maps were preferred by graduate students despite conferring no performance advantage over other statistical map designs (Mendonça and Delazari, 2014). Many authors suggest that preferences are influenced by familiarity, rather than performance advantages. Measuring preferences provides valuable insight into people’s engagement with different visualisations. However, such opinions must be treated appropriately, not used to inform conclusions about effectiveness.\nRensink (2021) presents recommendations for generating useful research findings. Using a single task, and manipulating a single feature of interest, over multiple trials, assists in identifying underlying mechanisms. Integrating explanations from prior research helps ensure explanations of mental processes are sufficiently detailed. Other important but frequently overlooked matters include appropriate counterbalancing, reporting effect sizes and acknowledging individual differences.\nThere are a multitude of variables that can be manipulated to gain insight into visualisations. Criticisms are sometimes levelled at studies with particularly high or low levels of experimental control. However, researchers must strike an appropriate balance between ecological validity and precision (Abdul-Rahman et al., 2020). Choosing suitable tasks for participants requires a similar trade-off (Suh et al., 2022).\nVision sciences offer a variety of paradigms for assessing various aspects of human performance in visualisation tasks. For example, experiments may evaluate accuracy (by comparing responses to a correct answer), precision (by quantifying variability in responses), or processing speed (by measuring reaction times, Elliott et al., 2020). However, chosen methods must be appropriate for a research question. Whereas methods from vision-sciences are typically concerned with performance in low-level perceptual tasks, other research focuses on message-level interpretations (Pandey et al., 2015) or decision-making (Padilla et al., 2018)."
  },
  {
    "objectID": "intro.html#perceptual-precision-in-data-visualisations",
    "href": "intro.html#perceptual-precision-in-data-visualisations",
    "title": "2  Introduction",
    "section": "2.2 Perceptual Precision in Data Visualisations",
    "text": "2.2 Perceptual Precision in Data Visualisations\nIdentifying gaps in our understanding of the psychology of data visualisations requires knowledge of prior lines of inquiry and established findings. Arguably the most influential study in the field of data visualisation is Cleveland and McGill’s (1984) investigation of elementary perceptual processes involved in viewing visualisations. This study sought to establish how precisely viewers can represent different graphical properties used to encode data (e.g., position, length, angle, etc.). For each encoding type, participants identified which of two marks conveyed the smaller value, and estimated the difference in size as a percentage. Subsequent ranking based on the magnitude of participants’ errors produced a hierarchy of visual encoding channels. Since position-encoding produced smaller errors than both length- and angle-encoding, this suggests that data will be represented most precisely when encoded using position on a common (aligned) scale.\nThis study’s findings have endured replication (Heer and Bostock) and enthusiasm for perceptual precision has inspired a great deal of important research in this field. This research spans visual processing of proportion (Spence and Lewandovsky, Hollands and Spence), variance (Stock and Behrens), correlation (Harrison et al., 2014, Hong et al., 2021), and other basic processes, such as visual comparison (Simkin and Hastie, 1987, Zacks et al. 1998) and colour discrimination (Szafir, 2018). The study has also influenced development of software for automating visualisation design (Mackinlay, 1986) and simulating visualisation comprehension (Lohse, 1993). However, to consider perceptual precision as the only relevant concern in data visualisation design is unwarranted; many additional factors require consideration."
  },
  {
    "objectID": "intro.html#beyond-perceptual-precision",
    "href": "intro.html#beyond-perceptual-precision",
    "title": "2  Introduction",
    "section": "2.3 Beyond Perceptual Precision",
    "text": "2.3 Beyond Perceptual Precision\nThose viewing data visualisations do not always employ optimally-precise visual cues, which can lead to inaccurate judgements about plotted data (Yuan et al., 2021). Furthermore, in particular tasks, precision can actually hinder, rather than facilitate, judgements. For example, because perceptual averaging benefits from lower spatial frequencies, colour encoding offers greater efficiency than more precise position encoding in line charts (Correll et al. 2012). Effective decision-making under uncertainty does not necessarily correspond to precision in probability estimation, because of the differences in mental processing associated with these two distinct tasks (Kale et al., 2020).\nFurthermore, the choice of graphical encodings employed in a data visualisation can influence the type of interpretation it elicits. For example, viewers are more likely to refer to trends when describing line graphs and discrete differences when describing bar charts. This can occur even when the nature of the plotted data is ill-suited to this type of characterisation (Zacks and Tversky, 1999). This means that a line chart may provoke a peculiar interpretation such as ‘a building becomes more secure as the alarm system becomes more active’, whereas a bar chart may provoke an interpretation such as ’a building with 10 motion sensors is more secure than a building with 5 motion sensors’. Similarly, production of bar charts and line charts is also influenced by whether a discrete or continuous relationship is specified in the brief. Design choices also influence beliefs about the distribution of underlying data, when presenting average values. Compared to a data point positioned ‘outside’ a bar, a data point positioned ‘inside’ a bar is more likely to be considered part of the underlying data (Newman and Scholl, 2012). However, confidence intervals eliminate this bias (Pentoney and Berger, 2016). This accords with the notions that metaphor (Ziemkiewicz and Kosara) and affordances (Kindlmann and Scheidegger) play a role in a visualisation’s ability to convey information.\nAttention is another important factor in comprehension of data visualisations. Complex tasks requiring selective attention can cause distinctive patterns in non-focal data to be completely overlooked (Boger et al., 2021). Features of data mentioned in textual summaries are over-weighted in viewers’ mental representations, causing difficulty with the ability to assume the perspective of a naïve viewer (Xiong et al. 2019). In addition, the salience of vertical bars may be responsible for incorrect reports of differences between histograms with identical distributions (Lem et al., 2014). As a solution, explicitly encoding differences between pairs of values can facilitate pattern recognition (Nothelfer and Franconeri, 2020) and highlighting particular attributes can facilitate recall (Ajani et al., 2021).\nSimply conveying information is not the only purpose of data visualisations, since they also influence recall, opinion-formation, and decision-making (Bertini et al., 2020). A large number of cognitive biases affect these, and several other aspects of processing data visualisations (Dimara et al., 2020). Whilst it is necessary to consider the precision of elementary perceptual processes, that alone is not sufficient for a comprehensive understanding of how data visualisations function (Bertini et al., 2020)."
  },
  {
    "objectID": "intro.html#manipulating-axes-in-data-visualisations",
    "href": "intro.html#manipulating-axes-in-data-visualisations",
    "title": "2  Introduction",
    "section": "2.4 Manipulating Axes in Data Visualisations",
    "text": "2.4 Manipulating Axes in Data Visualisations\nUnderstanding how inaccurate impressions arise provides insight into mechanisms involved in interpreting data visualisations. This, in turn, can inform recommendations for effective design. A notorious topic in the literature on misleading visualisations is axis truncation. This typically refers to the practice of employing a y-axis which commences with a non-zero value (Correll et al. 2020), though may also be considered any adjustment at either extreme of an x- or y-axis (Pandey, 2015).\nThere is considerable evidence that the range of axis values employed in charts influences interpretations of data. The majority of research on this topic has focused on how constraining the range of an axis, and thus increasing the physical distance between plotted values, increases the perceived magnitude of the difference between those values. For example, accountants appraising financial performance using line and bar charts interpreted plotted increases as larger when they were depicted using a truncated y-axis (Taylor and Anderson, 1986). Similarly, bar charts employing truncated axes biased students’ investment decisions (Arunachalam and Pei, 2002). A large-sample online experiment also observed that differences between values were considered larger when truncated bar charts were used (Pandey et al., 2015). This experiment examined message-level representations of data by framing questions in terms of subject matter (e.g., access to safe drinking water) rather than graphical elements (e.g., difference in bar length. Other axis manipulations, such as log-scales (Romano et al., 2020), inverted scales (Woodin et al., 2021, Pandey et al., 2015), and expanded axes in scatterplots (Cleveland, 1982) also influence judgements about data.\nRisk communication research has independently generated similar findings. Because many hazards cannot be completely avoided, data visualisations are often used to contrast the levels of risk associated with two scenarios (e.g., intervention versus no intervention). Thus, assessments of ‘risk reduction’ are essentially judgements about the magnitude of difference between two values. For example, one experiment compared stacked bar charts, which include additional information on the total number of individuals at risk, to bar charts which displayed only the number of individuals affected (Stone et al., 2003). The latter design increased the bars’ visual disparity, and subsequently increased impressions of the magnitude of difference.\nThe physical distance between data points consistently biases interpretations of the magnitude of difference in spite of attention to actual numerical values and also design features intended to highlight truncation (Correll et al., 2020). Bias is diminished, but still observed, following explicit warnings about errors in judgement due to y-axis truncation. This suggests that this effect is largely automatic, and does not primarily occur due to insufficient engagement of cognitive capabilities (Yang et al., 2021).*\nOne study observed no association between participants’ data visualisation literacy and their susceptibility to bias due to axis truncation in bar charts (Yang et al., 2021). Conversely, another experiment suggests that the effect of axis truncation on subjective judgements and quantitative estimates in line charts disappears when accounting for data visualisation literacy (Driessen et al., 2022). However, low variability in observed data visualisation literacy levels in the latter experiment raised concerns about the scale used to measure data visualisation literacy.\nPandey et al. (2015) and Yang et al (2021) propose that this bias could arise due to the dominance of first impressions during translation from graphical schemata (Pinker, 1990) to a ‘real-world’ conceptual understanding (see also, Carpenter and Shah, 1998, Tversky and Kahneman, 1974). Additionally, Yang et al. (2021) suggest that viewers’ beliefs about the communicative intent of a designer could play a role in viewers’ interpretations. Under Grice’s Co-operative Principle (Grice, 1975), communicative contributions in conversation are assumed to be truthful, relevant, clear, and sufficiently informative. To extrapolate this to data visualisations, viewers might infer that differences between values must be genuinely large if they appear large, because they would otherwise not be presented as such.\nIn How to Lie With Statistics, Huff (1954) suggests that axis truncation creates a false impression of plotted data. This practice has been labelled ‘deceptive’ for both bar and line charts (Lauer and O’Brien, 2020). A tool for automatically identifying and correcting misleading line charts extends y-axes to include zero whenever this value is omitted from the original chart (Fan et al., 2022).\nRecent work has presented an alternative perspective on this controversial practice. Non-truncated axes can obscure significant differences just as easily as truncated axes can exaggerate inconsequential differences. The appropriate magnitude to convey depends on what constitutes an important difference in the data at hand (Correll et al., 2020). Indeed, failing to truncate an axis could be considered misleading in certain circumstances (Wainer, 1984). Yang et al. (2021) suggest that effective designs will ensure that a viewer’s immediate characterisation of plotted data closely corresponds to their interpretation following a detailed inspection. Acknowledging that differences must be depicted in proportion to their significance, Witt (2019) reports that axes spanning approximately 1.5 standard deviations provide a balance between sensitivity and bias in fields with standardised effect size measures, such as psychology. Unfortunately, different domains will not necessarily share the same notion of what amounts to a meaningful difference. Choices regarding axis ranges are ultimately designers’ unavoidable decisions (Correll et al., 2020).\nAlthough line charts and bar charts are equally susceptible to biases due to truncation (Correll et al., 2020; Witt et al., 2019), there may be reason to treat them differently. Truncation distorts the mapping between a bar’s extent and the quantity it represents, but free-floating position-encoding used in line charts does not convey quantity in the same manner, providing immunity against such distortion (Bergstrom and West). Therefore, whilst starting an axis at zero cannot guarantee that differences between values are depicted appropriately, this does ensure adherence to a fundamental aspect of visualisation design. Alternatively, to avoid this trade-off, quantitative data with discrete categories can be plotted using position-encodings only (e.g., dot plots)."
  },
  {
    "objectID": "intro.html#misleading-data-visualisations",
    "href": "intro.html#misleading-data-visualisations",
    "title": "2  Introduction",
    "section": "2.5 Misleading Data Visualisations",
    "text": "2.5 Misleading Data Visualisations\nSome misleading visualisations may prevent viewers from accurately extracting numerical information. However, research on axis truncation illustrates that misleading visualisations may also interfere with subjective judgements. A line chart may avoid misrepresenting a dataset’s numerical properties yet generate a distorted impression of the magnitude of a trend. The latter is revealed not by assessing the performance of viewers, but their interpretations (Stone et al., 2015).\nInfluencing subjective judgements may still be considered a misleading practice because a dishonest framing of information could elicit an unreliable interpretation that would differ from the same viewer’s better-informed perspective. Not all aspects of deceptive design are inherently misleading, and deceptiveness can be context-dependent. Comparing examples of ‘misleaders’ from Ge et al.’s (2023) design space helps illustrate this distinction. ‘Concealed uncertainty’ and ‘cherry-picking’ refer to unambiguously deceptive practices, whereas ‘aggregation’ and ‘scale range’ must be preceded by the word inappropriate in order to convey their capacity to deceive."
  },
  {
    "objectID": "intro.html#data-visualisation-literacy",
    "href": "intro.html#data-visualisation-literacy",
    "title": "2  Introduction",
    "section": "2.6 Data Visualisation Literacy",
    "text": "2.6 Data Visualisation Literacy\nUnderstanding individual differences in the ability to comprehend data in visualisations is important for understanding the psychology of data visualisations (Boy et al., 2014). Research on this topic requires reliable tools for measuring data visualisation literacy.\nGalesic and Garcia-Retamero’s 13-item test (2011) was based on Friel et al.’s (2001) hierarchy of skills for interpreting visualisations, which ranges from comprehension to extrapolation. Research has demonstrated that this scale can predict whether a graphical representation will facilitate understanding of risk information (Okan et al., 2012). A different 53-item test employs a wide range of data visualisation formats, and higher scores are positively associated with both numeracy and need for cognition (Lee et al., 2019).\nResearch on data visualisation literacy has tended to focus on interpretation of well-designed charts (Ge et al., 2023). However, the ability to detect (Camba et al., 2022) and make sense of (Ge et al., 2023) misleading charts should be considered an important feature of data visualisation literacy. A robust 30-item test enables assessment of an individual’s ability to accurately comprehend deceptive designs (Ge et al., 2023). This work also suggests that attention and critical thinking may benefit viewers in avoiding some, but not all, biased interpretations. Using Galesic and Garcia-Retamero’s 13-item test (2011), Okan et al. (2016) found that higher data visualisation literacy is associated with more time processing a visualisation’s misleading features, thus promoting correct interpretations. Lower data visualisation literacy is associated with greater reliance on conventions (e.g., the relationship between vertical position and magnitude).\nThe empirical work presented in this thesis employs the 5-item version of Garcia-Retamero et al.’s (2016) Subjective Graph Literacy scale. Users are asked to rate their competence in working with bar charts, line charts, and pie charts, and also their ability to perform simple tasks using bar charts. This approach echoes prior work in the development of subjective numeracy scales. Despite its short completion time and use of subjective ratings, it is strongly correlated with an objective measure of data visualisation literacy (Galesic and Garcia-Retamero, 2011). The scale also produces a final score out of 30, offering greater sensitivity than a similarly brief objective scale, where tallying correct responses produces a final score out of 4. These characteristics make for an appropriate tool for assessing participants’ data visualisation literacy in experimental studies. Indeed, this measure has been used to assess variability between participants in studies on axis truncation (Yang et al., 2021), correlation (Strain et al., 2023), information synthesis (Mantri et al., 2022), and explanation of visualisations (Yang et al., 2023)."
  },
  {
    "objectID": "intro.html#interpreting-absolute-magnitude",
    "href": "intro.html#interpreting-absolute-magnitude",
    "title": "2  Introduction",
    "section": "2.7 Interpreting Absolute Magnitude",
    "text": "2.7 Interpreting Absolute Magnitude\nData visualisation design has the potential to impact subjective judgements of many aspects of data, such as variability, noise, and numerosity. Prior research has closely examined how axis truncation can influence judgements of relative magnitude (differences between values). In contrast, little is known about how axis limits may influence judgements of absolute magnitude: how large or small values are. Limited insight into how magnitude is interpreted in data visualisations impedes understanding of how visualisations may effectively communicate magnitude. Prior research on this topic is summarised below.\nIn bar charts displaying data on individuals affected by a risk, perceived likelihood decreased when the total population at risk was emphasised using shaded bars, rather than blank space (Stone et al., 2017). In bar charts violating the convention of mapping higher values to higher positions, participants frequently misinterpreted magnitudes (Okan et al., 2012). This was due to difficulty in rejecting first impressions, particularly for participants with low data visualisation literacy. Other work combining judgements of values’ magnitudes with judgements of relative differences has limited examination of the former (Okan et al., 2018). Visualisations that facilitate comprehension of relative differences may fail to effectively communicate the absolute magnitudes of values depicted, illustrating a potential trade-off in design (Reyna et al., 2008).\nOne study has specifically focused on how axis ranges may inform impressions of absolute magnitude. Sandman et al. (1994) manipulated risk ladders, where individual probabilities are presented on vertical scales incorporating a range of probability values. Changing this range alters the position of a plotted value. Perceived threat (a composite measure made up of perceived likelihood, danger, reported concern and fear) was higher when the risk appeared near to the top of the ladder, compared to near the bottom. However, the position of plotted values did not completely dictate magnitude judgments. A numerically higher risk plotted at the same position near the top of the ladder generated higher ratings. There was also mixed evidence regarding the effects on intentions to spend money mitigating the risk. Confidence in the robustness of these findings is limited by various factors including use of a single trial per participant, a single scenario, a composite measure obscuring pure magnitude ratings, and a confounding variable of the risk ladder’s range.\nComparing linear and logarithmic risk ladders, Freeman et al. (2021) did not replicate Sandman et al.’s (1994) main finding. However, in addition to a graphical cue to magnitude, they used risk ladders which employed additional symbolic number cues in their titles, labels, and accompanying descriptions. A broken scale may also have reduced the degree to which inferences were based on the value’s physical position. Therefore, participants’ judgements may not have been based purely on the appearance of visualisations.\nThis thesis presents a detailed investigation into how impressions of the magnitude of numerical values are influenced by data visualisation design."
  },
  {
    "objectID": "intro.html#overview-of-thesis",
    "href": "intro.html#overview-of-thesis",
    "title": "2  Introduction",
    "section": "2.8 Overview of Thesis",
    "text": "2.8 Overview of Thesis\nThe objective of this thesis is to demonstrate that design choices influence cognitive processing of the magnitude of values presented in data visualisations. I present several empirical studies, each investigating different factors affecting viewers’ interpretations and each using a different data visualisation format.\nChapter 2 discusses how reproducibility issues threaten the validity and trustworthiness of research projects. A lack of transparency decreases the utility of published work for both authors and their peers. Approaches for remedying these issues are presented, in line with recommendations from a variety of disciplines. A particular emphasis is placed on computational reproducibility: the capacity to replicate the computational environment used in generating results. This provides background for the practices used in each of the following empirical research chapters.\nChapter 3 presents a pair of experiments which establish that interpretations of magnitude can be influenced by data visualisation designs. The first experiment demonstrated that manipulating axis limits in dot plots affected participants’ judgements of overall magnitude. A second experiment investigated whether this occurred because axis limits altered the absolute or relative positions of plotted values within axis limits. In dot plots with inverted y-axes, where higher numerical values are presented at lower positions, values near bottom were associated with higher magnitudes. This illustrates that interpretations of magnitude are informed by the relative positions of values within axis limits.\nChapter 4 presents an experiment which explores how visualisations may influence interpretations of magnitude even when the appearance of plotted values remains unchanged. This experiment demonstrated that manipulating color legend limits in choropleth maps affected participants’ judgements of overall magnitude. Participants rated magnitudes as lower when the range of values on the color legend extended beyond the largest plotted value. This illustrates that the numerical context accompanying plotted values can influence interpretations of magnitude, without altering the physical appearance of those values.\nChapter 5 presents a pair of experiments which investigate the role of contextual information on interpretations of magnitude. The first experiment demonstrated that participants’ judgements of overall magnitude were affected by extension of bar charts’ upper axis limits which incorporated a denominator value. A second experiment revealed that participants’ bias was increased when this denominator information was excluded from the text accompanying the chart. This illustrates that knowledge about a dataset’s characteristics (e.g., denominator value) can influence the extent to which design choices affect interpretations of magnitude.\nFinally, Chapter 6 presents a synthesis of this empirical work, alongside a discussion of implications and future directions."
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "3  Reproducibility",
    "section": "",
    "text": "4 Comparing Containers with Virtual Machines\nVirtual machines perform a similar function to containers. However a notable difference is that virtual machines are large, whilst containers are comparatively lightweight (Piccolo and Frampton, 2016). This difference is due to the fact that virtual machines use their own kernel, whereas containers use the operating system kernel provided by the local machine. This reduces the relative size of a container, and enhances it computational power (Cito et al., 2016). Thus, virtual machines may be considered more comprehensive than containers, offering a greater degree of separation from the characteristics of the host machine (Grüning et al., 2018, Piccolo and Frampton, 2016). However, containers are typically compatible with version control systems (Piccolo and Frampton, 2016) and offer greater transparency (Nüst et al., 2020). Furthermore, due to their modular features, making minor adaptations is trivial with a container but comparatively prolonged with a virtual machine."
  },
  {
    "objectID": "reproducibility.html#problems-in-research",
    "href": "reproducibility.html#problems-in-research",
    "title": "3  Reproducibility",
    "section": "3.1 Problems in Research",
    "text": "3.1 Problems in Research\nIoaniddis, 2005 - Many lines of inquiry may be producing unreliable conclusions. Better powered experiments are required. Highlights a general concern with the trustworthiness of findings.\nOpen Science Collaboration, 2015 - replication study, revealing that the evidence many established findings was not as strong as initial reported.\nImproving methods, plus reporting and dissemination, will improve the quality of published research. But these are different to efforts to improve reproducibility. A lack of transparency underlies issues with reproducibility, whereas problematic research methods underly issues with replicability (Munafo et al., 2017)\nHARKing, p-hacking, low power, publication bias (Bishop, 2019)\nWidespread recognition of problems in science (Baker, 2016). Researchers have neglected to focus on reproducibility (Ince, 2010).\nCrüwell et al ., 2019 - behaviours that support reproducibility and replicability in general may be referred to using the umbrella term ‘Open Science Practices’.\nWhite et al. 2013 - open science recommendations are similar across disciplines.\nNosek et al., 2015 - TOP Guidelines for Open Science practices, requires sharing.\nReplication allows for independent evaluation of research findings (in computational science), but is resource intensive. Assessing reproducibility offers another simpler way to evaluate reliability of work (Peng, 2011).\nThe pre-requisite is that these resources are available and suitable packaged to reflect original researchers’ use. This provides opportunity to assess one aspect of research, in order to gauge its trustworthiness.\nKlein et al. (2018) - Reproducibility crisis has sparked “urgent conversation”.\nThis chapter will review the literature on reproducibility, covering best practices then outline my approach used in this thesis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n— — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - -"
  },
  {
    "objectID": "reproducibility.html#reasons-for-sharing",
    "href": "reproducibility.html#reasons-for-sharing",
    "title": "3  Reproducibility",
    "section": "3.2 Reasons for Sharing",
    "text": "3.2 Reasons for Sharing\nThere are many convincing arguments for reproducibility. Scientific approaches require that researchers can properly assess the credibility of research (Klein et al., 2018) and independently authenticate other researchers’ conclusions (Blischak et al., 2019) and . Thus, supporting third parties in reproducing research can increase perceptions of its robustness and reliability (Sandve et al., 2013). This also facilitates identification of errors in analysis (Klein et al., 2018). In addition to these motivating factors, authors may even appreciate the advantages of reproducible practices more than their peers (Piccolo and Frampton, 2016). For example, these practices can save time and effort (Sandve et al., 2013), and permanently sharing resources provides insurance against the loss of those resources (Klein et al., 2018)."
  },
  {
    "objectID": "reproducibility.html#definitions",
    "href": "reproducibility.html#definitions",
    "title": "3  Reproducibility",
    "section": "3.3 Definitions",
    "text": "3.3 Definitions\nInconsistent definitions of reproducibility have been employed in academic works (Plesser, 2018), particularly between experimental and computational disciplines (Drummond, 2009). In this thesis, I will use the following definitions. ’Reproducibility’ in research involves generating results exactly as reported using the project’s original data and code. ‘Replicability’ in research involves generating new data to assess consistency with an existing finding (Peng, 2011)."
  },
  {
    "objectID": "reproducibility.html#sharing-code-and-data",
    "href": "reproducibility.html#sharing-code-and-data",
    "title": "3  Reproducibility",
    "section": "3.4 Sharing Code and Data",
    "text": "3.4 Sharing Code and Data\nA textual description of analysis in a manuscript presents an incomplete and vague acount of the analytical process (Piccolo and Frampton, 2016). Researchers must share code in order to detail the journey from the original dataset to inferential statistics (Klein et al., 2018), otherwise their software is a ‘black box’ (Morin et al., 2012). Initially, the possibility of issues or inconsistencies arising from computer code was overlooked (Plesser, 2018). However, it is now widely recognised that a computational analysis pipeline presents many opportunities for error. Making code openly available permits independent reproduction of all computational processes (Stodden et al., 2016). This, in turn, can engender trust, promote collaboration, and facilitate new applications (Jiménez et al., 2017). Each stage of processing must be included (Sandve et al., 2013) and any files produced using the analytical pipeline should be expendable, since reproducing them using the code supplied should be trivial (Marwick et al., 2018). For full transparency, data should be supplied in a raw, unprocessed form (White et al., 2013). Keeping raw data separate from other files ensures that the original file is not altered and the stages of processing are clear (Marwick et al., 2018). Other resources, such as stimuli and experiment scripts should also be shared alongside data and code (Klein et al., 2018).\nThe FAIR principles (Wilkinson et al., 2016) propose that data (and metadata) should be Findable (easily discovered), Accessible (easily obtained), Interoperable (easily integrated with other tools), and Reusable (easily employed beyond their original use). FAIR principles are also relevant to other computational tools (Lamprecht et al., 2020), with similarities to Open Source Software, which does not place limits on who may examine, adapt and extend the underlying code (Jiménez et al., 2017).\nWhen sharing resources, a researcher’s choices can either assist or obstrust re-use (Chen et al., 2019). For example, using non-proprietary file types ensures that third parties can readily access resources (White et al. 2013). Rather than personal or institutional websites, independent providers (e.g., Open Science Framework) are recommended for depositing these resources (Chen et al., 2019, Klein et al., 2018). Effective documentation is also important. A ‘codebook’ or ‘data dictionary’ can be used to explain the contents of a data file (Klein et al., 2018), inline comments can be used to explain code (Rule et al., 2019), and a README can be used to cover elementary information such as setup instructions (Lee et al., 2018). Documentation can also provide details on data collection and known issues (White et al. 2013). Finally, licences contribute to a research project’s longevity, and provide clear statement for third parties, ensuring that their use of resources is appropriate (Jiménez et al., 2017). Where possible, lenient licences should be employed to avoid unneccessary restrictions (White et al., 2013)."
  },
  {
    "objectID": "reproducibility.html#the-importance-of-public-sharing",
    "href": "reproducibility.html#the-importance-of-public-sharing",
    "title": "3  Reproducibility",
    "section": "3.5 The Importance of Public Sharing",
    "text": "3.5 The Importance of Public Sharing\nIf authors consistently shared data and code on request, freely available access could be considered unnecessary. However, empirical research demonstrates why it is important to share resources publicly. In a study of 204 papers from a journal which required authors to provide data and code on request, only 44% delivered on this promise (Stodden et al., 2018). Where research code is not publically available, various issues preclude procurement. These include local storage failures, restrictive institutional licences, concern about potential use, and concern about labour involved in providing support (Collberg & Proebsting, 2016). Provision of data and code on request simply cannot be guaranteed, necessitating public sharing. In the field of data visualisation research, public sharing has historically been uncommon. Of papers submitted to the VIS 2017 conference, 15% shared materials openly and 6% shared data openly (Haroz, 2018). Greater transparency would increase the credibility of data visualisation research and facilitate identification and recification of issues in published work (Kosara and Haroz, 2018).\nResearchers’ working practices and technological solutions both contribute to reproducibility. Whilst it has been suggested that behaviour and technology play equal roles (Sandve et al. 2013), others argue that innovations have been so effective that researchers’ engagement with these tools is now the primary challenge (Grüning et al., 2018). Researchers report that several factors impede or deter their sharing of research data, including lack of expertise, lack of precedent, and lack of time (Houtkoop et al., 2018)."
  },
  {
    "objectID": "reproducibility.html#effective-programming-practices",
    "href": "reproducibility.html#effective-programming-practices",
    "title": "3  Reproducibility",
    "section": "3.6 Effective Programming Practices",
    "text": "3.6 Effective Programming Practices\nProgramming with an automated approach has three main benefits over manual processing: increased reproducibility, increased efficiency, and reduced error (Sandve et al., 2013). Writing functions in a modular style avoids redundant repetition, promotes comprehension and supports reuse of code (Wilson et al., 2015). Researchers should also split code into appropriate chunks which each achieve a clearly-defined goal (Rule et al., 2019). These approaches shares many similarities with the Unix philosophy (Gancarz, 2003).\nThe task of preparing data prior to analysis is an important aspect of working with data. Wickham (2014) presents a set of tools, and underlying theory for this task, arguing that analysis can be facilitated by ensuring that data is in the correct structure. This structure is known as ‘tidy’ data, which consists of a column for each variable (each type of measurement) and a row for each observation (each unit measured). A principled approach simplifies the process of creating a tidy dataset using Wickham’s functions. Because each function treats data in a standardised manner, various functions can be employed in concert. The collection of R packages containing these functions (the ‘Tidyverse’) was designed with a concern for humans, not just computational performance (Wickham et al., 2019), so Tidyverse-style code is likely to promote comprehension (Bertin and Baumer, 2020).\nSeveral other coding behaviours can facilite or inhibit reproducibility. For example, absolute file paths refer to a specific directory on a user’s machine, which will not be replicated on other users’ machines. Using relative file paths, which locate files in relation to the project directory, ensure code is portable and can be used on any machine (Bertin and Baumer, 2020). Additionally, independent researchers cannot successfully verify findings if only an approximate resemblance is achieved. Therefore, for any process involving random number generation, a random seed must be specified within the script, to ensure exact reproduction of results (Sandve et al., 2013). For maximum transparency, researchers should avoid controlling code through comments, but use functional approaches instead (Wilson et al., 2015)."
  },
  {
    "objectID": "reproducibility.html#literate-programming-and-dynamic-documents",
    "href": "reproducibility.html#literate-programming-and-dynamic-documents",
    "title": "3  Reproducibility",
    "section": "3.7 Literate Programming and Dynamic Documents",
    "text": "3.7 Literate Programming and Dynamic Documents\nKnuth (1984) presented a novel perspective on comprehensibility in computer programming which has been influential in the literature on computational reproducibility. Knuth’s premise is that a programming script should not be regarded primarily as a set of instructions for a computer to follow, but a tool to assist humans in understanding those instructions. This approach, known as ‘literate programming’, involves pairing code with corresponding text, such that reporting and documentation are closely linked to underlying code (Sandve et al., 2013; Piccolo and Frampton, 2016). Dynamic documents allow authors to mix code and narrative within a single file, with results updated whenever the document is rendered. Producing (and re-producing) an entire manuscript using a dynamic document offers opportunities to easily observe the implementation of code used for each aspect of analysis (Peikert and Brandmeier, 2021). In addition to descriptive and inferential statistics, data visualisations may also be rendered dynamically (FitzJohn et al., 2014). This efficient format enhances transparency (Holmes et al., 2021), supports interactivity (Rule et al., 2019) and avoids errors due to manually collating results (Peikert and Brandmeier, 2021). Including computationally-expensive code (e.g., complex statistical models) within a dynamic document can be problematic since this code is executed every time the document is rendered (FitzJohn et al., 2014). However, capacity for model caching provides a convenient antidote."
  },
  {
    "objectID": "reproducibility.html#computational-environments",
    "href": "reproducibility.html#computational-environments",
    "title": "3  Reproducibility",
    "section": "3.8 Computational Environments",
    "text": "3.8 Computational Environments\nUnfortunately, providing data and code is necessary, but not sufficient, for guaranteeing reproducibility. For example, research has found that even when the nominally required resources are available, it is not always possible to reproduce results exactly (Stodden et al., 2018), or even to execute the code (Collberg & Proebsting, 2016). A study using an automated approach to test the execution of 379 Python files found that success depended in part on the Python version used and the presence of files capturing dependencies (Trisovic et al., 2021). Another study used a similar approach to test over 9000 R scripts (Trisovic et al., 2022). Approximately three in four scripts produced errors when executed. Implementing a code-cleaning algorithm reduced this number, but the majority (56%) still failed to run successfully. This indicates that good programming practices can improve code but cannot totally eliminate issues. Another source of error was incompatibility of R software versions and required packages. Thus, a failure to recreate the computational environment used when originally running the script prevented successful execution.\nPeng (2011) argues that reproducibility can be considered on a spectrum. Sharing code offers some benefits over a standalone publication, providing data increases reproducibility further, but ensuring that the code can be precisely executed is even better. Each researcher’s unique preferences and proficiencies result in roughly the same number of computational environments as individual researchers, illustrating the necessity to record one’s computational environment (Nüst et al., 2017). Additionally, software under continuous development, such as the Tidyverse collection of packages, is frequently updated, meaning code can stop functioning unless specific versions are recorded (Holmes et al., 2021). Other software dependencies and parameter settings also complicate reproduction, requiring precision and comprehensiveness in documentation in order to achieve full computational reproducibility (Piccolo and Frampton, 2016)."
  },
  {
    "objectID": "reproducibility.html#capturing-computational-environments-using-containers",
    "href": "reproducibility.html#capturing-computational-environments-using-containers",
    "title": "3  Reproducibility",
    "section": "3.9 Capturing Computational Environments Using Containers",
    "text": "3.9 Capturing Computational Environments Using Containers\nLike many other aspects of reproducibility, innovations in software have made it possible for researchers to capture their computational environments. R package managers, such as renv (Ushey, 2020) conveniently load specific package versions for individual projects. However, they do not guarantee computational reproducibility, becuase they do not preserve the version of R in the same way (Holmes et al., 2021) or support additional dependencies (Peikert and Brandmeier, 2021, Nüst et al., 2017). Containerisation technology offers an effective solution. A ‘container’ can capture a much greater extent of the computational environment than a package manager (Grüning et al., 2018). This technology also provides an efficient and principled approach for recreating the environment, compared to a list of instructions for manual execution (Marwick et al., 2018).\nDocker (Merkel, 2014) is a popular tool for generating containers. This process begins with a Dockerfile: a text-based file which provides instructions for installing specific package versions and loading other dependencies and resources. The Dockerfile is used to build a Docker image, which captures the computational environment. When this image is running, the environment is activated, and users may interact with this environment (Nüst et al., 2020b, Boettiger and Eddelbuettel, 2017).\nCollating all dependency information in a single Dockerfile provides simplicity, and ensures that the original computational environment can be reproduced even after updating the software. Since the primary objective is ensuring reproducibility, this approach prioritises openness and human readability over optimising performance (Nüst et al., 2020b, Boettiger, 2015). As well as simple implementations, complex arrangements can be accomodated, but present additional challenges. For example, dynamic document generation may also require specifying LaTeX dependencies (Boettiger, 2015)."
  },
  {
    "objectID": "reproducibility.html#rocker-for-capturing-r-environments",
    "href": "reproducibility.html#rocker-for-capturing-r-environments",
    "title": "3  Reproducibility",
    "section": "3.10 Rocker for Capturing R Environments",
    "text": "3.10 Rocker for Capturing R Environments\nResearchers can save time and ensure consistency by using pre-existing Docker images (Nüst et al., 2020b). One particularly valuable example of this is Rocker which captures R environments for use in Docker. This tool provides portable R environments for use with a variety of systems, facilitating computational reproducbility (Boettiger and Eddelbuettel, 2014). Consequently, any researcher can execute, edit, and extend R code in a replica of the environment originally used for its development. Developing Rocker images involves a trade-off between generalisibility and specificity. An image designed to be too widely applicable would be cumbersome, but images with overly-specific use cases would be hard to find (Boettiger and Eddelbuettel, 2017). The solution involves providing base images that are easily expanded for specific requirements, with various Rocker images ‘stacked’ together as required, avoiding unnecessary complexity (Nüst et al., 2020a)."
  },
  {
    "objectID": "reproducibility.html#the-best-is-the-enemy-of-the-good",
    "href": "reproducibility.html#the-best-is-the-enemy-of-the-good",
    "title": "3  Reproducibility",
    "section": "4.1 The Best is the Enemy of the Good",
    "text": "4.1 The Best is the Enemy of the Good\nDespite the myriad recommendations for best practice (), a principle often endorsed in the literature on reproducibilty concerns the merits of small efforts. Taking some steps to increase reproducibility still enhances a project’s quality compared to neglecting this aspect altogether (Piccolo and Frampton, 2016). Witholding resources in pursuit of continuous refinement risks never sharing them at all. This fallacy is captured by the maxim ‘the best is the enemy of the good’. Analysis code does not need be perfect to in order to be useful to others (Klein et al., 2018), and it is impossible to benefit from external inquiry if the code is not shared (Barnes, 2010). Barnes (2010) argues that perceived limitations simply reflect that the code works only for the specific scenario at hand; inessential improvements are by definition not required for basic functioning. Researchers ought to accept these limitations and share their code anyway. In addition to code, this notion has also been applied to metadata (White et al., 2013) and containerisation (Nüst et al., 2020)."
  },
  {
    "objectID": "reproducibility.html#my-approach",
    "href": "reproducibility.html#my-approach",
    "title": "3  Reproducibility",
    "section": "4.2 My Approach",
    "text": "4.2 My Approach\nbuildmer fits with the idea of showing incremental work, by automatically re-creating the process of the models that didn’t work however it also ensures consistency, and optimises output As an R package, it is archived, and its source code is transparent Rule et al. say: Document the process, not the results. \nReproducing the manuscript\nDocker containers and YAML frontmatter are types of metadata (Leipzig et al., 2021).\nGiven that much of the discussion around adoption of software concerns researcher motivation, compared to availability of tools, this thesis serves as a case study for reproducible research. I do not explore all the capabilities of these tools, but employ a simple implementation of a few in order to demonstrate a minimal use case.\nIn experimental psychology, sharing stimuli and experiment scripts is another important aspect of transparent research practices (Klein et al., 2018).\nPeirce et al., 2019 - PsychoPy was developed as a tool for conducting open and reproducible research. Designed to be used on different platforms, and to make available the underlying Python scripts for each experiment, open source, uses non-proprietary file formats. In keeping with Previous versions of the software can be specified easily, to avoid encountering errors as a result of new releases."
  },
  {
    "objectID": "reproducibility.html#conclusion",
    "href": "reproducibility.html#conclusion",
    "title": "3  Reproducibility",
    "section": "4.3 Conclusion",
    "text": "4.3 Conclusion\nThis chapter has discussed how a lack of reproducibility in published research can reduce credibility, and has revealed how various approaches can increase reproducbility. At the heart of these recommendations is the need comprehensively share resources and embrace technological solutions. Making research code and raw data openly available helps an opaque analysis process to become transparent. When an entire paper’s results can be fully reproduced by an independent third party, they can be thoroughly verified.\nWorking reproducibly is a duty (Sandve et al., 2013).\nThe notion of reproducible research code was discussed over 30 years ago, with ‘electronic documents’ providing the ability to package code with a manuscript (Claerbout, 1992).\nThus, it is necessary to capture the specific computational environment used when originally running the software. When specifying a project’s dependencies (the requisite files and software), researchers should ensure that the exact version of each package and program are supplied.\nTo capture dependencies, one must reproduce the computational environment used. (Boettiger, 2015)\naccessible and transparent, increase engagement, responsible\nThe reporting of scientific studies involves lots of summary. Raw data is collected then processed, then analysed. In a small set of inferential statistics upon which a claim rests, a lot is going on behind the scenes. The ability to trace conclusions back to their original source is not just useful, but a crucial aspect of science."
  },
  {
    "objectID": "reproducibility.html#beyond-the-scope",
    "href": "reproducibility.html#beyond-the-scope",
    "title": "3  Reproducibility",
    "section": "5.1 Beyond the Scope",
    "text": "5.1 Beyond the Scope\nA suitable approach to reproducibility considers the data, analysis, and other characteristcis of the research project (Chen et al., 2019). Lee et al., 2018- Other aspects of recommendations are beyond the scope of empirical research, such as error messages, help commands and documentation for APIs (application programming interfaces). Garijo et al., 2022 - some recommendations are perhaps beyond the scope of a single empirical research project - e.g. considering policies on software contributions and long-term maintenance. Provides guidance on managing research software repositories. Reproducing the environment used to generate a container is an additional challenge to the sustainability of this solution (Grüning et al., 2018). Peikert and Brandmeier, 2021 - Make (and Makefiles) outline the sequence in which programmes should be executed, so is appropriate when a workflow involves significant processing of data prior to analysis. Nüst et al., 2017 - introduce the notion of Executable Research Compendia, providing data, code, computational environment, and interface. The concept is that this is ‘self-contained’. This follows the development of Research Compendia (Gentleman and Lang, 2007), which focused on providing data and analysis code, and use of dynamic documents. Executable Research Compendia are a way of packaging research resources/output. Key to the notion of Executable Research Compendia is being providing a ‘one-click’ functionality that indicates successful reproduction of all computational processes, resulting in an exact correspondence between generated and reported results. However, perfect reproducibility only indicates that computational processes run as expected, not that the conclusions of the research independently valid. Continuous integration offers the ability to detect whether each new change has not prevented successful outputs/compilation (FitzJohn et al., 2014). Whilst I did not follow a specific pre-defined workflow, my approach closely resembles published workflows (e.g., van Lissa et al., 2020; Peikert and Brandmeier, 2021). [end of beyond the scope]\nWhite et al. 2013 - Figshare is an ‘all-purpose’ solution. Bahaidarah et al., 2022 - reproducibility in research is ‘essential’ Klein et al. (2018) - access to the constituent parts of research accelerates scientific progress (Ioannidis, 2012). Marwick et al., 2018 - research compendia organise resources for reproducibility. Bertin and Baumer’s (2020) package also tracks dependencies.\nThe challenges are no longer completely depending on innovations in technology, but researchers’ engagement with these tools (Grüning et al., 2018).\nThe authors present a package which supports reproducible practices. Users who specify a local working directory are confronted with an error which instructs them to choose an appropriate file path instead. Also encourages use of functionality which sets the random seed (Bertin and Baumer, 2020).\nRather than focusing on absolute best practices, Wilson et al. (2015) present a starting point for repoducible working, smaller steps towards fully reproducible work.\nResearchers should also ensure that variable names are comprehensible (Wilson et al., 2015) and avoid spaces in file names in order to facilitate programming (White et al., 2013).\nWrite functions to avoid duplication, make code modular (Rule et al., 2019). Literate programming facilitates comprehension of code (Piccolo and Frampton, 2016). Varaibles should be appropriately named and explained (Knuth, 1984). It’s useful to pair textual description and explanation with corresponding analysis code, literate programming tools make this easier (Sandve et al., 2013).\nKnitr provides opportunities for literate programming (FitzJohn et al., 2014).\nComputational notebooks (e.g. Jupyter notebooks) integrate several aspects of analysis workflow into a single document (Rule et al., 2019). Rmarkdown provides transparency across the entire analytical workflow (Holmes et al., 2021). Dynamic documents promote use of literate programming, resulting in reproducible manuscripts (Peikert and Brandmeier, 2021). Avoid spaces in files names (White et al. 2013). Definition of replicability as generation of new data to further examine a reported finding (Simons, 2014)\nClaerbout uses reproducibility as I would - but replication differently - see Rougier et al. (2017) Stodden et al., 2015 - different types of reproducibility: empirical (robustness of methods and their reporting), computational (robustness of software), analytical (robustness of analysis, Stodden, 2011, 2013).\nIndependent provides (.e.g OSF) provide a range of useful features (Klein et al., 2018).\nWhite et al. 2013 - Avoid simple errors like failing to separate values from units and ambiguous blank cells.\nChen et al., 2019 - suggests moving simply beyond notions openness and transparent, and the importance of whether such resources are usable. This does involve preserving a workflow, licensing, using established repositories. Nüst et al., 2017 - Sharing is not sufficient becuase what is shared must be comprehensible.\nSmith et al., 2016 - citations of software should be considered equally important as citations of research. The authors should be credited, versions and variants should be named, and associated resources should be easily accessible. Methods for identifying citations should be persistent and unique to the software. Collected 601 papers and attemped ‘weak repeatability’ - finding and executing programs that supported the work. Of elibigle papers, under half could be successfully executed by the authors. In many cases, the software was not publically available. Various issues in reproduction include sharing non-final versions of code, original programmer no longer available (Collberg & Proebsting, 2016). Studied 204 papers from a journal which required authors to provide data and code on request - results were successfully reproduced in only 26% of cases (Stodden et al., 2018). Very few files analysed were R markdown, which allows dynamic document generation, which would expose initial researchers to more errors. Errors also arose due to incompatibility of R package versions, which reveals the benefits of containerisation/capturing the computational environment. Recommend renv, relative file paths, and Docker (Trisovic et al., 2022). Recording the dependencies is necessary (Grüning et al., 2018). Computational notebooks and tools for dynamic document generation (Jupyter, R Markdown) do not capture the computational environment (Grüning et al., 2018).\nDockerfiles capture a greater level of dependencies than makefiles, because they can capture software versions (Boettiger, 2015). Nüst et al., 2020b - It is necessary to capture the specific versions of packages and other software.\nNüst et al., 2020b - Package managers may be used in concert with Dockerfiles,\nThat Dockerfiles are human-readable promotes transparency. Tools must be practicable as well as productive. Lack of familiarity is a barrier (Boettiger, 2015). Nüst et al., 2020b - Conventions facilitate comprehnsion, and following guidance can assist less experienced researchers generate useful outputs. Nüst et al., 2020b - Dockerfiles should be both comprehensible to humans and machine-readable. Nüst et al., 2020b - All files required by the Dockerfile must be included in the repository. Nüst et al., 2020b - Even old Docker containers may not function properly.\nNüst et al., 2017 - Docker cannot be used in conjuction with proprietary software. Nüst et al., 2020b - Proprietary and GUI software are not compatible with containers.\nNüst et al., 2020b - Caching is another consideration.\nHaroz (2018) - journal articles alone do not provide a complete picture of a research project, so data and code must be openly accessible for the research to be considered reliable/trustworthy. Haroz (2018) - Certain sites are less reliable than others because they are easily modified, or are short-lived, becoming afflicted with ‘link-rot’.\nKosara and Haroz suggest that the absence of a replication crisis in the visualisation field doesn’t necessarily point to lack of an issue but rather a lack of replications (though they note a handful of conceptual replications). Their papers discusses several features of poor-quality/sub-standard empirical work that might invalidate a study’s conclusions. Along with other issues (including excessive research degrees of freedom and experimental design issues). Sharing everything allows others to carry out all three types of replication to be achieved - including those which are typically challenging - reanalysis and direct replication.\nEffective data management facilitates advances in knowledge. Machine readability is important for using the ever-increasing proliferation of data (Wilkinson et al., 2016).\nThe FAIR principles (Wilkinson et al., 2016) were developed to facilitate use of data by machines. Documentation to introduce each program (Wilson et al., 2015).\nLee et al., 2018 - The contents of a README should cover elementary information such as setup instructions and licence. Different types of resources require different metadata implementations/considerations. Metadata provides an important contribution towards reproducbility (Leipzig et al., 2021).\nData files should not include blank cells, which introduce ambiguity regarding whether data points are missing (White et al. 2013).\nThrough version control, archiving of changes to a document does not rely solely on user’s descriptions but digital records of actual modifications (Wilson et al. 2015). Sandve et al. 2013 - Version control facilitates inspection of prior versions of code, allowing the stability of results to be assessed across different versions. Maintaining a record of intermediate steps facilitates diagnosis of issues, plus inspection of entire approach preceding conclusions.  Using github to support computational reproducibility (Perez-Riverol et al., 2016)\nStodden et al. 2013 - irreproducible computational working practices are akin to poor record keeping. The expectations around documentation of software do not match the care required for other aspects of research. The typical researcher’s documentation of software is inconsistent with their attention to detail in other aspects of research.\nComputational working practices should receive the same attention to detail typically applied to other aspects of research (Stodden et al., 2013), yet research code is not usually produced with sustainability in mind (Trisovic et al., 2022)."
  },
  {
    "objectID": "axis-extension/axis-extension.html",
    "href": "axis-extension/axis-extension.html",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "",
    "text": "5 Abstract\nConsider a statistic corresponding to the number of public transport users in a particular town. Gauging whether this number is large or small requires awareness of the total population (the denominator). In data visualisations, an axis extended beyond plotted values can act as a graphical cue to a denominator value, but default axis upper limits (e.g., in ggplot2) are typically based on the highest plotted value. In two experiments (combined N = 350), we explore the influence of default and extended axes on interpretations of the magnitude in bar charts. We also investigate the influence of accompanying denominator information on participants’ assessments. We observe that values plotted using default axes were rated as higher, compared to values plotted using extended axes. The absence of denominator information amplifies the effect of axis limits on judgements. This demonstrates that axes which incorporate denominator values influence interpretations of presented data. Whereas prior work has often focused on judgements of the differences between values, this work contributes to an understanding of how the magnitudes of the values themselves are interpreted by viewers. We also discuss implications for effective design, which involve considering both axis limits and accompanying contextual information.\nThe question ‘Is it a big number?’ is often raised on the BBC radio programme More or Less when probing eye-catching statistics. A figure of several million pounds may initially seem large, but may represent a small proportion of total government spending. Awareness of a denominator value can influence judgement of a number’s magnitude. In data visualisation, this contextual information can be displayed by extending an axis to accommodate the denominator value. However, this approach is infrequently used, since typical default axis settings are based on plotted data only. In this study, we investigate how these axis limits affect interpretations of how large or small plotted values are.\nAxis limits can be easily manipulated in common data visualisation software, in order to include a visual cue to denominator information. However, these defaults are based on plotted data only, so often omit denominator information. We demonstrate that plotted values’ magnitudes were interpreted as smaller for bar charts with axes that extended to the denominator value, rather than those employing default axis settings. The influence of axis limits was particularly large when no denominator information was included in the text accompanying a chart. This provides insight into the cognitive process involved in magnitude judgements, indicating that denominator information is an important aspect in interpreting plotted values in bar charts.\nIn Experiment 1, we identified a framing effect, wherein charts with axes that accommodated a denominator value elicited smaller magnitude judgements compared to charts with default axes. In both conditions, the denominator was explicitly presented in the text. Additionally, some extreme responses in the default condition appeared to represent a disregard for denominator information. Given the apparent importance of this information, we conducted another experiment in order to examine the denominator’s role in the cognitive processing of magnitude. We examined how interpretations were affected by the absence of denominator information, thus capturing how this information was incorporated differently across chart designs.\nExperiment 2 makes several additional contributions. First, it replicated the main effect from Experiment 1. That is, we observed a propensity to interpret magnitudes as smaller when values were shown with an extended axis, rather than a default axis. Second, it illustrates the impact of including the denominator in accompanying text. This cue affects viewers’ interpretations differently depending on whether a chart’s axis also incorporates the same value. Without denominator information in text, the magnitude of values plotted using default axes can be ambiguous. Accordingly, drastically higher ratings in the absence of denominator information illustrate the denominator’s role in reducing ambiguity. Interpretation of values plotted using extended axes was affected to a lesser extent by the denominator’s absence. Thus, the impact of a bar chart’s axis is greater when not accompanied by a denominator. This suggests axis limits facilitate recognition of denominator information when interpreting magnitudes.\nThird, Experiment 2 replicated the pattern of responses observed in Experiment 1 for charts with default axes and accompanying denominator information. This pattern consists of a small number of higher magnitude ratings, in contrast to the general tendency for lower magnitude ratings. Figure 8.2 reveals a close resemblance between the distribution of these higher ratings and the overall distribution of ratings for default charts without accompanying denominator information. This suggests that these extreme ratings may share a cause. Unusually high responses in the presence of denominator information likely result from failure to account for the denominator and a subsequent reliance on the chart’s appearance. The analogous responses to charts without accompanying denominators (Experiment 2) can be considered an experimentally-induced instance of the same effect.\nFourth, additional ratings collected in Experiment 2 provide insight into participants’ confidence. Although analysis of these ratings indicated a main effect of axis limits and an interaction between denominator information and axis limits, the minuscule effect sizes cast doubt over the practical significance of these effects. In spite of this, absence of a denominator clearly lowered confidence. This suggests that participants were hesitant to form magnitude judgements based solely on a bar chart’s appearance. Inclusion of a denominator value in text was preferred regardless of graphical cues to context."
  },
  {
    "objectID": "axis-extension/axis-extension.html#related-work",
    "href": "axis-extension/axis-extension.html#related-work",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "6.1 Related Work",
    "text": "6.1 Related Work\nA wealth of research demonstrates biases in the interpretation of numbers. A survival rate elicits different judgements of a disease compared to its corresponding mortality rate (Tversky and Kahneman, 1981), the fat content of a meat product elicits different judgements of the meat compared to its corresponding lean content (Levin, 1987). The units used to express the same values (e.g., months vs. years) affect comparisons (Burson et al., 2009; Monga and Bagchi, 2012) and also interpretations of precision and accuracy (Zhang and Schwarz, 2012).\nVarious biases in magnitude judgements reveal the importance of accounting for numerical context. Base rate neglect describes difficulty acknowledging population-level characteristics when making judgements about a sample (Cosmides and Tooby, 1996). Format neglect describes a bias against incorporating set size information when judging percentage formats (top 20%) and numerical formats (top 10, (Sevilla et al., 2018)). Denominator neglect occurs in judgements which overweight numerator information at the expense of denominator information (Reyna and Brainerd, 2008). The latter also leads (in part) to large percentages of a small number appearing greater than the numerically equivalent smaller percentages of a larger number (Li and Chapman, 2013). The general mechanism responsible for these biases is a failure to properly acknowledge numerical context.\nVisualisations can help combat biases. Denominator information becomes visually available when icon arrays present both focal outcomes (e.g., number deceased) and also alternative outcomes (e.g., number survived). Research suggests that the combined array acts as a visual cue to the denominator (e.g., total number at risk), facilitating reasoning. For example, including alternative outcomes in an icon array, and therefore displaying denominator information, reduces denominator neglect, increasing comprehension of relative risk (Garcia-Retamero and Galesic, 2010). Icon arrays displaying both types of outcome are particularly helpful for understanding datasets with unequal denominators, and for individuals with high graph literacy (Okan et al., 2012). However, these effects are largest when depicting small probabilities (Okan et al., 2020).\nStacked bar charts function similarly to icon arrays: lower bars represent the focal outcome, upper bar represent the alternative outcome, and their combination represents the denominator. Like icon arrays, stacked bar charts lessen the influence of denominator neglect (Stone et al., 2003). However, denominator information can be displayed in bar charts without using additional stacked bars representing alternative outcomes. Extending an axis to incorporate the denominator value also communicates relevant numerical context. In this case, the blank space between the bars for focal outcomes and the upper axis limit corresponds to the alternative outcomes. Research has demonstrated that bar charts representing alternative outcomes using blank space increase perceptions of risk likelihood compared to those representing alternative outcomes using stacked bars (Stone et al., 2017). This research did not examine how presenting denominator information influences interpretation, since identical axis limits were employed across conditions.\nDirectly manipulating bar charts’ upper axis limits provides insight into use of this source of denominator information. Bar charts with axes which extended to the denominator value produce more accurate estimates of changes in risk (Garcia-Retamero and Galesic, 2010). This design also elicits decreased ratings of risk perception (Okan et al., 2018), though numerical labels reduce this effect. In both studies, accompanying text included the denominator value. These studies demonstrate that extending axes to incorporate denominator values influences interpretation of risk. However, they do not provide specific evidence on how interpretations of plotted values’ magnitudes are affected. Garcia-Retamero and Galesic (2010) measured risk understanding: how faithfully participants represented the exact numbers displayed. Only assessing comprehension fails to capture individuals’ impressions of plotted information (Feldman-Stewart et al., 2007). Understanding the ‘gist’ obtained from a visualisation is crucial since this takes precedence over ‘verbatim’ information when making decisions (Reyna, 2008). Reyna (2008) argues that assessing gist requires consideration of how plotted values’ magnitudes are interpreted, since plotted values’ relative differences are only one aspect of a dataset conveyed by a visualisation. Indeed, Okan et al. (2018) collected magnitude ratings, yet these cannot be examined in isolation, since they were assimilated into a combined measure of perceived risk, along with ratings of the degree of difference between plotted values. Outside the risk communication literature, a substantial body of research has demonstrated that judgements of the difference between values change as a function of axis range (Correll et al., 2020; Pandey et al., 2015; Witt, 2019; Yang et al., 2021). In spite of this, research has neglected the effects of axis range on judgements of the magnitude of the values themselves. This is the focus of experiments presented in this work.\nVarious accounts have sought to explain the consequences of including denominator information in visualisations. Depicting denominators may facilitate understanding of part-to-whole relationships, diminishing class-inclusion errors associated with denominator neglect (Reyna, 2008). This argument is consistent with Fuzzy Trace Theory, which also predicts the influence of physical attributes in gist representations, such that the appearance of short bars in charts with extended axes may contribute to smaller magnitude judgements (Reyna, 2008). Additionally, (Stone et al., 2018) suggest facilitation of proportional reasoning may be largely responsible for observed effects. Increasing the salience of the denominator in text fails to affect judgements, yet a graphical representation effectively communicates the true scale of the denominator, helping put numerators into perspective.\nPrior work on extending axes, discussed above, did not disclose methods for determining axis limits in charts without denominators. The values used as upper limits appear to be arbitrary. In the present study, we increase ecological validity by employing default axis limits from {ggplot2} (Wickham, 2016), a popular visualisation tool used in the R programming environment. Furthermore, previous studies’ statistical power and generalisability have been limited by the use of one (Okan et al., 2018) or two (Garcia-Retamero and Galesic, 2010) trials per participant. Our experiments explore a range of scenarios (32 experimental trials per participant). The data presented are also unrelated to risk judgements, the domain of this prior work.\n\n6.1.0.1 Open Research Statement\nData, analysis code, and pre-registrations are available at osf.io/854uc/."
  },
  {
    "objectID": "axis-extension/axis-extension.html#bibliography-2",
    "href": "axis-extension/axis-extension.html#bibliography-2",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "References",
    "text": "References\n\n\nBurson KA, Larrick RP, Lynch JG. 2009. Six of One, Half Dozen of the Other: Expanding and Contracting Numerical Dimensions Produces Preference Reversals. Psychological Science 20:1074–1078. doi:10.1111/j.1467-9280.2009.02394.x\n\n\nCorrell M, Bertini E, Franconeri S. 2020. Truncating the Y-Axis: Threat or Menace?Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. Honolulu HI USA: ACM. pp. 1–12. doi:10.1145/3313831.3376222\n\n\nCosmides L, Tooby J. 1996. Are humans good intuitive statisticians after all? Rethinking some conclusions from the literature on judgment under uncertainty. Cognition 58:1–73. doi:10.1016/0010-0277(95)00664-8\n\n\nFeldman-Stewart D, Brundage MD, Zotov V. 2007. Further Insight into the Perception of Quantitative Information: Judgments of Gist in Treatment Decisions. Medical Decision Making 27:34–43. doi:10.1177/0272989X06297101\n\n\nGarcia-Retamero R, Galesic M. 2010. Who profits from visual aids: Overcoming challenges in people’s understanding of risks. Social Science & Medicine 70:1019–1025. doi:10.1016/j.socscimed.2009.11.031\n\n\nLevin IP. 1987. Associative effects of information framing. Bulletin of the Psychonomic Society 25:85–86. doi:10.3758/BF03330291\n\n\nLi M, Chapman GB. 2013. A big fish or a small pond? Framing effects in percentages. Organizational Behavior and Human Decision Processes 122:190–199. doi:10.1016/j.obhdp.2013.07.003\n\n\nMonga A, Bagchi R. 2012. Years, Months, and Days versus 1, 12, and 365: The Influence of Units versus Numbers. Journal of Consumer Research 39:185–198. doi:10.1086/662039\n\n\nOkan Y, Garcia-Retamero R, Cokely ET, Maldonado A. 2012. Individual Differences in Graph Literacy: Overcoming Denominator Neglect in Risk Comprehension. Journal of Behavioral Decision Making 25:390–401. doi:10.1002/bdm.751\n\n\nOkan Y, Stone ER, Bruine De Bruin W. 2018. Designing Graphs that Promote Both Risk Understanding and Behavior Change: Graphs Promoting Risk Understanding and Behavior Change. Risk Analysis 38:929–946. doi:10.1111/risa.12895\n\n\nOkan Y, Stone ER, Parillo J, Bruine de Bruin W, Parker AM. 2020. Probability Size Matters: The Effect of Foreground‐Only versus Foreground+Background Graphs on Risk Aversion Diminishes with Larger Probabilities. Risk Analysis 40:771–788. doi:10.1111/risa.13431\n\n\nPandey AV, Rall K, Satterthwaite ML, Nov O, Bertini E. 2015. How Deceptive are Deceptive Visualizations?: An Empirical Analysis of Common Distortion TechniquesProceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI ’15. Seoul, Republic of Korea: ACM Press. pp. 1469–1478. doi:10.1145/2702123.2702608\n\n\nReyna VF. 2008. A Theory of Medical Decision Making and Health: Fuzzy Trace Theory. Medical Decision Making 28:850–865. doi:10.1177/0272989X08327066\n\n\nReyna VF, Brainerd CJ. 2008. Numeracy, ratio bias, and denominator neglect in judgments of risk and probability. Learning and Individual Differences 18:89–107. doi:10.1016/j.lindif.2007.03.011\n\n\nSevilla J, Isaac MS, Bagchi R. 2018. Format Neglect: How the Use of Numerical Versus Percentage Rank Claims Influences Consumer Judgments. Journal of Marketing 82:150–164. doi:10.1177/0022242918805455\n\n\nStone ER, Bruin W, Wilkins AM, Boker EM, MacDonald Gibson J. 2017. Designing Graphs to Communicate Risks: Understanding How the Choice of Graphical Format Influences Decision Making. Risk Analysis 37:612–628. doi:10.1111/risa.12660\n\n\nStone ER, Reeder EC, Parillo J, Long C, Walb L. 2018. Salience Versus Proportional Reasoning: Rethinking the Mechanism Behind Graphical Display Effects: Proportional Reasoning. Journal of Behavioral Decision Making 31:473–486. doi:10.1002/bdm.2051\n\n\nStone ER, Sieck WR, Bull BE, Frank Yates J, Parks SC, Rush CJ. 2003. Foreground:background salience: Explaining the effects of graphical displays on risk avoidance. Organizational Behavior and Human Decision Processes 90:19–36. doi:10.1016/S0749-5978(03)00003-7\n\n\nTversky A, Kahneman D. 1981. The Framing of Decisions and the Psychology of Choice. Science 211:453–458. doi:10.1126/science.7455683\n\n\nWickham H. 2016. ggplot2. New York, NY: Springer Science+Business Media, LLC.\n\n\nWitt JK. 2019. Graph Construction: An Empirical Investigation on Setting the Range of the Y-Axis. Meta-Psychology 2:1–20. doi:10.15626/MP.2018.895\n\n\nYang BW, Vargas Restrepo C, Stanley ML, Marsh EJ. 2021. Truncating Bar Graphs Persistently Misleads Viewers. Journal of Applied Research in Memory and Cognition S2211368120300978. doi:10.1016/j.jarmac.2020.10.002\n\n\nZhang YC, Schwarz N. 2012. How and Why 1 Year Differs from 365 Days: A Conversational Logic Analysis of Inferences from the Granularity of Quantitative Expressions. Journal of Consumer Research 39:248–259. doi:10.1086/662612"
  },
  {
    "objectID": "axis-extension/axis-extension.html#introduction-1",
    "href": "axis-extension/axis-extension.html#introduction-1",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThis experiment investigates the influence of axis limits on interpretations of plotted values’ magnitude. Participants viewed bar charts with default axes, or axes which extended to a denominator value well above the bars. Comparing participants’ interpretations captures the influence of displaying the same data with and without numerical context."
  },
  {
    "objectID": "axis-extension/axis-extension.html#method",
    "href": "axis-extension/axis-extension.html#method",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "7.2 Method",
    "text": "7.2 Method\n\n7.2.1 Materials\nWe developed 40 scenarios about fictitious studies. Each study evaluated a specific outcome across five categories (e.g., the number of items produced without defects, for five manufacturing methods). The denominator (e.g., total number of items produced) was identical for each category.\nWe generated bar charts in R (R Core Team, 2022) using {ggplot2} (version 4.1.2), {tidyverse} (version 1.3.1) and {ggh4x} (version 0.2.1). The two versions of each chart displayed the same five values, but employed different y-axis limits. Denominator values (400, 500, or 600) were used to generate datasets: data were sampled from normal distribution with a mean equal to either 20% or 40% of a given denominator value, and a standard deviation equal to 1% of the denominator value.\nFor charts with extended axes, the denominator value was used as the y-axis upper limit. The other charts used a y-axis upper limit which was dictated by ggplot2’s default axis settings. These settings automatically identify a set of convenient breaks for each dataset, then slightly extend the plot area, adding an additional 5% of the axis range. In both conditions, a smaller expansion factor of 1% was applied to the lower axis limit, in order to eliminate visible space below the 0 baseline. Figure 7.1 shows example charts for both conditions.\n\n\n\n\n\nFigure 7.1: Example charts for Experiment 1. The same data appears in both charts. Accompanying text explained what the values represented: ‘The graph shows, for each manufacturing method, how many of the items were free from defects’. The chart with the default axes (left) employs an upper limit determined by ggplot2. The chart with the extended axes (right) employs an upper limit equal to the dataset’s denominator value.\n\n\n\n\nFor the majority of datasets generated, the default settings produced charts where the highest gridline did not exceed the tallest bar. For consistency, when the opposite situation occurred, we used a different random seed to generate an alternative dataset for both conditions. 10% of datasets used were generated using this method.\nIn experimental trials (32 total), plotted values consisted of relatively small proportions of the dataset’s denominator value (roughly 20% or 40%). To introduce variety and encourage attention, eight filler trials showed plotted values which were roughly 90% of the corresponding denominator value. Denominators for filler trials were selected so that numerical labels on the y-axis would approximately resemble either extended or default bar charts from experimental trials.\nWe included six attention check trials to assess participants’ engagement with the task. These trials were similar to experimental and filler trials, consisting of text, a bar chart, a question and a visual analogue scale. However, participants were instructed to ignore the bar chart and provide a specified response on the visual analogue scale.\n\n\n7.2.2 Design\nWe employed a within-participants design: participants viewed 16 different charts in each of the two conditions (32 experimental trials total). The correspondence between scenarios and conditions was counterbalanced using two lists. However, all participants saw the same versions of the eight filler items and six attention check items. There were a total of 46 trials, which were presented in a random order.\n\n\n7.2.3 Participants\nParticipants were recruited using Prolific.co. The experiment was advertised to fluent English speakers with normal-or-corrected to normal vision, who had previously participated in at least 100 studies on the site.\nData were returned by 157 participants. Per pre-registered exclusion criteria, seven participants’ submissions were rejected because they answered more than one of six attention check questions incorrectly. Participants whose submissions were accepted received £3.50.\nThe final sample consisted of 150 participants (57.33% male, 39.33% female, 3.33% non-binary). Mean age was 33.18 years (SD = 12.93). The mean data visualisation literacy score was 21.35 (SD = 4.73), out of a maximum of 30.\nThis experiment was approved by the University of Manchester’s Division of Neuroscience and Experimental Psychology Ethics Committee (ethics code: 2022-11115-24245).\n\n\n7.2.4 Procedure\nWe programmed the experiment using PsychoPy (version 2022.1.4, (Peirce et al., 2019)). Participants were instructed to carry out the experiment using a laptop or desktop computer (not a mobile phone or tablet). After providing informed consent, participants completed a demographic questionnaire and Garcia-Retamero et al.’s (Garcia-Retamero et al., 2016) five-item subjective data visualisation literacy scale.\nParticipants were asked to imagine they were a researcher tasked with determining the outcome of experiments and surveys. They were instructed to make an overall assessment of all data presented in a graph after studying the text, graph, and question. All questions asked about plotted values’ magnitudes (e.g., ‘How successful were the manufacturing methods?’), with participants responding on visual analogue scales with anchors at the extremes (e.g., ‘very unsuccessful’, ‘very successful’). Figure 7.2 shows an example trial.\n\n\n\n\n\nFigure 7.2: An example trial from Experiment 1, showing a bar chart with a default axis limit.\n\n\n\n\nParticipants were permitted to move the response marker as many times as they liked before proceeding to the next trial, but could not return to previous trials. The response scale’s granularity was altered for each attention check item, such that participants could only respond at the extremes or the middle of the scale. Finally, participants were informed that all data presented was fictitious and were given the option to provide comments on the experiment and describe any strategies used. Average completion time was 19.2 minutes."
  },
  {
    "objectID": "axis-extension/axis-extension.html#analysis",
    "href": "axis-extension/axis-extension.html#analysis",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "7.3 Analysis",
    "text": "7.3 Analysis\nWe conducted analysis using R (R Core Team, 2022) (version 4.2.1). Linear mixed models were built using {lme4} (Bates et al., 2015). Each model was based on a maximal model with by-participant and by-item random effects (Barr et al., 2013), and {buildmer} (Voeten, 2022) was used to identify the final random effects structure, ensuring convergence and removing terms not significantly contributing to explaining variance.\n\n7.3.1 Magnitude Ratings\nFigure 7.3 shows the distribution of ratings for charts with default axes and extended axes.\n\n\n\n\n\nFigure 7.3: The distribution of visual analogue scale ratings in response to default and extended axis limits. Each circle represents a participant’s response to an individual bar chart.\n\n\n\n\nLinear mixed-effects modelling revealed that participants awarded higher ratings to charts with default axes, compared to charts with extended axes: F(1, 152.54) = 44.90, p < .001, partial \\(\\eta^2\\) = 0.23.\nThis model employed a maximal random effects structure, capturing the baseline responses (intercepts) and differences between the two axis settings (slopes) separately for each individual participant and each individual item.\n\n\n7.3.2 Magnitude Ratings and Data Visualisation Literacy\nAccounting for differences in data visualisation literacy did not change the significant effect of axis limits: F(1, 152.57) = 44.95, p < .001, partial \\(\\eta^2\\) = 0.23."
  },
  {
    "objectID": "axis-extension/axis-extension.html#discussion",
    "href": "axis-extension/axis-extension.html#discussion",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "7.4 Discussion",
    "text": "7.4 Discussion\nThis experiment explored the consequences of including a graphical cue to a denominator value using bar charts’ axes. We observed that plotted values’ magnitudes were interpreted as smaller when a default axis limit was used, compared to an axis limit equal to the dataset’s denominator value. Therefore, assessments of data were biased by the presence or absence of numerical context in bar charts.\nDenominator information informs magnitude judgements. In bar charts with extended axes, denominator information was available through the accompanying text and through axes. Comparison with bar charts employing default axes, where denominator information was only available through text, reveals the contribution of the graphical cue to the denominator value. Inconsistency in the differences between conditions illustrates variation in interpretation. The relative similarity of lower magnitude ratings across conditions indicates some attention to denominator information in the absence of a graphical cue. However, some extreme high magnitude ratings suggest that the appearance of tall bars carried the implication of large values. These ratings may indicate a failure to account for denominator information in the absence of a graphical cue. We investigate the role of denominator information further in Experiment 2."
  },
  {
    "objectID": "axis-extension/axis-extension.html#bibliography-3",
    "href": "axis-extension/axis-extension.html#bibliography-3",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "References",
    "text": "References\n\n\nBarr DJ, Levy R, Scheepers C, Tily HJ. 2013. Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language 68:255–278. doi:10.1016/j.jml.2012.11.001\n\n\nBates D, Mächler M, Bolker B, Walker S. 2015. Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software 67. doi:10.18637/jss.v067.i01\n\n\nGarcia-Retamero R, Cokely ET, Ghazal S, Joeris A. 2016. Measuring Graph Literacy without a Test: A Brief Subjective Assessment. Medical Decision Making 36:854–867. doi:10.1177/0272989X16655334\n\n\nPeirce J, Gray JR, Simpson S, MacAskill M, Höchenberger R, Sogo H, Kastman E, Lindeløv JK. 2019. PsychoPy2: Experiments in behavior made easy. Behavior Research Methods 51:195–203. doi:10.3758/s13428-018-01193-y\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing.\n\n\nVoeten CC. 2022. Buildmer: Stepwise Elimination and Term Reordering for Mixed-Effects."
  },
  {
    "objectID": "axis-extension/axis-extension.html#introduction-2",
    "href": "axis-extension/axis-extension.html#introduction-2",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nExperiment 1 found differences in interpretations of data presented using different axes limits. Overall, plotted data were associated with lower magnitudes when presenting using axes which extended to a denominator value. Compared to bar charts with extended axes, charts with no graphical cue to a denominator value elicited a wider variety of responses. This variety appears to reflect differences in how the denominator information supplied in accompanying text is used in magnitude judgements. This raises questions about how text, including denominator information, might influence the interpretation of different chart designs.\nBy manipulating the presence of denominator information in accompanying text, in addition to manipulating axis limits, we investigate how these textual and graphical cues inform assessments of data. This allows us to understand how different chart designs are interpreted with and without additional numerical context. This 2x2 design also allows us to determine whether we can replicate the findings from Experiment 1 and also gives us the opportunity to explore whether ratings in the absence of denominator information correspond to the previously observed pattern of extreme ratings.\nThis second experiment requires minor adaptations to materials and procedure. First, there is a risk that highly ambiguous trials without denominator information supplied in text will elicit unreliable random ratings. Therefore, we collect additional confidence ratings to directly index this aspect of participants’ evaluations. This provides a more comprehensive view of participants’ cognitive states and interpretations. Second, when denominators are not supplied in text, participants may use denominator values supplied in previous trials to inform their judgements. A limited range of denominators (as in Experiment 1) would artificially diminish uncertainty regarding possible values, inhibiting authentic, spontaneous judgements. Therefore, we expand the range of denominator values in Experiment 2. Third, increasing the number of fillers (which depict relatively high magnitudes) to match the number of experimental items (which depict relatively low magnitudes) will avoid priming effects by ensuring high and low magnitudes seem equally plausible."
  },
  {
    "objectID": "axis-extension/axis-extension.html#method-1",
    "href": "axis-extension/axis-extension.html#method-1",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "8.2 Method",
    "text": "8.2 Method\n\n8.2.1 Materials\nWe generated bar charts in R using {ggplot2} (version 4.2.1), {tidyverse} (version 1.3.2) and {ggh4x} (version 0.2.3).\nBar charts were generated using the same method as in Experiment 1. We used the same scenarios from Experiment 1, and generated 24 new scenarios for use as additional filler items, thus employing 32 experimental items and 32 filler items. To increase variation across datasets, we employed a wider range of denominators (200, 400, 600, and 800) meaning the plotted values differed from Experiment 1.\nWe added the word ‘surveyed’ or ‘assessed’ to the accompanying text for seven items where the absence of a denominator may have implied that data were collected for the entire population under study. For example, where the study concerned data collected in five towns, the final sentence read ‘The graph shows, for each town, how many people surveyed used public transport regularly’, to avoid the implication that the denominator was equal to an entire town’s population. This ensured that the inclusion of denominator values was equally informative across all scenarios.\n16% of datasets used were re-generated to ensure that the highest gridline of a default axis did not exceed the highest plotted value.\n\n\n8.2.2 Design\nWe employed a within-participants 2x2 Latin-squared design with two factors: axis limits (default vs. extended) and denominator information (present vs. absent). Participants viewed 8 different charts for each combination of conditions (32 experimental trials total). The correspondence between scenarios and conditions was counterbalanced using four lists. However, all participants saw the same versions of the 32 filler items and six attention check items.\n\n\n8.2.3 Participants\nParticipants were recruited using Prolific.co, using the same inclusion criteria as Experiment 1. Additionally, the experiment was not advertised to individuals who completed Experiment 1.\nData were returned by 208 participants. Per pre-registered exclusion criteria, eight participants’ submissions were rejected because they answered more than one of six attention check questions incorrectly. Participants whose submissions were accepted received £5.00.\nThe final sample consisted of 200 participants (60.00% male, 38.00% female, 1.00% non-binary, 0.50% other, 0.50% prefer not to say). Mean age was 33.17 years (SD = 10.34)1. The mean data visualisation literacy score was 21.72 (SD = 4.84), out of a maximum of 30.\nThis experiment was approved by the University of Manchester’s Division of Neuroscience and Experimental Psychology Ethics Committee (ethics code: 2023-11115-28428).\n\n\n8.2.4 Procedure\nThe procedure was identical to Experiment 1, except for the addition of a confidence rating, where participants were asked ‘How confident are you in your response?’. The anchors on the response scale were ‘Not very confident’ and ‘Very confident’. Figure 8.1 shows an example trial.\nFor attention check items, participants were asked to provide a specific response on the magnitude rating scale, and a random response on the confidence rating scale.\nAverage completion time was 29.3 minutes.\n\n\n\n\n\nFigure 8.1: An example trial from Experiment 2, showing a bar chart with an extended axis limit. Note the presence of an additional confidence rating scale."
  },
  {
    "objectID": "axis-extension/axis-extension.html#analysis-1",
    "href": "axis-extension/axis-extension.html#analysis-1",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "8.3 Analysis",
    "text": "8.3 Analysis\n\n8.3.1 Magnitude Ratings\nFigure 8.2 shows the distribution of magnitude ratings for charts with default axes and extended axes, where denominators were absent from text, and where they were present.\n\n\n\n\n\nFigure 8.2: The distribution of visual analogue scale ratings in response to default and extended axis limits, shown separately for trials where denominator values were absent from accompanying text (top), and trials where denominator values were present in accompanying text (bottom). Each circle represents a participant’s response to an individual bar chart.\n\n\n\n\nA mixed effects model revealed that charts with default axes elicited higher ratings compared to charts with extended axes (F(1, 198.22) = 311.45, p < .001, partial \\(\\eta^2\\) = 0.61) and charts not accompanied by a denominator in text elicited higher ratings than those accompanied by a denominator (F(1, 82.23) = 380.50, p < .001, partial \\(\\eta^2\\) = 0.82).\nCrucially, there was also a significant interaction between axis limits and denominator information: F(1, 5,741.16) = 1,540.86, p < .001, partial \\(\\eta^2\\) = 0.21. Figure 8.3 plots this interaction.\nPairwise comparisons produced using {emmeans} (Lenth, 2021) revealed that charts with extended and default axes were rated differently when the denominator was present, replicating the effect from Experiment 1 (z = -9.19, p < .001), and also when the denominator was absent (z = -25.35, p < .001). Therefore, the interaction indicates that the magnitude of influence exerted by a bar chart’s axis varied according to whether the denominator was present or absent.\nThis model employed by-participant and by-item random effects. For each participant, there were random intercepts, plus random slopes for axis settings and denominator information. For each item, there were random intercepts, plus random slopes for denominator information.\n\n\n\n\n\nFigure 8.3: The interaction between axis limits and denominator information, for magnitude ratings. Estimated marginal means are generated by the linear mixed model used in analysis. Translucent bars show 95% confidence intervals.\n\n\n\n\n\n\n8.3.2 Magnitude Ratings and Data Visualisation Literacy\nAccounting for differences in data visualisation literacy did not change the significant interaction: F(1, 5,741.14) = 1,540.85, p < .001, partial \\(\\eta^2\\) = 0.21., the main effect of axis limits (F(1, 198.24) = 311.46, p < .001, partial \\(\\eta^2\\) = 0.61) or the main effect of denominator information (F(1, 82.29) = 380.60, p < .001, partial \\(\\eta^2\\) = 0.82).\n\n\n8.3.3 Confidence Ratings\n\n\n\n\n\nFigure 8.4: The distribution of confidence ratings in response to default and extended axis limits, shown separately for trials where denominator values were absent from accompanying text (top), and trials where denominator values were present in accompanying text (bottom). Each circle represents a participant’s response to an individual bar chart.\n\n\n\n\nFigure 8.4 shows the distribution of confidence ratings for charts with default axes and extended axes, where denominators were absent from text, and where they were present.\n\n\n\n\n\nFigure 8.5: The interaction between axis limits and denominator information, for confidence ratings. Estimated marginal means are generated by the linear mixed model used in analysis. Translucent bars show 95% confidence intervals.\n\n\n\n\nA mixed effects model revealed a main effect associated with axis limits (F(1, 199.00) = 5.97, p = .015, partial \\(\\eta^2\\) = 0.03), a main effect associated with denominator information (F(1, 198.99) = 184.93, p < .001, partial \\(\\eta^2\\) = 0.48) and an interaction F(1, 5,799.00) = 27.74, p < .001, partial \\(\\eta^2\\) < .01. This interaction consisted of a difference between extended and default charts when the denominator was absent from text (z = -4.69, p < .001), but no difference between charts when the denominator was present (z = 0.42, p = .988). However, it is clear from Figure 8.5, as well as the partial \\(\\eta^2\\) values, that the effect sizes associated with axis limits and the interaction are trivial.\n\n\n8.3.4 Confidence Ratings and Data Visualisation Literacy\nAccounting for differences in data visualisation literacy did not change the pattern of results. There was a main effect associated with axis limits (F(1, 199.01) = 5.97, p = .015, partial \\(\\eta^2\\) = 0.03) and a main effect associated with denominator information (F(1, 198.99) = 184.93, p < .001, partial \\(\\eta^2\\) = 0.48) and an interaction F(1, 5,798.99) = 27.74, p < .001, partial \\(\\eta^2\\) < .01."
  },
  {
    "objectID": "axis-extension/axis-extension.html#discussion-1",
    "href": "axis-extension/axis-extension.html#discussion-1",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "8.4 Discussion",
    "text": "8.4 Discussion\nThis experiment manipulated bar charts’ axis limits and the presence of denominator values in accompanying text. The results demonstrate that values presented in charts with default axis limits are associated with higher magnitude judgements than charts with extended axes, in both the presence and absence of denominator information. However, the absence of denominator information amplifies this bias. These results also suggest that extreme high magnitude ratings for default charts in the presence of a denominator value may be driven by a failure to incorporate that value into reasoning. Finally, confidence in judgements is reliably affected by the inclusion of denominator information in text."
  },
  {
    "objectID": "axis-extension/axis-extension.html#bibliography-4",
    "href": "axis-extension/axis-extension.html#bibliography-4",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "References",
    "text": "References\n\n\nLenth RV. 2021. Emmeans: Estimated Marginal Means, aka Least-Squares Means."
  },
  {
    "objectID": "axis-extension/axis-extension.html#relationship-to-prior-work",
    "href": "axis-extension/axis-extension.html#relationship-to-prior-work",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "9.1 Relationship to Prior Work",
    "text": "9.1 Relationship to Prior Work\nOur focus on judgements of values’ magnitudes is noteworthy because the vast majority of related work has explored participants’ judgements of differences between values (Correll et al., 2020; Garcia-Retamero and Galesic, 2010; Okan et al., 2020, 2018, 2012; Stone et al., 2018, 2003; Witt, 2019; Yang et al., 2021). Responses to questions about values’ magnitudes have often been obscured through inclusion in composite measures (e.g., Okan et al., 2018), or have been collected to assess comprehension, rather than interpretation (e.g., Garcia-Retamero and Galesic, 2010). As Stone et al. (2015) discuss, failing to consider interpretations of values’ magnitudes reflects two issues. First, neglecting values’ magnitudes overlooks a relevant aspect of numerical information. Second, neglecting participants’ interpretations limits insight into decision-making, which is not simply governed by accurate retrieval of information (see Reyna, 2008).\nWhereas much prior research has been limited to interpretation of risk information (Garcia-Retamero and Galesic, 2010; Okan et al., 2020, 2018, 2012; Stone et al., 2018, 2017, 2015, 2003), we demonstrate that biases in interpretation extend to a wide range of non-risk scenarios. This provides confidence that these findings are widely applicable, and using multiple trials per participant enhances statistical power. When generating charts that do not include denominator values, previous experiments (Garcia-Retamero and Galesic, 2010; Okan et al., 2018) appear to have employed abritary axis limits. By employing axis limits based on {ggplot2}’s default settings, our materials reflect a common practice, enhancing our experiment’s ecological validity.\nWe contribute to a large body of evidence illustrating biases in the interpretation of numerical information, specifically framing effects (Tversky and Kahneman, 1981). Our results are consistent with research demonstrating that manipulating bar charts’ axis limits influences interpretation of plotted values (Garcia-Retamero and Galesic, 2010; Okan et al., 2018). Furthermore, Okan et al. (2018) found that participants’ perceptions of risk were influenced more by bar charts’ axis limits when labels containing numerators and denominators were excluded. Similarly, we observed that interpretations of magnitude were influenced more by bar charts’ axis limits when denominator values were omitted from accompanying text.\nA previous study exploring interpretation of magnitude in bar charts observed different responses according to whether stacked bars or blank space conveyed alternative outcomes (Stone et al., 2017). We demonstrate that manipulating the amount of blank space above bars can elicit different magnitude judgements, without plotting alternative outcomes explicitly. Earlier work investigating (in)consistency in the formats used to display numerators and denominators is also relevant. Stone et al. (2015) found that displaying a value using icons, accompanied by a denominator in text, increased impressions of that value’s magnitude, compared to when both the value and denominator were presented in text. We too found higher ratings when values displayed using bars were only accompanied by a denominator in text, compared to when a corresponding graphical cue to the denominator value was also present.\nAccording to Fuzzy Trace Theory, different interpretations can arise due to different gist-level representations, despite accurate comprehension of presented values (Reyna, 2008). Therefore, access to denominator information in accompanying text did not prevent our chart designs influencing judgements. Encoding of gist is reported to be influenced by the appearance of graphical elements (Reyna, 2008). This suggests that the taller bars in our default axis conditions were responsible for impressions of greater magnitude, compared to shorter bars in our extended axis conditions. That charts with extended axes elicited lower magnitude ratings is also consistent with Stone et al.’s (2018) proportional reasoning account, which suggests that part-to-whole displays facilitate processing of a larger numerical context.\nWe did not find evidence that data visualisation literacy affected our results. This is contrary to the finding that data visualisation literacy predicted the efficacy of using icon arrays to reduce denominator neglect (Okan et al., 2012). However, this is consistent with the finding that the impact of manipulations like ours (axis range, numerical labels) are independent of data visualisation literacy (Okan et al., 2018). This measure may capture whether people have sufficient ability to extract information from a visualisation, rather than predicting the degree to which they will be influenced by subtler design choices (Yang et al., 2021). Numeracy is associated with decreased sensitivity to framing effects (Peters et al., 2006), so this may be a better candidate for understanding individual differences in response to visualisation design."
  },
  {
    "objectID": "axis-extension/axis-extension.html#implications",
    "href": "axis-extension/axis-extension.html#implications",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "9.2 Implications",
    "text": "9.2 Implications\nWhen conveying values’ magnitudes, both axis limits and accompanying text warrant consideration from data visualisation designers. A bar chart produced using default settings is not equivalent to a bar chart with an axis that incorporates a denominator value. Extending an axis in this manner increases consistency in judgements and may provide insurance against individuals who fail to account for accompanying denominator information. Similarly, where constraints prevent inclusion of a denominator value in text, an extended axis should facilitate viewers’ recognition of this numerical context. We observed that confidence ratings were consistently high in the absence of a denominator in text, despite use of an extended axis. Explicitly providing denominator values in text, regardless of graphical cues, would therefore promote viewers’ confidence in their judgements.\nIt is also worth considering situations which may accentuate the observed bias. High cognitive load exacerbates the numerosity bias (Pelham et al., 1994), therefore may also interfere with magnitude judgements. Even when denominator information is supplied in text, high cognitive load could prevent this information from informing interpretations. This would likely increase reliance on bar charts’ appearances, like in Experiment 2. Additionally, assuming that an audience has knowledge of a dataset’s denominator may increase biases in individuals who are unfamiliar with the topic."
  },
  {
    "objectID": "axis-extension/axis-extension.html#limitations-and-future-work",
    "href": "axis-extension/axis-extension.html#limitations-and-future-work",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "9.3 Limitations and Future Work",
    "text": "9.3 Limitations and Future Work\nThis work is concerned with visualisations intended to convey plotted values’ magnitudes. However, design considerations will differ when conveying differences between values. In this case, axis ranges should be determined by the magnitude of the differences (Correll et al., 2020; Witt, 2019; Yang et al., 2021). Consequently, our recommendations are not relevant for all communicative scenarios. However, maintaining awareness of the implication of plotted values’ magnitudes may help avoid misinterpretation of data, even if this type of judgement is not a primary concern.\nOur experiments apply best to controlled scenarios, such as surveys and experiments where all plotted values share the same denominator. These findings may also extend to datasets with unequal denominators, if bars are used to depict proportions or percentages, permitting use of a single meaningful axis limit. However, this design will not be suitable for plotting other types of dataset. We also acknowledge that proportions are not the only factor influencing magnitude judgements: subject matter is also likely to inform assessments. For example, bars clearly depicting one or two hours spent on administrative tasks within a 35-hour work week will still elicit some differences of opinion regarding whether these values are high or low.\nAll materials were produced using {ggplot2}. Therefore, our conclusions about default axis limits only pertain to bar charts created using this package’s settings, though we expect other visualisation libraries’ default settings to elicit similar responses, due to similarity in their behaviour. For uniformity in our materials, we only employed default charts where the highest gridline was positioned below the highest value, since this was the most common visual arrangement. We did not examine the minority of cases where the highest gridline exceeds the highest value. Whether this influences magnitude judgements could be explored in future experiments. In addition, future work should employ decision-making tasks to quantify the impact of axis limits on applied judgements."
  },
  {
    "objectID": "axis-extension/axis-extension.html#conclusion",
    "href": "axis-extension/axis-extension.html#conclusion",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "9.4 Conclusion",
    "text": "9.4 Conclusion\nIn two experiments, we generated evidence on the effects of default and extended axis limits, illustrating the influential role of denominators in gauging magnitude. We provide insight into the cognitive processes involved in interpreting plotted values’ magnitudes in bar charts and offer recommendations for facilitating judgements. Framing effects demonstrate the power of presentation choices on the interpretation of numbers."
  },
  {
    "objectID": "axis-extension/axis-extension.html#bibliography-5",
    "href": "axis-extension/axis-extension.html#bibliography-5",
    "title": "4  Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts",
    "section": "References",
    "text": "References\n\n\nCorrell M, Bertini E, Franconeri S. 2020. Truncating the Y-Axis: Threat or Menace?Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. Honolulu HI USA: ACM. pp. 1–12. doi:10.1145/3313831.3376222\n\n\nGarcia-Retamero R, Galesic M. 2010. Who profits from visual aids: Overcoming challenges in people’s understanding of risks. Social Science & Medicine 70:1019–1025. doi:10.1016/j.socscimed.2009.11.031\n\n\nOkan Y, Garcia-Retamero R, Cokely ET, Maldonado A. 2012. Individual Differences in Graph Literacy: Overcoming Denominator Neglect in Risk Comprehension. Journal of Behavioral Decision Making 25:390–401. doi:10.1002/bdm.751\n\n\nOkan Y, Stone ER, Bruine De Bruin W. 2018. Designing Graphs that Promote Both Risk Understanding and Behavior Change: Graphs Promoting Risk Understanding and Behavior Change. Risk Analysis 38:929–946. doi:10.1111/risa.12895\n\n\nOkan Y, Stone ER, Parillo J, Bruine de Bruin W, Parker AM. 2020. Probability Size Matters: The Effect of Foreground‐Only versus Foreground+Background Graphs on Risk Aversion Diminishes with Larger Probabilities. Risk Analysis 40:771–788. doi:10.1111/risa.13431\n\n\nPelham BW, Sumarta TT, Myaskovsky L. 1994. The Easy Path From Many To Much: The Numerosity Heuristic. Cognitive Psychology 26:103–133. doi:10.1006/cogp.1994.1004\n\n\nPeters E, Västfjäll D, Slovic P, Mertz CK, Mazzocco K, Dickert S. 2006. Numeracy and Decision Making 17.\n\n\nReyna VF. 2008. A Theory of Medical Decision Making and Health: Fuzzy Trace Theory. Medical Decision Making 28:850–865. doi:10.1177/0272989X08327066\n\n\nStone ER, Bruin W, Wilkins AM, Boker EM, MacDonald Gibson J. 2017. Designing Graphs to Communicate Risks: Understanding How the Choice of Graphical Format Influences Decision Making. Risk Analysis 37:612–628. doi:10.1111/risa.12660\n\n\nStone ER, Gabard AR, Groves AE, Lipkus IM. 2015. Effects of Numerical Versus Foreground-Only Icon Displays on Understanding of Risk Magnitudes. Journal of Health Communication 20:1230–1241. doi:10.1080/10810730.2015.1018594\n\n\nStone ER, Reeder EC, Parillo J, Long C, Walb L. 2018. Salience Versus Proportional Reasoning: Rethinking the Mechanism Behind Graphical Display Effects: Proportional Reasoning. Journal of Behavioral Decision Making 31:473–486. doi:10.1002/bdm.2051\n\n\nStone ER, Sieck WR, Bull BE, Frank Yates J, Parks SC, Rush CJ. 2003. Foreground:background salience: Explaining the effects of graphical displays on risk avoidance. Organizational Behavior and Human Decision Processes 90:19–36. doi:10.1016/S0749-5978(03)00003-7\n\n\nTversky A, Kahneman D. 1981. The Framing of Decisions and the Psychology of Choice. Science 211:453–458. doi:10.1126/science.7455683\n\n\nWitt JK. 2019. Graph Construction: An Empirical Investigation on Setting the Range of the Y-Axis. Meta-Psychology 2:1–20. doi:10.15626/MP.2018.895\n\n\nYang BW, Vargas Restrepo C, Stanley ML, Marsh EJ. 2021. Truncating Bar Graphs Persistently Misleads Viewers. Journal of Applied Research in Memory and Cognition S2211368120300978. doi:10.1016/j.jarmac.2020.10.002"
  },
  {
    "objectID": "conclusion.html#main-findings",
    "href": "conclusion.html#main-findings",
    "title": "5  Conclusion",
    "section": "5.1 Main Findings",
    "text": "5.1 Main Findings\nThe three sets of experiments in this thesis consider the same overarching question, but each provides a unique contribution.\nThe study in Chapter 1 manipulated axis limits in dot plots. This manipulation altered the position of plotted values, such that they appeared at high or low positions. After establishing that interpretations of values’ magnitude were affected by axis limits, I explored whether judgements were influenced by data points’ relative positions within axis limits, or data points’ absolute physical positions. An inverted axis orientation made it possible to distinguish between these potential explanations. Despite conventions around displaying magnitude, participants rated magnitudes as greater when values appeared closer to bottom of the visualisation, which was associated with higher numerical values. This illustrates that data points’ relative positions within axis limits inform magnitude judgements.\nThe study in Chapter 2 manipulated the limits of colour legends accompanying choropleth maps. Rather than altering the appearance of plotted values, this manipulation only modified the range of colours and numerical values in colour legends. Participants interpreted magnitude as greater when plotted values’ corresponding positions were at a colour legend’s upper limit, rather than its halfway point. This illustrates that magnitude judgements are informed by data points’ correspondence with axis values, not simply the appearance of plotted values.\nThe study in Chapter 3 manipulated axis limits in bar charts. This manipulation compared upper limits based on ggplot2’s defaults to upper limits corresponding to datasets’ denominator values. After establishing that participants interpreted magnitude as greater in the latter condition, I explored how awareness of numerical context affects the use of graphical cues to magnitude. Removing accompanying text containing datasets’ denominator values increased the degree to which axis limits informs magnitude judgements. This illustrates that knowledge about data points’ numerical context limits the influence of axis limits on interpretations.\nThe stimuli used in these studies reveal different approaches to conveying magnitude using axis limits. Frequencies plotted in bar charts (Chapter 3) were associated with denominators, so upper axis limits were intrisincally linked to datasets. Conversely, measurements plotted in choropleth maps (Chapter 2) upper axis limits were not determined by denominator values. The relative frequencies plotted as percentages in dot plots (Chapter 1) could not be less than 0 or more than 100, so axis limits were subject to logical constraints. Designers must consider the type of data at hand when identifying suitable axis limits for data visualisations. What constitutes context is not uniform.\nOverall, this work provides consistent evidence of framing effects affecting magnitude judgements in data visualisations. Axes served as a frame of reference biasing the interpretation of numbers, thus mental representation of information was influenced by its presentation (Tversky and Kahneman, 1981; Kahneman, 1992). Furthermore, according to models of graph comprehension (Pinker, Carpenter and Shah), visual patterns are encoded prior to comprehension of numerical values and contextualisation of information. Therefore, first impressions elicited by a data visualisation may prejudice overall interpretations (Pandey et al. 2015; Yang et al. 2021)."
  },
  {
    "objectID": "conclusion.html#data-visualisation-literacy",
    "href": "conclusion.html#data-visualisation-literacy",
    "title": "5  Conclusion",
    "section": "5.2 Data Visualisation Literacy",
    "text": "5.2 Data Visualisation Literacy\nData visualisation literacy levels did not account for variation in responses to axis manipulations. This accords with research on axis truncation, which found that the same subjective literacy measure was not associated with the degree of bias in judgements of relative differences (Yang et al., 2021). The same has also been observed using an objective literacy measure (Okan et al., 2018). Another experiment on the same issue, using yet another literacy measure, did observe that data visualisation literacy predicted bias (Driessen et al., 2022). However, a lack of diversity in the sample’s literacy levels diminished confidence in the utility of this measure.\nVariability in participants’ responses is evident, suggesting the data visualisation literacy employed measure was not best placed to capture these differences. The measure may register basic skills for comprehending visualisations, rather than susceptibility to visualisation design (Yang et al., 2021). Ge et al. suggest that critical thinking skills predict the degree of bias elicited by deceptive designs, and produce a literacy measure designed to capture this capacity. However, its considerable length makes it unsuitable for use as a covariate in experimental studies. A shorter literacy measure of critical thinking in comprehension of data visualisations would be valuable."
  },
  {
    "objectID": "conclusion.html#contributions-and-implications",
    "href": "conclusion.html#contributions-and-implications",
    "title": "5  Conclusion",
    "section": "5.3 Contributions and Implications",
    "text": "5.3 Contributions and Implications\nThis investigation addresses a neglected topic in data visualisation research. Empirical results demonstrating viewers’ sensitivity to absolute magnitude present new considerations for design. Experiments were designed to generate robust evidence (Kosara, 2016), enhancing the findings of intial work on this topic (Sandman et al., 1994). Furthermore, a focus on identifying underlying cognitive mechanisms provides insight into how graphical cues to magnitude are processed, and additional factors influencing interpretations.\nThis thesis challenges the notion of ‘objectively correct’ data visualisation design, presenting a more nuanced perspective. Certaintly, misrepresenting numerical values is an invalid practice. However, the axis manipulations explored in the present work do not alter the numerical values presented, but the subjective judgements of these values. As Correll et al., (2020) argue, becuase these judgements depend on context, there is no inherently truthful axis setting which avoids influencing viewers. Therefore, designers ought to be aware of what their visualisation implies about plotted data, and make design choices which faithfully represent relevant characteristics.\nThe present work discusses bias, but use of this term does not necessarily warrant a negative perspective. Unquestioning acceptance of the message conveyed by a visualisation is obviously detrimental. However, failing to account for visual cues to values’ magnitudes could also be considered a limitation in cognitive processing. Sensitivity to values’ relative positions within an axis range and ability to weight this information in accordance with other knowledge constitute a powerful capability. Appropriate use of graphical cues to magnitude provides an opportunity to leverage this bias for effective communication.\nSensitivity to norms in communication is another relevant factor. Under the assumption that visualisation design follows Gricean pragmatic principles (Grice, 1975), viewers’ judgements will be guided by a chart’s implied message. Values’ relative positions within axes would be considered relevant and reliable information, thus informing inferences about magnitude."
  },
  {
    "objectID": "conclusion.html#limitations-and-future-directions",
    "href": "conclusion.html#limitations-and-future-directions",
    "title": "5  Conclusion",
    "section": "5.4 Limitations and Future Directions",
    "text": "5.4 Limitations and Future Directions\nThis work does not suggest that conveying absolute magnitude will be appropriate in all scenarios. Other aspects of data may be considered be more pertinent. Conflicting graphical cues may prevent designers conveying magnitude in addition to other aspects. For example, a truncated axis may clearly display relative differences between data points but omit context illustrating values’ absolute magnitudes. Alternatively, an extended axis may clearly display absolute magnitudes but obscure values’ relative differences. A possible compromise to this trade-off may involve generating two visualisations: one to illustrate absolute magnitudes and another to promote discrimination between values. A similar approach is discussed in work on axis truncation (Correll et al.), and has been reported to benefit users (Ritchie et al., 2019). Future work could investigate the utility of this approach for conveying both absolute magnitudes and relative differences.\nEliciting judgements using rating scales is an established method for measuring interpretations of visualisations (see Correll et al. 2020, Witt, 2019, Yang et al. 2021, Pandey et al. 2015, Stone et al. 2017, 2018, Okan et al. 2018, Okan et al. 2020, Sandman et al. 1994). To ensure participants’ responses reflected message-level interpretations, all scales captured ratings pertaining to the information presented, not simply the graphical elements. However, future work employing decision-making measures would complement this work. First, this would provide insight into how axis limits’ presentation of magnitude influences behaviour. Second, this would more effectively illustrate the strength of effects observed. Measuring responses in terms of well-defined units (e.g., monetary value) can capture the magnitude of bias in a more concrete manner. Other experiments in the field offer inspiration for effective and comprehensible decision-making tasks (e.g., Kale et al. 2021, Bancilhon et al., 2019).\nFuture work could also investigate the contribution of numerical context when plotting information about familiar measurements and relative frequencies. One approach to investigating this question would involve testing multiple different axis ranges whilst maintaining data points’ approximate physical positions. This would reveal whether interpretations of magnitude are directly proportional to the range of numerical values on an axis. Furthermore, this thesis focuses on how axis limits inform magnitude judgements, but many other factors may also influence interpretations, from numerical units to prior beliefs. A comprehensive account of cognitive processing of magnitude in data visualisations will also incorporate additional biases and mechanisms.\nAn algebraic approach to visualisation design suggests that substantial changes in data should be reflected accordingly in a visualisation (Kindlmann and Scheidegger, 2014). However, this capacity requires ability to convey substantial change in the specific aspect of a dataset of interest. A design supporting comprehension of relative differences may be poorly suited to conveying other messages. This thesis demonstrates that conventional data visualisation formats present an opportunity for communicating an often-overlooked aspect of a dataset: absolute magnitude. Just as adjusting axis limits influences interpretations about absolute magnitude, other design choices may help convey other messages. Future work should identify whether graphical cues to other features of data, such as noise or numerosity, have been similarly under-explored. Developing an understanding of cognitive processes involved in interpretating these features would inform effective design."
  },
  {
    "objectID": "conclusion.html#closing-remarks",
    "href": "conclusion.html#closing-remarks",
    "title": "5  Conclusion",
    "section": "5.5 Closing Remarks",
    "text": "5.5 Closing Remarks\nContext makes numbers meaningful. In data visualisations, context be can established not only by plotted data, but also extrinsic details. By determining a visualisation’s physical characteristics, designers may also influence the message conveyed about the data. Viewers’ sensitivity to the framing of data illustrates the power of design choices to enlighten or mislead."
  }
]