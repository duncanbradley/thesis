<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>An Investigation Into the Cognitive Processing of Magnitude in Data Visualisations - 3&nbsp; Reproducibility</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./axis-extension/axis-extension.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducibility</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Investigation Into the Cognitive Processing of Magnitude in Data Visualisations</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">index.html</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reproducibility.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducibility</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./axis-extension/axis-extension.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#problems-in-research" id="toc-problems-in-research" class="nav-link active" data-scroll-target="#problems-in-research"><span class="toc-section-number">3.1</span>  Problems in Research</a></li>
  <li><a href="#reasons-for-sharing" id="toc-reasons-for-sharing" class="nav-link" data-scroll-target="#reasons-for-sharing"><span class="toc-section-number">3.2</span>  Reasons for Sharing</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="toc-section-number">3.3</span>  Definitions</a></li>
  <li><a href="#sharing-code-and-data" id="toc-sharing-code-and-data" class="nav-link" data-scroll-target="#sharing-code-and-data"><span class="toc-section-number">3.4</span>  Sharing Code and Data</a></li>
  <li><a href="#the-importance-of-public-sharing" id="toc-the-importance-of-public-sharing" class="nav-link" data-scroll-target="#the-importance-of-public-sharing"><span class="toc-section-number">3.5</span>  The Importance of Public Sharing</a></li>
  <li><a href="#effective-programming-practices" id="toc-effective-programming-practices" class="nav-link" data-scroll-target="#effective-programming-practices"><span class="toc-section-number">3.6</span>  Effective Programming Practices</a></li>
  <li><a href="#literate-programming-and-dynamic-documents" id="toc-literate-programming-and-dynamic-documents" class="nav-link" data-scroll-target="#literate-programming-and-dynamic-documents"><span class="toc-section-number">3.7</span>  Literate Programming and Dynamic Documents</a></li>
  <li><a href="#computational-environments" id="toc-computational-environments" class="nav-link" data-scroll-target="#computational-environments"><span class="toc-section-number">3.8</span>  Computational Environments</a></li>
  <li><a href="#capturing-computational-environments-using-containers" id="toc-capturing-computational-environments-using-containers" class="nav-link" data-scroll-target="#capturing-computational-environments-using-containers"><span class="toc-section-number">3.9</span>  Capturing Computational Environments Using Containers</a></li>
  <li><a href="#rocker-for-capturing-r-environments" id="toc-rocker-for-capturing-r-environments" class="nav-link" data-scroll-target="#rocker-for-capturing-r-environments"><span class="toc-section-number">3.10</span>  Rocker for Capturing R Environments</a></li>
  <li><a href="#comparing-containers-with-virtual-machines" id="toc-comparing-containers-with-virtual-machines" class="nav-link" data-scroll-target="#comparing-containers-with-virtual-machines"><span class="toc-section-number">4</span>  Comparing Containers with Virtual Machines</a>
  <ul class="collapse">
  <li><a href="#the-best-is-the-enemy-of-the-good" id="toc-the-best-is-the-enemy-of-the-good" class="nav-link" data-scroll-target="#the-best-is-the-enemy-of-the-good"><span class="toc-section-number">4.1</span>  The Best is the Enemy of the Good</a></li>
  <li><a href="#my-approach" id="toc-my-approach" class="nav-link" data-scroll-target="#my-approach"><span class="toc-section-number">4.2</span>  My Approach</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">4.3</span>  Conclusion</a></li>
  </ul></li>
  <li><a href="#manuscraps" id="toc-manuscraps" class="nav-link" data-scroll-target="#manuscraps"><span class="toc-section-number">5</span>  Manuscraps</a>
  <ul class="collapse">
  <li><a href="#beyond-the-scope" id="toc-beyond-the-scope" class="nav-link" data-scroll-target="#beyond-the-scope"><span class="toc-section-number">5.1</span>  Beyond the Scope</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducibility</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="problems-in-research" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="problems-in-research"><span class="header-section-number">3.1</span> Problems in Research</h2>
<p><em>Ioaniddis, 2005</em> - Many lines of inquiry may be producing unreliable conclusions. Better powered experiments are required. Highlights a general concern with the trustworthiness of findings.</p>
<p><em>Open Science Collaboration, 2015</em> - replication study, revealing that the evidence many established findings was not as strong as initial reported.</p>
<p>Improving methods, plus reporting and dissemination, will improve the quality of published research. But these are different to efforts to improve reproducibility. A lack of transparency underlies issues with reproducibility, whereas problematic research methods underly issues with replicability (Munafo et al., 2017)</p>
<p>HARKing, p-hacking, low power, publication bias (Bishop, 2019)</p>
<p>Widespread recognition of problems in science (Baker, 2016). Researchers have neglected to focus on reproducibility (Ince, 2010).</p>
<p><em>Crüwell et al ., 2019</em> - behaviours that support reproducibility and replicability in general may be referred to using the umbrella term ‘Open Science Practices’.</p>
<p><em>White et al.&nbsp;2013</em> - open science recommendations are similar across disciplines.</p>
<p><em>Nosek et al., 2015</em> - TOP Guidelines for Open Science practices, requires sharing.</p>
<p>Replication allows for independent evaluation of research findings (in computational science), but is resource intensive. Assessing reproducibility offers another simpler way to evaluate reliability of work (Peng, 2011).</p>
<p>The pre-requisite is that these resources are available and suitable packaged to reflect original researchers’ use. This provides opportunity to assess one aspect of research, in order to gauge its trustworthiness.</p>
<p><em>Klein et al.&nbsp;(2018)</em> - Reproducibility crisis has sparked “urgent conversation”.</p>
<p>This chapter will review the literature on reproducibility, covering best practices then outline my approach used in this thesis.</p>
<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>— — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - - — - - - - - - - - - - - - - - - - - - - — — - - - - - - -</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="reasons-for-sharing" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="reasons-for-sharing"><span class="header-section-number">3.2</span> Reasons for Sharing</h2>
<p>There are many convincing arguments for reproducibility. Scientific approaches require that researchers can properly assess the credibility of research (Klein et al., 2018) and independently authenticate other researchers’ conclusions (Blischak et al., 2019) and . Thus, supporting third parties in reproducing research can increase perceptions of its robustness and reliability (Sandve et al., 2013). This also facilitates identification of errors in analysis (Klein et al., 2018). In addition to these motivating factors, authors may even appreciate the advantages of reproducible practices more than their peers (Piccolo and Frampton, 2016). For example, these practices can save time and effort (Sandve et al., 2013), and permanently sharing resources provides insurance against the loss of those resources (Klein et al., 2018).</p>
</section>
<section id="definitions" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">3.3</span> Definitions</h2>
<p>Inconsistent definitions of reproducibility have been employed in academic works (Plesser, 2018), particularly between experimental and computational disciplines (Drummond, 2009). In this thesis, I will use the following definitions. ’Reproducibility’ in research involves generating results exactly as reported using the project’s original data and code. ‘Replicability’ in research involves generating new data to assess consistency with an existing finding (Peng, 2011).</p>
</section>
<section id="sharing-code-and-data" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sharing-code-and-data"><span class="header-section-number">3.4</span> Sharing Code and Data</h2>
<p>A textual description of analysis in a manuscript presents an incomplete and vague acount of the analytical process (Piccolo and Frampton, 2016). Researchers must share code in order to detail the journey from the original dataset to inferential statistics (Klein et al., 2018), otherwise their software is a ‘black box’ (Morin et al., 2012). Initially, the possibility of issues or inconsistencies arising from computer code was overlooked (Plesser, 2018). However, it is now widely recognised that a computational analysis pipeline presents many opportunities for error. Making code openly available permits <em>independent</em> reproduction of all computational processes (Stodden et al., 2016). This, in turn, can engender trust, promote collaboration, and facilitate new applications (Jiménez et al., 2017). Each stage of processing must be included (Sandve et al., 2013) and any files <em>produced</em> using the analytical pipeline should be expendable, since reproducing them using the code supplied should be trivial (Marwick et al., 2018). For full transparency, data should be supplied in a raw, unprocessed form (White et al., 2013). Keeping raw data separate from other files ensures that the original file is not altered and the stages of processing are clear (Marwick et al., 2018). Other resources, such as stimuli and experiment scripts should also be shared alongside data and code (Klein et al., 2018).</p>
<p>The FAIR principles (Wilkinson et al., 2016) propose that data (and metadata) should be Findable (easily discovered), Accessible (easily obtained), Interoperable (easily integrated with other tools), and Reusable (easily employed beyond their original use). FAIR principles are also relevant to other computational tools (Lamprecht et al., 2020), with similarities to Open Source Software, which does not place limits on who may examine, adapt and extend the underlying code (Jiménez et al., 2017).</p>
<p>When sharing resources, a researcher’s choices can either assist or obstrust re-use (Chen et al., 2019). For example, using non-proprietary file types ensures that third parties can readily access resources (White et al.&nbsp;2013). Rather than personal or institutional websites, independent providers (e.g., Open Science Framework) are recommended for depositing these resources (Chen et al., 2019, Klein et al., 2018). Effective documentation is also important. A ‘codebook’ or ‘data dictionary’ can be used to explain the contents of a data file (Klein et al., 2018), inline comments can be used to explain code (Rule et al., 2019), and a README can be used to cover elementary information such as setup instructions (Lee et al., 2018). Documentation can also provide details on data collection and known issues (White et al.&nbsp;2013). Finally, licences contribute to a research project’s longevity, and provide clear statement for third parties, ensuring that their use of resources is appropriate (Jiménez et al., 2017). Where possible, lenient licences should be employed to avoid unneccessary restrictions (White et al., 2013).</p>
</section>
<section id="the-importance-of-public-sharing" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-importance-of-public-sharing"><span class="header-section-number">3.5</span> The Importance of Public Sharing</h2>
<p>If authors consistently shared data and code <em>on request</em>, freely available access could be considered unnecessary. However, empirical research demonstrates why it is important to share resources <em>publicly</em>. In a study of 204 papers from a journal which <em>required</em> authors to provide data and code on request, only 44% delivered on this promise (Stodden et al., 2018). Where research code is not publically available, various issues preclude procurement. These include local storage failures, restrictive institutional licences, concern about potential use, and concern about labour involved in providing support (Collberg &amp; Proebsting, 2016). Provision of data and code on request simply cannot be guaranteed, necessitating public sharing. In the field of data visualisation research, public sharing has historically been uncommon. Of papers submitted to the VIS 2017 conference, 15% shared materials openly and 6% shared data openly (Haroz, 2018). Greater transparency would increase the credibility of data visualisation research and facilitate identification and recification of issues in published work (Kosara and Haroz, 2018).</p>
<p>Researchers’ working practices and technological solutions both contribute to reproducibility. Whilst it has been suggested that behaviour and technology play <em>equal</em> roles (Sandve et al.&nbsp;2013), others argue that innovations have been so effective that researchers’ engagement with these tools is now the primary challenge (Grüning et al., 2018). Researchers report that several factors impede or deter their sharing of research data, including lack of expertise, lack of precedent, and lack of time (Houtkoop et al., 2018).</p>
</section>
<section id="effective-programming-practices" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="effective-programming-practices"><span class="header-section-number">3.6</span> Effective Programming Practices</h2>
<p>Programming with an automated approach has three main benefits over manual processing: increased reproducibility, increased efficiency, and reduced error (Sandve et al., 2013). Writing functions in a modular style avoids redundant repetition, promotes comprehension and supports reuse of code (Wilson et al., 2015). Researchers should also split code into appropriate chunks which each achieve a clearly-defined goal (Rule et al., 2019). These approaches shares many similarities with the Unix philosophy (Gancarz, 2003).</p>
<p>The task of preparing data prior to analysis is an important aspect of working with data. Wickham (2014) presents a set of tools, and underlying theory for this task, arguing that analysis can be facilitated by ensuring that data is in the correct structure. This structure is known as ‘tidy’ data, which consists of a column for each variable (each type of measurement) and a row for each observation (each unit measured). A principled approach simplifies the process of creating a tidy dataset using Wickham’s functions. Because each function treats data in a standardised manner, various functions can be employed in concert. The collection of R packages containing these functions (the ‘Tidyverse’) was designed with a concern for <em>humans</em>, not just computational performance (Wickham et al., 2019), so Tidyverse-style code is likely to promote comprehension (Bertin and Baumer, 2020).</p>
<p>Several other coding behaviours can facilite or inhibit reproducibility. For example, <em>absolute</em> file paths refer to a specific directory on a user’s machine, which will not be replicated on other users’ machines. Using <em>relative</em> file paths, which locate files in relation to the project directory, ensure code is <em>portable</em> and can be used on any machine (Bertin and Baumer, 2020). Additionally, independent researchers cannot successfully verify findings if only an approximate resemblance is achieved. Therefore, for any process involving random number generation, a random seed must be specified within the script, to ensure exact reproduction of results (Sandve et al., 2013). For maximum transparency, researchers should avoid controlling code through comments, but use functional approaches instead (Wilson et al., 2015).</p>
</section>
<section id="literate-programming-and-dynamic-documents" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="literate-programming-and-dynamic-documents"><span class="header-section-number">3.7</span> Literate Programming and Dynamic Documents</h2>
<p>Knuth (1984) presented a novel perspective on comprehensibility in computer programming which has been influential in the literature on computational reproducibility. Knuth’s premise is that a programming script should not be regarded primarily as a set of instructions for a computer to follow, but a tool to assist humans in understanding those instructions. This approach, known as ‘literate programming’, involves pairing code with corresponding text, such that reporting and documentation are closely linked to underlying code (Sandve et al., 2013; Piccolo and Frampton, 2016). Dynamic documents allow authors to mix code and narrative within a single file, with results updated whenever the document is rendered. Producing (and re-producing) an entire manuscript using a dynamic document offers opportunities to easily observe the implementation of code used for each aspect of analysis (Peikert and Brandmeier, 2021). In addition to descriptive and inferential statistics, data visualisations may also be rendered dynamically (FitzJohn et al., 2014). This efficient format enhances transparency (Holmes et al., 2021), supports interactivity (Rule et al., 2019) and avoids errors due to manually collating results (Peikert and Brandmeier, 2021). Including computationally-expensive code (e.g., complex statistical models) within a dynamic document can be problematic since this code is executed every time the document is rendered (FitzJohn et al., 2014). However, capacity for model caching provides a convenient antidote.</p>
</section>
<section id="computational-environments" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="computational-environments"><span class="header-section-number">3.8</span> Computational Environments</h2>
<p>Unfortunately, providing data and code is necessary, but not sufficient, for guaranteeing reproducibility. For example, research has found that even when the nominally required resources are available, it is not always possible to reproduce results exactly (Stodden et al., 2018), or even to execute the code (Collberg &amp; Proebsting, 2016). A study using an automated approach to test the execution of 379 Python files found that success depended in part on the Python version used and the presence of files capturing dependencies (Trisovic et al., 2021). Another study used a similar approach to test over 9000 R scripts (Trisovic et al., 2022). Approximately three in four scripts produced errors when executed. Implementing a code-cleaning algorithm reduced this number, but the majority (56%) still failed to run successfully. This indicates that good programming practices can improve code but cannot totally eliminate issues. Another source of error was incompatibility of R software versions and required packages. Thus, a failure to recreate the <em>computational environment</em> used when originally running the script prevented successful execution.</p>
<p>Peng (2011) argues that reproducibility can be considered on a spectrum. Sharing code offers some benefits over a standalone publication, providing data increases reproducibility further, but ensuring that the code can be precisely executed is even better. Each researcher’s unique preferences and proficiencies result in roughly the same number of computational environments as individual researchers, illustrating the necessity to record one’s computational environment (Nüst et al., 2017). Additionally, software under continuous development, such as the Tidyverse collection of packages, is frequently updated, meaning code can stop functioning unless specific versions are recorded (Holmes et al., 2021). Other software dependencies and parameter settings also complicate reproduction, requiring precision and comprehensiveness in documentation in order to achieve full <em>computational reproducibility</em> (Piccolo and Frampton, 2016).</p>
</section>
<section id="capturing-computational-environments-using-containers" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="capturing-computational-environments-using-containers"><span class="header-section-number">3.9</span> Capturing Computational Environments Using Containers</h2>
<p>Like many other aspects of reproducibility, innovations in software have made it possible for researchers to capture their computational environments. R package managers, such as <em>renv</em> (Ushey, 2020) conveniently load specific package versions for individual projects. However, they do not guarantee computational reproducibility, becuase they do not preserve the version of R in the same way (Holmes et al., 2021) or support additional dependencies (Peikert and Brandmeier, 2021, Nüst et al., 2017). Containerisation technology offers an effective solution. A ‘container’ can capture a much greater extent of the computational environment than a package manager (Grüning et al., 2018). This technology also provides an efficient and principled approach for recreating the environment, compared to a list of instructions for manual execution (Marwick et al., 2018).</p>
<p>Docker (Merkel, 2014) is a popular tool for generating containers. This process begins with a Dockerfile: a text-based file which provides instructions for installing specific package versions and loading other dependencies and resources. The Dockerfile is used to build a Docker image, which captures the computational environment. When this image is running, the environment is activated, and users may interact with this environment (Nüst et al., 2020b, Boettiger and Eddelbuettel, 2017).</p>
<p>Collating all dependency information in a single Dockerfile provides simplicity, and ensures that the original computational environment can be reproduced even after updating the software. Since the primary objective is ensuring reproducibility, this approach prioritises openness and human readability over optimising performance (Nüst et al., 2020b, Boettiger, 2015). As well as simple implementations, complex arrangements can be accomodated, but present additional challenges. For example, dynamic document generation may also require specifying LaTeX dependencies (Boettiger, 2015).</p>
</section>
<section id="rocker-for-capturing-r-environments" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="rocker-for-capturing-r-environments"><span class="header-section-number">3.10</span> Rocker for Capturing R Environments</h2>
<p>Researchers can save time and ensure consistency by using pre-existing Docker images (Nüst et al., 2020b). One particularly valuable example of this is Rocker which captures R environments for use in Docker. This tool provides portable R environments for use with a variety of systems, facilitating computational reproducbility (Boettiger and Eddelbuettel, 2014). Consequently, any researcher can execute, edit, and extend R code in a replica of the environment originally used for its development. Developing Rocker images involves a trade-off between generalisibility and specificity. An image designed to be too widely applicable would be cumbersome, but images with overly-specific use cases would be hard to find (Boettiger and Eddelbuettel, 2017). The solution involves providing base images that are easily expanded for specific requirements, with various Rocker images ‘stacked’ together as required, avoiding unnecessary complexity (Nüst et al., 2020a).</p>
</section>
<section id="comparing-containers-with-virtual-machines" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Comparing Containers with Virtual Machines</h1>
<p>Virtual machines perform a similar function to containers. However a notable difference is that virtual machines are large, whilst containers are comparatively lightweight (Piccolo and Frampton, 2016). This difference is due to the fact that virtual machines use their own kernel, whereas containers use the operating system kernel provided by the local machine. This reduces the relative size of a container, and enhances it computational power (Cito et al., 2016). Thus, virtual machines may be considered more comprehensive than containers, offering a greater degree of separation from the characteristics of the host machine (Grüning et al., 2018, Piccolo and Frampton, 2016). However, containers are typically compatible with version control systems (Piccolo and Frampton, 2016) and offer greater transparency (Nüst et al., 2020). Furthermore, due to their modular features, making minor adaptations is trivial with a container but comparatively prolonged with a virtual machine.</p>
<section id="the-best-is-the-enemy-of-the-good" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-best-is-the-enemy-of-the-good"><span class="header-section-number">4.1</span> The Best is the Enemy of the Good</h2>
<p>Despite the myriad recommendations for best practice (), a principle often endorsed in the literature on reproducibilty concerns the merits of small efforts. Taking <em>some</em> steps to increase reproducibility still enhances a project’s quality compared to neglecting this aspect altogether (Piccolo and Frampton, 2016). Witholding resources in pursuit of continuous refinement risks never sharing them at all. This fallacy is captured by the maxim ‘the best is the enemy of the good’. Analysis code does not need be perfect to in order to be useful to others (Klein et al., 2018), and it is impossible to benefit from external inquiry if the code is not shared (Barnes, 2010). Barnes (2010) argues that perceived limitations simply reflect that the code works only for the specific scenario at hand; inessential improvements are by definition not required for basic functioning. Researchers ought to accept these limitations and share their code anyway. In addition to code, this notion has also been applied to metadata (White et al., 2013) and containerisation (Nüst et al., 2020).</p>
</section>
<section id="my-approach" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="my-approach"><span class="header-section-number">4.2</span> My Approach</h2>
<p><sub><em>buildmer fits with the idea of showing incremental work, by automatically re-creating the process of the models that didn’t work</em></sub> <sub><em>however it also ensures consistency, and optimises output</em></sub> <sub><em>As an R package, it is archived, and its source code is transparent</em></sub> <sub><em>Rule et al.&nbsp;say: Document the process, not the results. </em></sub></p>
<p><sub><em>Reproducing the manuscript</em></sub></p>
<p><sub><em>Docker containers and YAML frontmatter are types of metadata (Leipzig et al., 2021).</em></sub></p>
<p><sub><em>Given that much of the discussion around adoption of software concerns researcher motivation, compared to availability of tools, this thesis serves as a case study for reproducible research. I do not explore all the capabilities of these tools, but employ a simple implementation of a few in order to demonstrate a minimal use case.</em></sub></p>
<p><sub><em>In experimental psychology, sharing stimuli and experiment scripts is another important aspect of transparent research practices (Klein et al., 2018).</em></sub></p>
<p><sub><em>Peirce et al., 2019 - PsychoPy was developed as a tool for conducting open and reproducible research. Designed to be used on different platforms, and to make available the underlying Python scripts for each experiment, open source, uses non-proprietary file formats. In keeping with Previous versions of the software can be specified easily, to avoid encountering errors as a result of new releases. </em></sub></p>
</section>
<section id="conclusion" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.3</span> Conclusion</h2>
<p>This chapter has discussed how a lack of reproducibility in published research can reduce credibility, and has revealed how various approaches can increase reproducbility. At the heart of these recommendations is the need comprehensively share resources and embrace technological solutions. Making research code and raw data openly available helps an opaque analysis process to become transparent. When an entire paper’s results can be fully reproduced by an independent third party, they can be thoroughly verified.</p>
<p>Working reproducibly is a duty (Sandve et al., 2013).</p>
<p>The notion of reproducible research code was discussed over 30 years ago, with ‘electronic documents’ providing the ability to package code with a manuscript (Claerbout, 1992).</p>
<p>Thus, it is necessary to capture the specific computational environment used when originally running the software. When specifying a project’s dependencies (the requisite files and software), researchers should ensure that the exact version of each package and program are supplied.</p>
<p>To capture dependencies, one must reproduce the computational environment used. (Boettiger, 2015)</p>
<p>accessible and transparent, increase engagement, responsible</p>
<p>The reporting of scientific studies involves lots of summary. Raw data is collected then processed, then analysed. In a small set of inferential statistics upon which a claim rests, a lot is going on behind the scenes. The ability to trace conclusions back to their original source is not just useful, but a crucial aspect of science.</p>
<hr>
<hr>
<hr>
<hr>
<hr>
<hr>
</section>
</section>
<section id="manuscraps" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Manuscraps</h1>
<section id="beyond-the-scope" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="beyond-the-scope"><span class="header-section-number">5.1</span> Beyond the Scope</h2>
<p>A suitable approach to reproducibility considers the data, analysis, and other characteristcis of the research project (Chen et al., 2019). <em>Lee et al., 2018</em>- Other aspects of recommendations are beyond the scope of empirical research, such as error messages, help commands and documentation for APIs (application programming interfaces). <em>Garijo et al., 2022</em> - some recommendations are perhaps beyond the scope of a single empirical research project - e.g.&nbsp;considering policies on software contributions and long-term maintenance. Provides guidance on managing research software repositories. Reproducing the environment used to generate a container is an additional challenge to the sustainability of this solution (Grüning et al., 2018). <em>Peikert and Brandmeier, 2021</em> - Make (and Makefiles) outline the sequence in which programmes should be executed, so is appropriate when a workflow involves significant processing of data prior to analysis. <em>Nüst et al., 2017</em> - introduce the notion of Executable Research Compendia, providing data, code, computational environment, and interface. The concept is that this is ‘self-contained’. This follows the development of Research Compendia (Gentleman and Lang, 2007), which focused on providing data and analysis code, and use of dynamic documents. Executable Research Compendia are a way of packaging research resources/output. Key to the notion of Executable Research Compendia is being providing a ‘one-click’ functionality that indicates successful reproduction of all computational processes, resulting in an exact correspondence between generated and reported results. However, perfect reproducibility only indicates that computational processes run as expected, not that the conclusions of the research independently valid. Continuous integration offers the ability to detect whether each new change has not prevented successful outputs/compilation (FitzJohn et al., 2014). Whilst I did not follow a specific pre-defined workflow, my approach closely resembles published workflows (e.g., van Lissa et al., 2020; Peikert and Brandmeier, 2021). [end of beyond the scope]</p>
<p><em>White et al.&nbsp;2013</em> - Figshare is an ‘all-purpose’ solution. <em>Bahaidarah et al., 2022</em> - reproducibility in research is ‘essential’ <em>Klein et al.&nbsp;(2018)</em> - access to the constituent parts of research accelerates scientific progress (Ioannidis, 2012). <em>Marwick et al., 2018</em> - research compendia organise resources for reproducibility. Bertin and Baumer’s (2020) package also tracks dependencies.</p>
<p>The challenges are no longer completely depending on innovations in technology, but researchers’ engagement with these tools (Grüning et al., 2018).</p>
<p>The authors present a package which supports reproducible practices. Users who specify a local working directory are confronted with an error which instructs them to choose an appropriate file path instead. Also encourages use of functionality which sets the random seed (Bertin and Baumer, 2020).</p>
<p>Rather than focusing on absolute best practices, Wilson et al.&nbsp;(2015) present a starting point for repoducible working, smaller steps towards fully reproducible work.</p>
<p>Researchers should also ensure that variable names are comprehensible (Wilson et al., 2015) and avoid spaces in file names in order to facilitate programming (White et al., 2013).</p>
<p>Write functions to avoid duplication, make code modular (Rule et al., 2019). Literate programming facilitates comprehension of code (Piccolo and Frampton, 2016). Varaibles should be appropriately named and explained (Knuth, 1984). It’s useful to pair textual description and explanation with corresponding analysis code, literate programming tools make this easier (Sandve et al., 2013).</p>
<p>Knitr provides opportunities for literate programming (FitzJohn et al., 2014).</p>
<p>Computational notebooks (e.g.&nbsp;Jupyter notebooks) integrate several aspects of analysis workflow into a single document (Rule et al., 2019). Rmarkdown provides transparency across the entire analytical workflow (Holmes et al., 2021). Dynamic documents promote use of literate programming, resulting in reproducible manuscripts (Peikert and Brandmeier, 2021). Avoid spaces in files names (White et al.&nbsp;2013). Definition of replicability as generation of new data to further examine a reported finding (Simons, 2014)</p>
<p>Claerbout uses reproducibility as I would - but replication differently - see Rougier et al.&nbsp;(2017) <em>Stodden et al., 2015</em> - different types of reproducibility: empirical (robustness of methods and their reporting), computational (robustness of software), analytical (robustness of analysis, Stodden, 2011, 2013).</p>
<p>Independent provides (.e.g OSF) provide a range of useful features (Klein et al., 2018).</p>
<p><em>White et al.&nbsp;2013</em> - Avoid simple errors like failing to separate values from units and ambiguous blank cells.</p>
<p><em>Chen et al., 2019</em> - suggests moving simply beyond notions openness and transparent, and the importance of whether such resources are usable. This does involve preserving a workflow, licensing, using established repositories. <em>Nüst et al., 2017</em> - Sharing is not sufficient becuase what is shared must be comprehensible.</p>
<p><em>Smith et al., 2016</em> - citations of software should be considered equally important as citations of research. The authors should be credited, versions and variants should be named, and associated resources should be easily accessible. Methods for identifying citations should be persistent and unique to the software. Collected 601 papers and attemped ‘weak repeatability’ - finding and executing programs that supported the work. Of elibigle papers, under half could be successfully executed by the authors. In many cases, the software was not publically available. Various issues in reproduction include sharing non-final versions of code, original programmer no longer available (Collberg &amp; Proebsting, 2016). Studied 204 papers from a journal which required authors to provide data and code on request - results were successfully reproduced in only 26% of cases (Stodden et al., 2018). Very few files analysed were R markdown, which allows dynamic document generation, which would expose initial researchers to more errors. Errors also arose due to incompatibility of R package versions, which reveals the benefits of containerisation/capturing the computational environment. Recommend renv, relative file paths, and Docker (Trisovic et al., 2022). Recording the dependencies is necessary (Grüning et al., 2018). Computational notebooks and tools for dynamic document generation (Jupyter, R Markdown) do not capture the computational environment (Grüning et al., 2018).</p>
<p>Dockerfiles capture a greater level of dependencies than makefiles, because they can capture software versions (Boettiger, 2015). <em>Nüst et al., 2020b</em> - It is necessary to capture the specific versions of packages and other software.</p>
<p><em>Nüst et al., 2020b</em> - Package managers may be used in concert with Dockerfiles,</p>
<p>That Dockerfiles are human-readable promotes transparency. Tools must be practicable as well as productive. Lack of familiarity is a barrier (Boettiger, 2015). <em>Nüst et al., 2020b</em> - Conventions facilitate comprehnsion, and following guidance can assist less experienced researchers generate useful outputs. <em>Nüst et al., 2020b</em> - Dockerfiles should be both comprehensible to humans and machine-readable. <em>Nüst et al., 2020b</em> - All files required by the Dockerfile must be included in the repository. <em>Nüst et al., 2020b</em> - Even old Docker containers may not function properly.</p>
<p><em>Nüst et al., 2017</em> - Docker cannot be used in conjuction with proprietary software. <em>Nüst et al., 2020b</em> - Proprietary and GUI software are not compatible with containers.</p>
<p><em>Nüst et al., 2020b</em> - Caching is another consideration.</p>
<p>Haroz (2018) - journal articles alone do not provide a complete picture of a research project, so data and code must be openly accessible for the research to be considered reliable/trustworthy. Haroz (2018) - Certain sites are less reliable than others because they are easily modified, or are short-lived, becoming afflicted with ‘link-rot’.</p>
<p>Kosara and Haroz suggest that the absence of a replication crisis in the visualisation field doesn’t necessarily point to lack of an issue but rather a lack of replications (though they note a handful of conceptual replications). Their papers discusses several features of poor-quality/sub-standard empirical work that might invalidate a study’s conclusions. Along with other issues (including excessive research degrees of freedom and experimental design issues). Sharing <em>everything</em> allows others to carry out all three types of replication to be achieved - including those which are typically challenging - reanalysis and direct replication.</p>
<p>Effective data management facilitates advances in knowledge. Machine readability is important for using the ever-increasing proliferation of data (Wilkinson et al., 2016).</p>
<p>The FAIR principles (Wilkinson et al., 2016) were developed to facilitate use of data by machines. Documentation to introduce each program (Wilson et al., 2015).</p>
<p><em>Lee et al., 2018</em> - The contents of a README should cover elementary information such as setup instructions and licence. Different types of resources require different metadata implementations/considerations. Metadata provides an important contribution towards reproducbility (Leipzig et al., 2021).</p>
<p>Data files should not include blank cells, which introduce ambiguity regarding whether data points are missing (White et al.&nbsp;2013).</p>
<p><sub><em>Through version control, archiving of changes to a document does not rely solely on user’s descriptions but digital records of actual modifications (Wilson et al.&nbsp;2015).</em></sub> <sub><em>Sandve et al.&nbsp;2013 - Version control facilitates inspection of prior versions of code, allowing the stability of results to be assessed across different versions. Maintaining a record of intermediate steps facilitates diagnosis of issues, plus inspection of entire approach preceding conclusions. </em></sub> <sub><em>Using github to support computational reproducibility (Perez-Riverol et al., 2016)</em></sub></p>
<p>Stodden et al.&nbsp;2013 - irreproducible computational working practices are akin to poor record keeping. The expectations around documentation of software do not match the care required for other aspects of research. The typical researcher’s documentation of software is inconsistent with their attention to detail in other aspects of research.</p>
<p>Computational working practices should receive the same attention to detail typically applied to other aspects of research (Stodden et al., 2013), yet research code is not usually produced with sustainability in mind (Trisovic et al., 2022).</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./axis-extension/axis-extension.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Axis Limits and Denominator Information Influence Magnitude Ratings in Bar Charts</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>