# Related Work

The purpose of this chapter is to place my work in a wider context - to give the reader an idea of what type of research has previously been carried out in this area, to provide a setting. Some notes on my key motivations: 
  * we need good evidence, we can’t rely on assumptions
  * we need to move beyond precision and look at interpretations
  * we need to move away from the idea that certain designs are *inherently* misleading, and think about context

# The State of Visualisation and Psychology Research

Kosara (2016) makes the case for a solid empirical foundation for design guidelines. However, the evidence base is weak (Kosara, 2016). Some of the recieved wisdom has not been tested at all, other guidance has been disproven or has been confirmed only recently. It’s not even always clear where the evidence ends and the opinion starts - intuition and unfounded statements make for “visualisation folklore” (Correll, 2022, pg. 3). The role of data visualisation research is to generate robust evidence to fill gaps in knowledge. As Correll’s article explores reserarchers’ shared values (). This work was not a systematic review of all literature, but points towards a lack of consistency in achieving new knowledge. 

The stuff that has been really successful isn't really new. It's established psychology effects applied to visualisation (attention for focus and declutter, curse of knowledge, datasaurus (Boger)). What does this say about where we need to look for new knoweldge?

Adjacent fields like perceptual psychology might tackle problems well because of more established methods, related empirical work and theory (Correll, 2022). The best source of theory in visualisation might be pyshcological perception and cognition research (Rensink, 2021).

Hegarty on need for evidence-based claims. 

There’s a distinction between my point about asking designers what they think works and asking viewers what they think works. The first is a point about whether claims are built on any evidence at all. The second is a point about whether claims are built upon *the right type* of evidence. 

Unfortunately the guidance contains a lot of received wisdom. The most famous is chart junk. Essentially represents a preference for mimimalist designs. There’s nothing wrong with it per se, other than the fact it wasn’t based on evidence. Indeed, there have been situations where it’s been shown to be helpful. Defining the term experimentally would also clarify exactly what features constitute ‘chart junk’, making guidance clearer.

Robert Kosara pie chart misconception work
Chart Junk
  * Beyond memorability - Borkin
  * Kopp et al. - embellishments
  * Okan et al. (2018) - labels are useful
  * Borgo et al. (2012)
  * Haroz et al. (2015)
  * Skau et al. (2015)
  * Stock and Behrens (1991)

So, having established that we need an evidence base, next comes the question of how to generate this evidence. One option would be to base our guidance on what people think works best. However, introspection and hunches are not always a reliable source. 
Elting et al. (1999) - compared to tables, pie charts, and bar charts, icon arrays resulted in the most accurate judgements about clinical trials, but none of the 34 physicians in the sample preferred this method.
Burns et al. (2021) - participants estimated that the pictographs took longer to understand, compared to equivalent charts without icons. However, this self-report measure was at odds with the actual response times, which indicated no differences between chart types.
Borkin (2011) - self-report conflicts with utility of rainbow colourmap
See highlights in Çöltekin et al. (2017) references
This is not to say that collecting these judgements isn’t useful. Indeed, it provides a valuable insight into people’s willingness to engage with visualisations. However, it’s important that these findings are treated appropriately, and aren’t used to inform conclusions about effectiveness.

How should data visualisations be studied?
Rensink (2021) - how not to study a visualisation
Elliott et al. (2020) - role of vision science
Munzner (2009)
Isenberg et al. (2013)
Lam et al (2011)
Tory (2014)
Hullman et al. (2019) - importance of decision tasks and not just precision tasks.
El Greco

Different approaches to studying - correctness vs interpretation

MacDonald-Ross (1977; see Spence and Levandovsky, 1991) also cautions against making dismissive judgements about ‘expert’ advice in the absence of empirical evidence. Pie charts were used widely without expert approval, but Spence and Lewandowsky find evidence that they are useful in certain scenarios.  - relate this to Tufte
  
The picture is complicated further by examples that don’t even conform to empirical findings
This is echoed by Bertini et al. (2020), who mention Heer et al.’s (2010) work which explores the notion that effective visualisations may not conform to the simple rules generated by empirical findings. 
Woodin vs. South China Morning post
Titles vs. Climate Stripes
Desbarats - is it all just ‘it depends’?
So we have to acknowledge that it’s not as simple as following a set of rules - context and rhetoric also play a role

# Precision

Arguably the most influential experiment in the data visualisation field was conducted by Cleveland and McGill (1984), who sought to identify elementary perceptual processes involved in viewing visualisations. There are many different ways of graphically representing values. However, the precision of the visual system’s differs according to the characteristics of the stimulus. Therefore, through psychophyiscal testing, it’s possible to work out how well interpretations of different presentation formats correspond to the actual numbers. For each visualisation, participants identified which of two marks conveyed the smaller value, and estimated, as a percentage, how much smaller this was than the larger mark. The consequent leaderboard is the “ranking of visual channels”.  Position-encoding produced less error than both length- and angle-encoding. Therefore, where possible, data should be encoded using position on a common (aligned) scale. This means that grouped bars elicit more precise representations than stacked bars. How do we interpret this ranking? Are there theoretical reasons for why area is not as good as other channels?

The issue here is not the work itself, rather the interpretation of this work. Indeed, this work has endured replication (Heer and Bostock), it’s findings are far-reaching and really useful. However, to naively take this as the *only* relevant consideration is unheeded, as there are many other tasks than judging ratios. (Bertini et al. 2020). 

Position-encoding is the optimal choice for extracting single values, but is not necessarily the optimal choice for other tasks (e.g., pattern recognition). The ability to extract values is not the only use of charts. Different tasks benefits from different encondings. One particularly interesting example is situations where precision hinders, rather than facilitates, judgements (Correll et al. 2012). Aggregretation tasks are less concerned with specifics, more concerned with overview, so there are better encoding types, which are less precise. Line charts typically used for timeseries data. The way the visual system tackles line graphs makes it more difficult to perform averaging. Averaging is particularly efficient at lower spatial frequencies, so there is better averaging with color than line graphs. This finding offers a new perspective on Anscombe’s quartet. Effectively conveying differences between datasets conceals their similarities. Using a less precise visual channel would facilitate recognition of the latter. 

Furthermore, different encoding types are suited to different types of data. For example, some are intrinsically suited to displaying part-to-whole relationships (e.g., pie charts, stacked bar charts), whilst others are better suited to displaying trends (e.g., line charts). Zacks and Tversky (different chart types serve different purposes); 

The choice of certain graphical encodings can influence interpretations of the type of data a visualisation presents. Zacks and Tversky (1999) found that participants were more likely to refer to discrete differences when describing bar charts, and trends when describing line graphs. Responses were more influenced by graph format than by data type. Zacks and Tversky explained how this can lead to an atypical description, with discrete gender differences characterised using gradated changes (e.g., *“The more male a person is…” *). However this particular (now well-known) comment was only given by 12% of participants. *Production* of bar charts and line graphs was also influenced by whether a discrete or continuous relationship was specified in the description. Zacks and Tversky argued that cultural conventions in visualisation design are likely to play a substantial role in the driving these effects, with perceptual and cognitive biases playing a more minor role. Metaphor/affordance/congruence.
Displaying proportions with encoding that factilitate recognition of part-to-whole relationships (e.g., Spence and Levandovsky, 1991) can be achieved using stacked bars and pie charts, but not with grouped bars, which would be favoured by the precision rankings. 

Finally, Bertini et al. explained that simply conveying information is not the only purpose of data visualisations. They can also be persuasive, help individuals to remember information, or aid decision-making. It is important not to narrowly define effectiveness as extraction efficiency, but to consider effectiveness within the wider context of the multiple functions that visualisations can serve. Hullman et al, (2011) - inefficent processing due to “visual difficulties” may benefit cognition. 

The precision work speaks to our concern with (in)accuracy - that we might be misleading others, that others may mislead us, that charts may be weak and lack clarity. But there is so much more than simply extracting values wrong.

# General Framing in Data Vis - Cognitive Bias

Previously discussed work has been concerned with whether values are extracted precisely. But thisn’t isn’t the only way values can miselad. What other ways could we be mislead, not by imprecision (perceptual bias), but by cognitive bias.

Striking patterns can be easily missed. Boger et al.’s datasaurus study illustrates the role of selective attention. Participants instructed to view dots in a particular colour/location failed to notice distinct dinosaur patterns in non-focal data. Even when one is trying to be assume the perspective of a naïve viewer, it’s difficult to adopt an objective outlook when viewing visualisations. Xiong et al.’s curse of knowledge study demonstrates that being briefed about a particular pattern results in over-weighting of these aspects in one’s cognitive representation. The implications for creators are that important messages may be missed if they are not addressed explicitly. What is obvious to a designer may not translate to a viewer. Focus and declutter has long been a recommendation, and is now supported by empirical evidence. This body of research also points to the use of text to support communication of a chart’s intended message. It may seem suspect to have this level of editorial control, instead wanting to let the data speak for themselves, but considering the various rhetorical tecniques, it’s clear there is no truly objective way to present data. 

Zacks and Tversky - if different chart types serve different purposes, are we at risk of being mislead by the chart type
Xiong proximity
Newman Scholl, Pentoney Berger, 

Does inclusion of a visualisation actually increase persuasion/trust/belief (Tal & Wansink) + failed replication

Hullman & Diakopoulos: Rhetoric

Taxonomies have captured the mapping between data types and visualisation types, revealing what is suitable for understanding particular datasets. 

Brust-Renck et al. (2013) - importance of intentionally selecting a message and designing chart appropriately to convey gist. 

Reyna and Brainerd (2008) - Suggest it’s not just about making humans more like computers and memorising verbatim detail, it’s about making sure the gist that they do extract captures the true nature of the information. 

How do I move from general cognitive bias to truncation specifically?
What’s different about truncation?

# Truncation

One topic that has enjoyed attention in recent years is axis truncation. This issue has also captured minds outside of academia. Perhaps it’s particularly interesting to engage with (for both groups) because it seems like a big change comes from a subtle manipulation. The data and the chart type both stay the same; all that’s needed is a tweak of the axis. It is another example of where popular opinion can be at odds with research findings. Part of this may be because people like simple rules, but the research doesn’t offer simple rules here. 

Sentence on why relative difference has been so widely studied. Sentence on why it’s particularly important for risk perception

Sentence on the benefits of studying impressions as well as value extractions. This is a hangover from Cleveland and McGill. See Stone et al. (2015)

Early studies - one sentence each (use citations of these in later work):
Cleveland et al. (1982)
Taylor and Anderson (1986)
Arunachalam et al. (2002)
Beattie and Jones (2002)
Raschke and Steinbart (2008)
Pennington and Tuttle (2009)

Deception in data visualisation does not necessarily involve displaying inaccurate data (Yang et al., 2020). A simple design choice can prompt users to form an inaccurate view of the data. Pandey et al. (2015) investigated four chart formats that have been classed as ‘deceptive’, either because they exaggerate or understate the message conveyed by the data, or because they present the opposite perspective. Exaggeration or understatement of patterns can occur through use of a truncated y-axis, use of area encoding for quantity, or an aspect ratio manipulation. An opposite perspective can be presented using an inverted *y*-axis. Pandey et al.’s investigation explored how these formats affect message-level representation of the data. This refers to the domain concepts communicated (e.g., greater access to safe drinking water) rather than just the graphical elements (e.g., higher bars). Indeed, they suggest that misunderstandings of the data may arise during translation from graphical schemata (Pinker, 1990) to real-world understanding. All deceptive techniques investigated in the study led to misunderstandings in participants, with large effect sizes.

Other research has also explored how axis ranges affect interpretation of data. Cleveland et al. (1982) explored how perception of correlation in scatterplots changes as axis limits change. When both axes are expanded, and the cluster appears smaller, perceived correlation increases. Yang et al. (2020) investigated the effects, possible mitigation, and individual differences associated with *y*-axis truncation. The rationale for this set of experiments was that previous work on *y*-axis truncation had low power (e.g., Pandey et al. 2015; Witt et al., 2019), or problematic experimental design (Raschke & Steinbart, 2008). Visualisations were created using standardised measurement of deception as defined by the Graph Discrepancy Index (Steinbart, 1989; although criticised by Mather et al., 2005). Participants indicated on a Likert scale their perceived relative difference between the two displayed values. In the initial experiment, a truncation effect was observed, with greater truncation resulting in greater perceived differences. Analysis revealed no association between participants’ susceptibility to the truncation effect and their graph literacy. 

Investigating the effectiveness of warning participants about possible errors in judgement due to y-axis truncation can help to reveal the nature of the deception (Yang et al., 2020). That is, success in this intervention indicates that the deception may occur due to insufficient engagement of cognitive capabilities. Yang et al. found that even when an explicit warning was provided, a truncation effect was still observed. However, ratings of perceived difference were *diminished* in truncated charts, and also in control charts, to a lesser extent. The reduction in ratings for both conditions suggests that this warning may have fostered greater wariness overall. The truncation effect, therefore, appears largely automatic, with higher-level reasoning only affecting judgements to some degree. Despite this, evidence suggested that the effects of this type of warning do not decay in the 24 hours following its issue. 

To investigate whether expertise mediates the effects of *y*-axis truncation, Yang et al. (2020) tested PhD students in quantitative and qualitative fields. When no warning about truncated axes was provided, students in quantitative fields exhibited a reduced truncation effect compared to students in qualitative fields. However, a warning was provided, there was no difference between the two populations - both exhibited diminished truncation effects. Given their previous finding that graph literacy does not affect the truncation effect, Yang et al. suggested that graph literacy and training in a qualitative field actually reflect different proficiencies. Graph literacy captures knowledge concerning graph formats and one’s understanding of the role of different graphical elements. This type of knowledge can assist with basic interpretation but is unlikely to provide barriers against errors in judgement like the truncation effect.

One explanation for the deception caused by y-axis truncation, which Yang et al. (2020) discussed, draws upon the Gricean maxims of truth, relevance and clarity (Grice, 1975). That is, a user might assume that a difference must be genuinely large if it appears large, else it would not be presented in that way. Yang et al. also argued that truncation may be less misleading when other graphical encodings are used (e.g., position encoding in line graphs). They concluded that, in an effective visualisation, a user’s automatic, immediate characterisation will have a close correspondence with their view following a detailed inspection.
Correll et al. (2020) also found that participants’ ratings of the severity of effect sizes were affected by truncation, and this occurred despite design features intended to highlight it. Despite consistent findings revealing the misleading effects of axis truncation, Correll et al. argued that starting the *y*-axis at zero (i.e., no truncation) can still be deceptive, for example by obscuring a trend. Therefore, they conclude that any axis range setting has unavoidable consequences for interpretation: designers must always reflect on how this will affect users’ perceptions of effect size.

Starting the *y*-axis at zero can make differences between values harder to detect (Correll et al., 2020). Therefore, Witt et al. (2019) developed and tested optimal *y*-axis ranges in bar and line charts displaying various effect sizes. Three different axis scalings were employed: maximal (ranging from 0-100); minimal (restricted to the range of the displayed data); and standardised (between one and two standard deviations of the dependent variable). For each trial, a linear regression model was developed, with the intercept indicating participants’ bias, and the slope indicating their sensitivity. Bias was smallest and sensitivity greatest with the standardised axis scaling. Consequently, Witt et al. recommended that an axis should extend roughly 0.7 standard deviations in each direction or span roughly 1.5 standard deviations overall. In this study, participants judged the size of the effect (as small, medium, or large), but not the magnitude of difference between one data point and another. For the latter type of judgement, as Pandey et al.’s (2015) data suggests, starting the axis at any point other than zero remains a danger.

Driessen et al. (2022)

Risk studies:
Sandman et al. (1994)
Add other papers here

Other scale manipulations
Y-axis inversion - Woodin et al. (2021); Pandey (2015)
Log scales - Romano et al. (2020) - see also cited articles

To what extent is there a difference in the definitions of ‘axis truncation’:
  * both ends vs. one end
  * bars vs. axes
What is it about Witt’s study that means it can give such precise recommendations whereas others can’t

There is an important distinction between ‘error’ and ‘bias’ - various interpretations may not necessarily be wrong - just different.

Introducing my topic - magnitude judgements:
Rarely mentioned as deceptive when describing axis manipulations
Explain why risk research is so interested in magnitude of difference effects. The choice to do something always has an alternative action. Speigelhalter explains why relative risks are better than absolute risks. But sometimes magnitude does have to be communicated. 

Certain types of misleading design are consistently misleading (e.g. inverted y-axis). They are consistently misleading becuase this isn’t based on the data at all. Other types are not consistently misleading (e.g. truncated y-axis). Here, it’s impossible to say *a priori* that a chart is misleading because the quality of the design depends on the data. Therefore, it’s really about the *potential* to mislead. A simple test could be whether the ‘misleader’ (to use Ge et al.’s (2023) term) needs to have the word ‘inappropriate’ before it to convey the issue. For example, concealed uncertainty, cherry picking, missing data are all self explanatory, but ‘aggregation’ and ‘scale range’ need the word inappropriate before them in order to highlight the issue. It is the later that might be considered ‘potentially’ misleading. Ge et al. (2023) also note that assessing response accuracy requires a specific task, so misleadingness will always be related to specific tasks. 
  
# Visualisation Literacy

Increasing popularity and use of data visualisation has expanded the audience who are exposed to charts. Burns et al. (2023) discusses research aimed at understanding how relatively inexperienced users understand charts. This is important, but there is inconsistency around what constitutes a novice, and also a inconsistency between general population and samples used to explore that population. The term novice is unhelpful since it forces a binary perspective onto a more nuanced picture. Why not look at individual variation in ability? Burns reports very few cases where novice was defined by literacy.

It is useful to understand variability in biases by looking at individual differences. Get a better picture of how they might work, and the types of audience most likely to be affected. 

Some people are ‘novices’ - discuss the two novices papers - but there are not just two groups - literacy scales can be useful
Visualisation literacy is the ability to comprehend data presented in visualisations (Boy et al.)

Galesic and Garcia-Retamero’s 13-item scale (2011) was based on Friel et al.’s (2001) hierarchy of skills for interpreting visualisations. 
Garcia-Retamero and Galesic (2010) found that…
Okan et al. (2012) found that
Boy et al (2014) developed a new method
Garcia-Retamero et al. (2016) developed a new method due to the length and potential negative consequences of previous tests (i.e. inducing anxiety). Based on prior work developing numeracy scales through subjective judgements. The 5-item version is a refined version of the 10-item subjective scale, based on the items which most strongly correlated with the objective measure, and actually performs better than the 10-item scale. Final items focused on skills (how good are you at …)  more than preferences ( do you like ….) . Diverse participant pool (US m-turkers, UGs in Spain, surgeons and patients in Spanish hospitals). Captures variation in both objective abilities but also metacognition - beliefs about own abilities/skills. 
Okan et al. (2016) found
[more papers using the subjective scale](https://scholar.google.com/scholar?cites=12562930392859617416&as_sdt=2005&sciodt=0,5&hl=en)
Lee et al. (2017) - justification for creating the VLAT
Camba et al. (2022) suggest that the ability to detect misleading charts should be considered an important feature of data visualisation literacy. 
Ge et al. (2023) noted that previous work on visualisation literacy has focused on interpretation of well-designed charts. Despite this, the ability to make sense of poorly designed charts is also relevant. Good approach - not to ask Ps whether they think the charts are misleading, but to intersperse misleading and honest charts within a test and measure accuracy. Develop a robust test of 30 items to assess ability to accurately comprehend deceptive designs. Ge et al. (2023) suggest that sufficient attention and critical thinking can help combat misleading visualisations. However, some cognitive biases are persistent even against these efforts. Perhaps this is where the distiction between correctness and interpretation comes in (see Correll et al., 2020, Yang et al., 2021 Stone et al., 2015).


Why have I chosen this specific measure?

# Accessibility

# Colour

  * Schloss quantity biases
  * Schloss background colour
  * Szafir colour for different size marks

# Uncertainty

  * Kale
  * Kay
  * Correll VSUPs

# Prior Beliefs

  * Spiegelhalter pre-bunking
  * Heyer et al. 
  * Doug Markant
  * Xiong -Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation

# Text

See Zotero

# Numerical Framing

General knowledge on cognition about numbers - couple of papers summarising how good/bad we are and interesting findings. 
Non-visual perspective
What is framing generally? What does it show about how our brains represent information?
Lundquist et al. 
Link to Grice - readers/viewers make assumptions about communicative intent. They aren’t just passive receipients. 
Therefore there is a trust element - when trust is high, more likely to fail to question the delivery of the message. 
Which papers cite Brown (2008) and Borges (1974)?

Other numerical biases have been studied in a datavis context e.g. Dimara et al. 2017, 2019

Framing effects were initially primarily a reference to valence framing, but have since expanded to encompass a wider range of biases. 

# Vis theory

Grammar of graphics

Spyrison - what factors make a visualisation paper appealing? it’s not open data practices…
Cockburn et al. (2020) and Kosara & Haroz (2018) are both papers about open science in visualisation research. 

Hardwicke et al. (2018) explore “Data availability, reusability, and analytic reproducibility” (from title) - Kosara and Haroz suggest that visualisation would benefit from these efforts to improve research quality. 

Kosara and Haroz suggest that the absence of a replication crisis in the visualisation field doesn’t necessarily point to lack of an issue but rather a lack of replications (though they note a handful of conceptual replications).
Their papers discusses several features of poor-quality/sub-standard empirical work that might invalidate a study’s conclusions. Along with other issues (including excessive research degrees of freedom and experimental design issues), they discuss how issues in statistical analysis, such as misapplication of statistical tests, jeopardise validity. To remedy this, data and analysis code should be shared publicly. This facilitates identification of problems and their subsequent rectification. This practices serves as proof of validity: being transparent about analytic process lends credibility. Documenting experimental design assists identifcation of issues/confirmation of good design. Sharing *everything* allows others to carry out all three types of replication to be achieved - including those which are typically challenging - reanalysis and direct replication.

Haroz (2018) - journal articles alone do not provide a complete picture of a research project, so data and code must be openly accessible for the research to be considered reliable/trustworthy. In the visualisation field, a lack of sharing prevents proper examination of conclusions. In VIS 2017 - 15% shared materials openly, 6% shared data openly. Certain sites are less reliable than others because they are easily modified, or are short-lived, becoming afflicted by ‘link-rot’.


  
