# Introduction

When communicating with words, using metaphors helps convey ideas. When communicating with numbers, data visualisations perform a similar role. A list of numbers can allude to an upwards trend, but representing those numbers *as* visual phenomena facilitates comprehension.

Whilst metaphors are evocative, data visualisations afford *precision*. In a line chart plotting an upward trend, the pattern depicted corresponds to the exact nature of the increase. The human visual system’s sensitivity to this systematic depiction makes data visualisation a powerful tool: presenting a different pattern would elicit a different mental representation.

Broadly, data visualisations make it easier for the brain to process numerical information. As *external* representations of data, they reduce perceptual and cognitive burdens in interpretation (Scaife and Rogers, 1996). Imparting efficiency and clarity facilitates pattern-recognition and reasoning.

However, a single dataset can be depicted in countless different ways, and results can vary widely (Franconeri et al., 2021). Thus, data visualisation’s strength can also be its vulnerability. Outsourcing cognitive processes to a graphical depiction leaves a viewer at the mercy of the chosen method of visual representation. Thus, understanding successful design is crucial.

The effectiveness of data visualisations can be defined in many ways, encompassing their various objectives, which include informing, persuading, engaging and promoting memorability (Bertini et al., 2020). However, in general, successful data visualisations will convey pertinent information in a visually- and cognitively-comprehensible manner (Macklinay, 1986, van Wijk, 2005). Failing to meet these criteria risks misleading viewers, which is antithetical to the purpose of data visualisation. Understanding human factors in visualisation is therefore crucial for ensuring charts, graphs, and maps achieve their potential.

# Generating Evidence

**Regrettably, our understanding of how people interpret data visualisations (and subsequent guidance) is built on shaky foundations (Kosara, 2016). Some received wisdom has not been empirically tested at all, other claims have been debunked or have been confirmed only recently. Consequently, it is not always clear where evidence ends and opinion starts; intuition and unsubstantiated statements make for “visualisation folklore” (Correll, 2022, pg. 3). For example, evidence for the harm caused by ‘chart junk’ (visual embellishments, Tufte) is mixed (Franconeri et al. 2021), yet its deprecation is superficially attractive, appealing to aesthetic judgements and ‘common sense’, so it persists (Kosara, 2016). Rigorous data visualisation research is required to fill gaps in knowledge and generate a reliable evidence-base.**

**Visualisation research takes many forms. Studies on data visualisation have employed a range of techniques, including controlled experiments, usability tests, interviews, observations, and case studies, and have focused variously on perception, cognition, exploratory data analysis, and user experience (Lam et al., 2011). Experimental psychology studies on data visualisation are particularly valuable because they generate fundamental evidence on *how* visualisations are interpreted. Conversely, research focused on visualisation design without interest in human interpretation does not produce the same type of generalisable knowledge. Inadequate best practice recommendations indicate insufficient understanding of psychological mechanisms. However, progress can be slow, since theories about cognitive and perceptual processes are built through cumulative work (Chen et al., 2020). Psychological research confers benefits in the form of related empirical work, plus established methods and theories (Correll, 2022, Rensink, 2021).**

**Multiple studies illustrate that preferences and introspection are not a reliable source of information on effective visualisation practices. An experiment exploring physicians’ judgements about clinical trials found that icon arrays resulted in the most accurate judgements, compared to tables, pie charts, and bar charts (Elting et al., 1999). However, none of the 34 physicians in the sample preferred this format. In another study, medical students almost unanimously preferred visualisations with a rainbow colour scheme, but made fewer errors when using a diverging (e.g., red-blue) color scheme (Borkin et al., 2011). Tables of values may be favoured over visualisations in certain tasks where the visualisations actually offer significant benefits (Saket et al., 2019). Participants in Burns et al.'s (2021) study estimated that pictographs took longer to understand, compared to equivalent charts without icons. However, this self-report measure was at odds with recorded response times, which indicated no differences between chart types. Similarly, choropleth maps were preferred by graduate students despite conferring no performance advantage over other statistical map designs (Mendonça and Delazari, 2014). Many authors suggest that preferences are influenced by familiarity, rather than performance advantages. Measuring preferences provides valuable insight into people’s engagement with different visualisations. However, such opinions must be treated appropriately, not used to inform conclusions about effectiveness.**

**Rensink (2021) presents recommendations for generating useful research findings. Using a single task, and manipulating a single feature of interest, over multiple trials, assists in identifying underlying mechanisms. Integrating explanations from prior research helps ensure explanations of mental processes are sufficiently detailed. Other important but frequently overlooked matters include appropriate counterbalancing, reporting effect sizes and acknowledging individual differences.**

**There are a multitude of variables that can be manipulated to gain insight into visualisations. Criticisms are sometimes levelled at studies with particularly high or low levels of experimental control. However, researchers must strike an appropriate balance between ecological validity and precision (Abdul-Rahman et al., 2020). Choosing suitable tasks for participants requires a similar trade-off (Suh et al., 2022).**

**Vision sciences offer a variety of paradigms for assessing various aspects of human performance in visualisation tasks. For example, experiments may evaluate accuracy (by comparing responses to a correct answer), precision (by quantifying variability in responses), or processing speed (by measuring reaction times, Elliott et al., 2020). However, chosen methods must be appropriate for a research question. Whereas methods from vision-sciences are typically concerned with performance in low-level perceptual tasks, other research focuses on *message*-level interpretations (Pandey et al., 2015) or decision-making (Padilla et al., 2018).**

# Precision

**Identifying gaps in our understanding of the psychology of data visualisations requires knowledge of prior lines of inquiry and established findings. Arguably the most influential study in the field of data visualisation is Cleveland and McGill’s (1984) investigation of elementary perceptual processes involved in viewing visualisations. This study sought to establish how *precisely* viewers can represent different graphical properties used to encode data (e.g., position, length, angle, etc.). For each encoding type, participants identified which of two marks conveyed the smaller value, and estimated the difference in size as a percentage. Subsequent ranking based on the magnitude of participants’ errors produced a hierarchy of visual encoding channels. Since position-encoding produced smaller errors than both length- and angle-encoding, this suggests that data will be represented most precisely when encoded using position on a common (aligned) scale.**

**This study’s findings have endured replication (Heer and Bostock) and enthusiasm for perceptual precision has inspired a great deal of important research in this field, regarding visual processing of proportion (Spence and Lewandovsky, Hollands and Spence), variance (Stock and Behrens), correlation (Harrison et al., 2014, Hong et al., 2021), and other basic processes, such as visual comparison (Simkin and Hastie, 1987, Zacks et al. 1998) and color discrimination (Szafir, 2018). It has also influenced development of software for automating visualisation design (Mackinlay, 1986) and simulating visualisation comprehension (Lohse, 1993). However, to consider perceptual precision as the *only* relevant concern in data visualisation design is unwarranted. Many additional factors require consideration.**

# Beyond Precision

**Viewers do not always employ optimally-precise visual cues, which can lead to inaccurate judgements about plotted data (Yuan et al., 2021). Furthermore, in certain tasks, precision can actually hinder, rather than facilitate, judgements. Because perceptual averaging benefits from lower spatial frequencies, color encoding offers greater efficiency than more precise position encoding in line charts (Correll et al. 2012). Effective decision-making under uncertainty does not necessarily correspond to precision in probability estimation, because of differences in mental processing associated with these distinct tasks (Kale et al., 2020).**

**Furthermore, the choice of graphical encodings employed in a data visualisation can influence the *type* of interpretation it elicits. Viewers are more likely to refer to discrete differences when describing bar charts, and trends when describing line graphs, even when the nature of the plotted data is ill-suited to this type of characterisation (Zacks and Tversky, 1999). For example, a line chart may provoke an interpretation like ‘a building becomes more secure as the alarm system becomes more active’, whereas a bar chart may provoke an interpretation like ’a building with 10 motion sensors is more secure than a building with 5 motion sensors’. Similarly, *production* of bar charts and line charts is also influenced by whether a discrete or continuous relationship is specified in the brief. When presenting average values, design choices also influence beliefs about the distribution of underlying data. A data point positioned ‘inside’ a bar is considered more likely than one positioned ‘outside’ (Newman and Scholl, 2012), but confidence intervals eliminate this bias (Pentoney and Berger, 2016). This accords with the notions that metaphor (Ziemkiewicz and Kosara) and affordances (Kindlmann) play a role in a visualisation’s ability to convey information.**

**Attention is another important factor in comprehension of data visualisations. Complex tasks requiring selective attention can cause distinctive patterns (e.g., dinosaurs) in non-focal data to be completely overlooked (Boger et al., 2021). Features of data mentioned in textual summaries are over-weighted in viewers’ mental representations, causing difficulty in the ability to assume the perspective of a naïve viewer (Xiong et al. 2019). The salience of vertical bars may be responsible for incorrect reports of differences between histograms with identical distributions (Lem et al., 2014). Explicitly encoding differences between pairs of values can facilitate pattern recognition (Nothelfer and Franconeri, 2020) and highlighting particular attributes can facilitate recall (Ajani et al., 2021).**

**Simply conveying information is not the only purpose of data visualisations, since they also influence recall, opinion-formation, and decision-making (Bertini et al., 2020). A large number of cognitive biases affect these aspects of processing data visualisations, among several others (Dimara et al., 2020). Whilst it is necessary to consider the precision of elementary perceptual processes, that alone is not sufficient for a comprehensive understanding of how data visualisations function (Bertini et al., 2020).**

# Axis Manipulation

**Understanding how inaccurate impressions arise provides insight into mechanisms involved in interpreting data visualisations. This, in turn, can inform recommendations for effective design. A notorious topic in the literature on misleading visualisations is axis truncation. This typically refers to the practice of employing a y-axis which commences with a non-zero value (Correll et al. 2020), though may also be considered any adjustment at either extreme of an x- or y-axis (Pandey, 2015). There is considerable evidence that the range of axis values employed in charts influences interpretations of data.** 

**The majority of research on this topic has focused on how constraining the range of an axis, and thus increasing the physical distance between plotted values, increases the perceived magnitude of the difference between those values. Accountants appraising financial performance using line and bar charts interpreted plotted increases as larger when they were depicted using a truncated y-axis (Taylor and Anderson, 1986). Similarly, bar charts’ employing truncated axes biased students’ investment decisions (Arunachalam and Pei, 2002). A large-sample online experiment also observed that differences between values were considered larger when truncated bar charts were used, examining message-level representations of data by framing questions in terms of subject matter (access to safe drinking water) rather than graphical elements (difference in bar length, Pandey et al., 2015). Other axis manipulations, such as log-scales (Romano et al., 2020), inverted scales (Woodin et al., 2021, Pandey et al., 2015), and expanded axes in scatterplots (Cleveland, 1982) also influence judgements about data.**

**Risk communication research has independently generated similar findings. Since many hazards cannot be completely avoided, data visualisations are often used to contrast the levels of risk associated with two scenarios (e.g., intervention versus no intervention). Thus, assessments of ‘risk reduction’ are essentially judgements about the magnitude of difference between two values. Compared to stacked bar charts which include additional information on the total number of individuals at risk, those which display only the number of individuals *affected* (thus increasing bars’ visual disparity) increase impressions of the magnitude of difference (Stone et al., 2003).**

**The physical distance between data points consistently biases interpretations of the magnitude of difference in spite of attention to actual numerical values and design features intended to highlight truncation (Correll et al., 2020). Bias is diminished, but still observed, following explicit warnings about errors in judgement due to y-axis truncation. This suggests that this effect is largely automatic, and does not primarily occur due to insufficient engagement of cognitive capabilities (Yang et al., 2021).** 

**One study observed no association between participants’ data visualisation literacy and their susceptibility to axis truncation in bar charts (Yang et al., 2021). Conversely, another experiment suggests that the effect of axis truncation on subjective judgements and quantitative estimates in line charts disappears when accounting for data visualisation literacy (Driessen et al., 2022). However, heterogeneity of observed literacy levels in the latter experiment raised concerns about the scale used to measure data visualisation literacy.**

**Pandey et al. (2015) and Yang et al (2021) propose that this bias could arise due to the dominance of first impressions during translation from graphical schemata (Pinker, 1990) to a ‘real-world’ conceptual understanding (see also, Carpenter and Shah, 1998, Tversky and Kahneman, 1974).Additionally, Yang et al. (2021) suggest that viewers’ beliefs about a designer’s communicative intent could play a role in viewers’ interpretations. Under Grice’s *Cooperative Principle* (Grice, 1975), communicative contributions are assumed to be truthful, relevant, clear, and sufficiently informative. Therefore, viewers might infer that differences between values must be genuinely large if they appear large, else they would not be presented as such.**

**In *How to Lie With Statistics*, Huff (1954) suggests axis truncation creates a false impression of plotted data. This practice has been labelled ‘deceptive’ for both bar and line charts (Lauer and O’Brien, 2020). A tool for automatically identifying and correcting misleading line charts extends y-axes to include zero whenever this value is omitted from the original chart (Fan et al., 2022).** 

**Recent work has presented an alternative perspective on this controversial practice. Non-truncated axes can obscure significant differences just as easily as truncated axes can exaggerate inconsequential differences. The appropriate magnitude to convey depends on what constitutes an important difference in the data at hand (Correll et al., 2020). Indeed, *failing* to truncate an axis could be considered misleading in certain circumstances (Wainer, 1984). Yang et al. (2021) suggest that effective designs will ensure that viewers’ immediate characterisation of plotted data closely corresponds to their interpretation following a detailed inspection. Acknowledging that differences must be depicted in proportion to their significance, Witt (2019) reports that axes spanning approximately 1.5 standard deviations provide a balance between sensitivity and bias in fields with standardised effect size measures, such as psychology. Unfortunately, different domains will not necessarily share the same notion of what amounts to a meaningful difference. Choices regarding axis ranges are ultimately designers’ unavoidable decisions (Correll et al., 2020).**

**Line charts and bar charts are equally susceptible to biases due to truncation (Correll et al., 2020; Witt et al., 2019), yet there may be reason to treat them differently. Truncation distorts the mapping between a bar’s extent and the quantity it represents, but line charts’ free-floating position-encoding does not convey quantity in the same manner, providing immunity against such distortion (Bergstrom and West). Therefore, whilst starting an axis at zero provides no guarantee of an appropriate depiction of values’ differences, it ensures adherence to a fundamental aspect of visualisation design. Alternatively, quantitative data with discrete categories can be plotted using position-encodings only (e.g., dot plots), avoiding this trade-off.**

# Misleading Data Visualisations

**Some misleading visualisations may prevent viewers from accurately extracting numerical information. However, research on axis truncation illustrates that misleading visualisations may also interfere with subjective judgements. A line chart may avoid misrepresenting a dataset’s numerical properties yet generate a distorted impression of the magnitude of a trend. The latter is revealed not by assessing viewers’ *performance*, but their *interpretations* (Stone et al., 2015).**

**Influencing subjective judgements may still be considered a *misleading* practice because a dishonest framing of information could elicit a warped impression that would differ from the same viewer’s better-informed perspective. Therefore, not all aspects of deceptive design are *inherently* misleading, and deceptiveness can be context-dependent. Comparing examples of ‘misleaders’ from Ge et al.’s (2023) design space helps illustrate this distinction. ‘Concealed uncertainty’ and ‘cherry-picking’ refer to unambiguously deceptive practices, whereas ‘aggregation’ and ‘scale range’ must be preceded by the word *inappropriate* in order to convey their capacity to deceive.**
 
# The Present Thesis

**Data visualisation design has the potential to impact subjective judgements about many aspects of data, such as variability, noise, and numerosity. Prior research has closely examined how axis ranges/data visualisations can influence judgements of *relative* magnitude (differences between values). In contrast, little is known about how axis ranges/data visualisations may influence judgements of *absolute* magnitude: how large or small values are. This thesis demonstrates that interpretations of values’ *magnitudes* are influenced by axis ranges/data visualisation design.**

**In bar charts displaying data on individuals affected by a risk, perceived likelihood decreased when the total population at risk was also emphasised using shaded bars, rather than blank space (Stone et al., 2017). Other work directly manipulating axis limits impeded analysis of effects on values’ magnitudes by combining these judgements with measures of relative difference (Okan et al., 2018). In bar charts violating the convention of mapping higher values to higher positions, participants’ difficulty rejecting first impressions led to frequent misinterpretations of presented values’ magnitudes, particularly for participants with low literacy (Okan et al., 2012). Visualisations that facilitate comprehension of relative differences may fail to effectively communicate values’ absolute magnitudes, illustrating a potential trade-off in design (Reyna et al., 2008).**

**Two studies have specifically examined how axis ranges may inform impressions of absolute magnitude. Sandman et al. (1994) manipulated risk ladders, where individual probabilities are presented on vertical scales incorporating a range of probability values. Changing this range alters the position of a plotted value. Perceived threat (a composite measure made up of perceived likelihood, danger, reported concern and fear) was higher when the risk appeared near to the top of the ladder, compared to near the bottom. However, plotted values’ positions did not completely dictate magnitude judgments. A numerically higher risk plotted at the same position near the top of the ladder generated higher ratings. There was also mixed evidence regarding the effects on intentions to spend money mitigating the risk. Confidence in the robustness of these findings is limited by use of a single trial per participant, a single scenario, a composite measure obscuring pure magnitude ratings, and a confounding variable of the risk ladder’s range.**

**Comparing linear and logarithmic risk ladders, Freeman et al. (2021) did not replicate Sandman et al.’s (1994) main finding. However, in addition to a graphical cue to magnitude, risk ladders employed additional symbolic number cues in their titles, labels, and accompanying descriptions. A broken scale may also have reduced the degree to which inferences were based on the value’s physical position. Therefore, participants’ judgements may not have been purely based on appearance of visualisations.**

**Limited insight into how magnitude is interpreted in data visualisations impedes understanding of how visualisations may effectively communicate magnitude.**

# Visualisation Literacy
**Understanding individual differences in the ability to comprehend data in visualisations is important for understanding the psychology data visualisations (Boy et al., 2014). Research on this topic requires reliable tools for measuring data visualisation literacy.**

**Galesic and Garcia-Retamero’s 13-item test (2011) was based on Friel et al.’s (2001) hierarchy of skills for interpreting visualisations, from comprehension to extrapolation. Research has demonstrated that this scale can predict whether a graphical representation will facilitate understanding of risk information (Okan et al., 2012). Another 53-item test employs a wide range of data visualisation formats, and higher scores are positively associated with both numeracy and need for cognition (Lee et al., 2019).**

**Research on data visualisation literacy has tended to focus on interpretation of well-designed charts (Ge et al., 2023). Yet, the ability to detect (Camba et al., 2022) and make sense of (Ge et al., 2023) misleading charts should be considered an important feature of data visualisation literacy. A robust 30-item test enables assessment of individuals’ ability to accurately comprehend deceptive designs (Ge et al., 2023). This work also suggests that attention and critical thinking may benefit viewers in avoiding some, but not all, biased interpretations. Using Galesic and Garcia-Retamero’s 13-item test (2011), Okan et al. (2016) found that higher literacy is associated with more time processing a visualisation’s misleading features, thus promoting correct interpretations. Lower literacy is associated with greater reliance on conventions (e.g., the relationship between vertical position and magnitude).**

**The empirical work presented in this thesis employs the 5-item version of Garcia-Retamero et al.’s (2016) Subjective Graph Literacy scale. This measure echoes prior work in the development of subjective numeracy scales. Users are asked to rate their competence in working with bar charts, line charts, and pie charts, and also their ability to perform simple assessments using bar charts. Despite its short completion time and use of subjective ratings, it is strongly correlated with an objective measure of data visualisation literacy (Galesic and Garcia-Retamero, 2011). The scale also produces a final score out of 30, offering greater sensitivity than a similarly brief objective scale, where tallying correct responses produces a final score out of 4. These characteristics make for an appropriate tool for assessing participants’ data visualisation literacy in experimental studies. Indeed, this measure has been used to assess variability between participants in studies on axis truncation (Yang et al., 2021), correlation (Strain et al., 2023), information synthesis (Mantri et al., 2022), and explanation of visualisations (Yang et al., 2023).**

# Overview of Chapters

**Chapter 2** contains a review of relevant published research on data visualisation, providing background and justification for the empirical work contained in this thesis. **Chapter 3** contains a review of relevant published work on computational tools for reproducible research and details the computational approach used in this thesis. **Chapter 4** presents a pair of experiments which demonstrate that magnitude judgements can be influenced by data points’ relative positions within axis limits. **Chapter 5** presents an experiment which demonstrates that magnitude judgements can be influenced by the limits of color legends in choropleth maps. **Chapter 6** presents a pair of experiments which demonstrate the role of contextual information on interpretation of magnitude in data visualisation. **Chapter 7** contains a discussion of my empirical work, including implications and future directions. 